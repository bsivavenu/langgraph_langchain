{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25489ee",
   "metadata": {},
   "source": [
    "### Query Enhancement ‚Äì Query Expansion Techniques\n",
    "\n",
    "In a RAG pipeline, the quality of the query sent to the retriever determines how good the retrieved context is ‚Äî and therefore, how accurate the LLM‚Äôs final answer will be.\n",
    "\n",
    "That‚Äôs where Query Expansion / Enhancement comes in.\n",
    "\n",
    "#### üéØ What is Query Enhancement?\n",
    "Query enhancement refers to techniques used to improve or reformulate the user query to retrieve better, more relevant documents from the knowledge base.\n",
    "It is especially useful when:\n",
    "\n",
    "- The original query is short, ambiguous, or under-specified\n",
    "- You want to broaden the scope to catch synonyms, related phrases, or spelling variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff6ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.document_loaders import TextLoader\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410d3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step1 : Load and split the dataset\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f001ff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x13ff96c00>, search_type='mmr', search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### step 2: Vector Store\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(chunks,embedding_model)\n",
    "\n",
    "## step 3:MMR Retriever\n",
    "retriever=vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":5})\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e8f2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={'max_input_tokens': 200000, 'max_output_tokens': 100000, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x154b73c20>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x155068440>, root_client=<openai.OpenAI object at 0x14373c860>, root_async_client=<openai.AsyncOpenAI object at 0x154b99e50>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## step 4 : LLM and Prompt\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(\"openai:o4-mini\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "116e2cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\\n\\nOriginal query: \"{query}\"\\n\\nExpanded query:\\n')\n",
       "| ChatOpenAI(profile={'max_input_tokens': 200000, 'max_output_tokens': 100000, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x154b73c20>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x155068440>, root_client=<openai.OpenAI object at 0x14373c860>, root_async_client=<openai.AsyncOpenAI object at 0x154b99e50>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query expansion\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\n",
    "\n",
    "Original query: \"{query}\"\n",
    "\n",
    "Expanded query:\n",
    "\"\"\")\n",
    "\n",
    "query_expansion_chain=query_expansion_prompt| llm | StrOutputParser()\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d629dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expanded query:\\n\\n‚ÄúLangChain memory‚Äù OR ‚ÄúLangChain memory management‚Äù OR ‚ÄúLangChain memory module‚Äù OR ‚ÄúLangChain memory store‚Äù OR ‚ÄúLLM context persistence‚Äù OR ‚Äúsession state persistence‚Äù OR ‚Äústateful agent memory‚Äù OR ‚Äúconversational memory‚Äù OR ‚Äúchatbot memory‚Äù OR ‚Äúsession memory‚Äù OR ‚Äúconversation buffer memory‚Äù OR ‚ÄúConversationBufferMemory‚Äù OR ‚ÄúConversationSummaryMemory‚Äù OR ‚ÄúLongTermMemory‚Äù OR ‚ÄúKnowledgeGraphMemory‚Äù OR ‚Äúvector store memory‚Äù OR ‚Äúembedding store memory‚Äù OR ‚ÄúRAG memory‚Äù OR ‚Äúretrieval-augmented generation memory‚Äù OR ‚Äúcontext window extension‚Äù OR ‚Äútoken caching‚Äù OR ‚Äúin-memory vs. disk-based memory‚Äù OR ‚Äúpersistent memory store‚Äù OR ‚ÄúRedisMemory‚Äù OR ‚ÄúFaissMemory‚Äù OR ‚ÄúChromaMemory‚Äù OR ‚ÄúPineconeMemory‚Äù OR ‚ÄúMilvusMemory‚Äù OR ‚ÄúMongoDBMemory‚Äù OR ‚ÄúSQLiteMemory‚Äù OR ‚Äúmemory plugin‚Äù OR ‚Äúmemory component‚Äù OR ‚Äúmemory API‚Äù'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expansion_chain.invoke({\"query\":\"Langchain memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec6e3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG answering prompt\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm=llm,prompt=answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57e726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline with query expansion\n",
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x: retriever.invoke(query_expansion_chain.invoke({\"query\": x[\"input\"]}))\n",
    "    })\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddebe80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:  \n",
      "‚ÄúLangChain supported memory types and modules‚Äîwhat memory architectures, storage backends and interfaces does the LangChain framework provide? For example: conversation memory, buffer memory (ConversationBufferMemory), summary memory (ConversationSummaryMemory), windowed or sliding-window/token-window memory, vector-store memory, callback memory, long-term vs short-term memory, in-memory vs persistent (Redis, SQL, file-based) memory stores, custom memory implementations; memory management strategies, memory backends, memory modules and memory interfaces in LangChain.‚Äù\n",
      "‚úÖ Answer:\n",
      " LangChain‚Äôs built-in memory modules fall into two main flavors:  \n",
      "1. ConversationBufferMemory ‚Äì keeps the full history of the back-and-forth in memory, and  \n",
      "2. ConversationSummaryMemory ‚Äì rolls up earlier turns into a concise summary so you can stay within token limits.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"What types of memory does LangChain support?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"‚úÖ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd86621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:\n",
      "\n",
      "(\"CrewAI\" OR \"Crew AI\" OR \"CrewAI platform\" OR \"Crew AI agents\")  \n",
      "AND  \n",
      "(\"AI agents\" OR \"autonomous agents\" OR \"intelligent agents\" OR \"virtual assistants\" OR \"digital assistants\" OR \"software agents\" OR \"conversational agents\" OR \"multi-agent system\")  \n",
      "AND  \n",
      "(\"features\" OR \"capabilities\" OR \"architecture\" OR \"design patterns\" OR \"framework\" OR \"integration\" OR \"APIs\" OR \"SDK\" OR \"technical specifications\" OR \"documentation\")  \n",
      "AND  \n",
      "(\"use cases\" OR \"applications\" OR \"workflows\" OR \"automation\" OR \"customer support\" OR \"operations\" OR \"logistics\" OR \"team collaboration\")  \n",
      "AND  \n",
      "(\"natural language processing\" OR \"NLP\" OR \"reinforcement learning\" OR \"deep learning\" OR \"multi-modal AI\" OR \"agent orchestration\" OR \"decision making\" OR \"LLM\" OR \"GPT-4\" OR \"transformers\")  \n",
      "AND  \n",
      "(\"deployment\" OR \"scalability\" OR \"security\" OR \"performance\" OR \"best practices\")\n",
      "‚úÖ Answer:\n",
      " CrewAI agents are autonomous AI ‚Äúworkers‚Äù that you assemble into a team (a ‚Äúcrew‚Äù) to tackle complex tasks via a structured, role-based workflow. Key characteristics:  \n",
      "‚Ä¢ Roles ‚Äì each agent is assigned a specialized role (e.g. researcher, planner, executor, reviewer, etc.).  \n",
      "‚Ä¢ Semi-independent operation ‚Äì agents make decisions and take actions within their remit, but coordinate with one another through the CrewAI orchestrator.  \n",
      "‚Ä¢ Configurability ‚Äì you declare each agent‚Äôs role, goals, memory schema, and available tools in a simple YAML or JSON-style config.  \n",
      "‚Ä¢ Orchestration ‚Äì CrewAI manages the agent loop, turn-taking, state sharing, and decision-making so the crew advances toward the overall goal with minimal human intervention.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"CrewAI agents?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"‚úÖ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03d32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
