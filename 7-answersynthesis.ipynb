{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e61cda",
   "metadata": {},
   "source": [
    "### ðŸ§  Answer Synthesis from Multiple Sources\n",
    "âœ… What Is It?\n",
    "\n",
    "Answer synthesis from multiple sources is the process where an AI agent collects information from different retrieval tools or knowledge bases, and merges that information into a single, coherent, and contextually rich answer.\n",
    "\n",
    "This is a core capability in Agentic RAG, where the system is more than just a simple retriever â€” it plans, retrieves, and then synthesizes an answer that draws from multiple sources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e498f0",
   "metadata": {},
   "source": [
    "ðŸŽ¯ Why Itâ€™s Needed\n",
    "Most real-world queries are:\n",
    "- Multifaceted (require multiple types of information)\n",
    "- Ambiguous or incomplete (need refinement)\n",
    "- Open-ended (donâ€™t map to a single document or source)\n",
    "\n",
    "ðŸ” This makes retrieving from a single vector DB insufficient.\n",
    "\n",
    "Instead, we want an agent that can:\n",
    "\n",
    "- Decide what to fetch from where (retrieval planning)\n",
    "- Retrieve content from multiple tools (e.g., Wikipedia, PDFs, APIs, SQL)\n",
    "- Evaluate and merge that context\n",
    "- Produce a single human-like response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cb8854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_classic.schema import Document\n",
    "from langchain_classic.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders.youtube import YoutubeLoader\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.tools import WikipediaQueryRun\n",
    "from langchain_classic.utilities import WikipediaAPIWrapper\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "231639d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "llm=init_chat_model(\"openai:gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5597d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_retriever(file_path):\n",
    "    docs = TextLoader(file_path, encoding=\"utf-8\").load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    vs = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "    return vs.as_retriever()\n",
    "\n",
    "def load_youtube_retriever():\n",
    "    # Mocked YouTube transcript text\n",
    "    content = \"\"\"\n",
    "    This video explains how agentic AI systems rely on feedback loops, memory, and tool use.\n",
    "    It compares them to traditional pipeline-based LLMs. Temporal reasoning and autonomous tasking are emphasized.\n",
    "    \"\"\"\n",
    "    doc = Document(page_content=content, metadata={\"source\": \"youtube\"})\n",
    "    vectorstore = FAISS.from_documents([doc], OpenAIEmbeddings())\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "# def wikipedia_search(query: str) -> str:\n",
    "#     print(\"ðŸŒ Searching Wikipedia...\")\n",
    "#     return WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())(query)\n",
    "\n",
    "def arxiv_search(query: str) -> str:\n",
    "    print(\"ðŸ“„ Searching ArXiv...\")\n",
    "    results = ArxivLoader(query).load()\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in results[:2]) or \"No relevant papers found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "568f9ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_retriever = load_text_retriever(\"research_notes.txt\")\n",
    "youtube_retriever = load_youtube_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "867eea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### state\n",
    "class MultiSourceRAGState(BaseModel):\n",
    "    question: str\n",
    "    text_docs: List[Document] = []\n",
    "    yt_docs: List[Document] = []\n",
    "    wiki_context: str = \"\"\n",
    "    arxiv_context: str = \"\"\n",
    "    final_answer: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9f025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Nodes\n",
    "def retrieve_text(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    docs = text_retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"text_docs\": docs})\n",
    "\n",
    "def retrieve_yt(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    docs = youtube_retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"yt_docs\": docs})\n",
    "\n",
    "def retrieve_wikipedia(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    result = wikipedia_search(state.question)\n",
    "    return state.model_copy(update={\"wiki_context\": result})\n",
    "\n",
    "def retrieve_arxiv(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    result = arxiv_search(state.question)\n",
    "    return state.model_copy(update={\"arxiv_context\": result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed172aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## synthesize\n",
    "def synthesize_answer(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    \n",
    "    context = \"\"\n",
    "\n",
    "    context += \"\\n\\n[Internal Docs]\\n\" + \"\\n\".join([doc.page_content for doc in state.text_docs])\n",
    "    context += \"\\n\\n[YouTube Transcript]\\n\" + \"\\n\".join([doc.page_content for doc in state.yt_docs])\n",
    "    context += \"\\n\\n[Wikipedia]\\n\" + state.wiki_context\n",
    "    context += \"\\n\\n[ArXiv]\\n\" + state.arxiv_context\n",
    "\n",
    "    prompt = f\"\"\"You have retrieved relevant context from multiple sources. Now synthesize a complete and coherent answer.\n",
    "\n",
    "Question: {state.question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "\n",
    "    answer = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update={\"final_answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6201f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x149d8a180>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = StateGraph(MultiSourceRAGState)\n",
    "\n",
    "builder.add_node(\"retrieve_text\", retrieve_text)\n",
    "builder.add_node(\"retrieve_yt\", retrieve_yt)\n",
    "builder.add_node(\"retrieve_wiki\", retrieve_wikipedia)\n",
    "builder.add_node(\"retrieve_arxiv\", retrieve_arxiv)\n",
    "builder.add_node(\"synthesize\", synthesize_answer)\n",
    "\n",
    "builder.set_entry_point(\"retrieve_text\")\n",
    "builder.add_edge(\"retrieve_text\", \"retrieve_yt\")\n",
    "builder.add_edge(\"retrieve_yt\", \"retrieve_wiki\")\n",
    "builder.add_edge(\"retrieve_wiki\", \"retrieve_arxiv\")\n",
    "builder.add_edge(\"retrieve_arxiv\", \"synthesize\")\n",
    "builder.add_edge(\"synthesize\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bb30330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "def wikipedia_search(query: str) -> str:\n",
    "    print(\"ðŸŒ Searching Wikipedia...\")\n",
    "\n",
    "    wiki = WikipediaQueryRun(\n",
    "        api_wrapper=WikipediaAPIWrapper()\n",
    "    )\n",
    "\n",
    "    return wiki.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0789c928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ Searching Wikipedia...\n",
      "ðŸ“„ Searching ArXiv...\n",
      "âœ… Final Answer:\n",
      "\n",
      "Transformer agents represent a significant advancement in artificial intelligence, particularly in the field of multi-agent deep reinforcement learning (MADRL). These systems leverage the transformer architecture, which excels in understanding and processing complex data, to create autonomous agents capable of engaging in complex tasks within dynamic environments.\n",
      "\n",
      "### What Are Transformer Agents?\n",
      "\n",
      "Transformer agents are essentially AI agents that utilize the transformer neural network architecture to process information. They have been widely adopted for their ability to handle long-range dependencies in data and their parallel processing capabilities. These agents are often large language models (LLMs) capable of generating text, reasoning, and adapting their responses based on interactions with their environments and other agents.\n",
      "\n",
      "**Key Characteristics of Transformer Agents:**\n",
      "\n",
      "1. **Self-Attention Mechanism:** Unlike previous architectures that relied heavily on recurrence, transformers utilize self-attention to weigh the importance of different words or data points in a sequence, allowing for more nuanced understanding and quicker learning.\n",
      "\n",
      "2. **Generative Capabilities:** Transformer agents can produce human-like text and engage in dialogue, making them suitable for roles ranging from customer service bots to collaborative assistants in various tasks.\n",
      "\n",
      "3. **Learnable Communication:** These agents are capable of learning from interactions with their environment and other agents, adapting their communication strategies over time to enhance efficiency and effectiveness.\n",
      "\n",
      "4. **Multi-Agent Coordination:** Transformer agents can communicate and coordinate with multiple counterparts, leading to improved performance in cooperative or competitive scenarios through learned communication protocols.\n",
      "\n",
      "### Recent Evolutions in Research\n",
      "\n",
      "Research into transformer agents is rapidly evolving, with numerous advancements aimed at enhancing their capabilities and applicability. Here are some of the current trends and innovations:\n",
      "\n",
      "1. **Self-Evolving Agents:** Recent studies emphasize developing self-evolving agentsâ€”autonomous systems that adapt and optimize their internal processes based on continuous feedback from their environment. This includes adapting their communication strategies and workflow based on past interactions and observed outcomes.\n",
      "\n",
      "2. **Communication Efficiency:** The integration of communication in MADRL settings highlights the importance of efficient message sharing among agents. Researchers are exploring frameworks that determine when, how, and what to communicate, allowing agents to maintain useful interactions while minimizing overhead.\n",
      "\n",
      "3. **Robustness and Safety:** Ensuring that transformer agents can operate safely, especially in real-world applications, is a critical focus area. This involves creating robust systems capable of resisting adversarial attacks and maintaining functionality despite unpredictable interactions.\n",
      "\n",
      "4. **Memory and Tool Use:** Some transformer agents incorporate advanced memory architectures, enabling them to retain and access information over long periods, facilitating ongoing learning and adaptation. Additionally, they are being designed to effectively use external tools, greatly extending their range of actions in practical tasks.\n",
      "\n",
      "5. **Evaluation Frameworks:** As these agents become more complex, so too do the metrics for their evaluation. Current methodologies go beyond simple success rates to encompass reasoning quality, generalization abilities, and compliance with safety and alignment standards.\n",
      "\n",
      "6. **Domain-Specific Optimizations:** There is growing interest in tailoring transformer agents for specific domains, such as medicine or finance, where their architectures can be fine-tuned to meet the unique requirements of those fields.\n",
      "\n",
      "7. **Multi-Agent Systems:** New research highlights the development of systems where multiple transformer agents can collaborate, facilitating complex problem solving by distributing tasks among specialized agents, enhancing both efficiency and efficacy.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Transformer agents are at the forefront of AI research, playing essential roles in various applications by leveraging advanced models to understand, learn, and interact in complex environments. Ongoing research is focused on refining these agents' capabilities, addressing challenges related to communication, safety, and domain specialization, thus paving the way for more robust, efficient, and trustworthy AI systems equipped for real-world applications.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are transformer agents and how are they evolving in recent research?\"\n",
    "state = MultiSourceRAGState(question=question)\n",
    "result = graph.invoke(state)\n",
    "\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(result[\"final_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88402b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are transformer agents and how are they evolving in recent research?',\n",
       " 'text_docs': [Document(id='b06ae613-a46e-4478-9d87-a6aca5a64f3e', metadata={'source': 'research_notes.txt'}, page_content='TinyBERT:\\n- Used for classification: Support ticket priority tagging\\n- Evaluation: 87% F1 score, confusion in ambiguous class C\\n- Works well with 2-layer FFN adapter for domain transfer\\n\\nAdditional Experiments:\\n\\n1. FlashAttention2:\\n   - Integrated into LLaMA2\\n   - Reduces context latency by ~50%\\n\\n2. Chain-of-Thought prompting:\\n   - Outperforms direct answer prompting by 8% on logic tasks\\n   - Reflective prompting increases accuracy by 3%'),\n",
       "  Document(id='6f6ca8ab-a2a0-496f-9130-9a0fc7211a29', metadata={'source': 'research_notes.txt'}, page_content='7. Safety:\\n   - Toxicity detection via Detoxify\\n   - Out-of-scope filter via zero-shot classifier\\n   - Red teaming involved adversarial prompt testing\\n\\nConclusion:\\n\\n- LLaMA2 continues to be promising for controlled chat\\n- Sparse attention models are being prioritized for research\\n- Dynamic RAG pipelines via LangGraph outperform static RAG'),\n",
       "  Document(id='7c6df3ed-6d86-4be6-9339-2c529a7c6bbb', metadata={'source': 'research_notes.txt'}, page_content='Experiment Log: Transformer Evaluation - July 2024\\n\\nEfficientFormer:\\n- Top-1 accuracy: 92.4% on TinyImageNet\\n- Peak memory usage: 290MB (batch size = 16)\\n- Deployment target: Raspberry Pi 4\\n- Notes: Works well in quantized int8 mode with no major accuracy drop\\n\\nLongformer:\\n- Evaluation on customer support logs (max tokens: 8192)\\n- Latency >1.2s per query in streaming setting\\n- Observations: Chunk-based hybrid attention may reduce delay'),\n",
       "  Document(id='c03e1df8-4d42-499e-aec9-e7c6f1aba40e', metadata={'source': 'research_notes.txt'}, page_content='Reformer:\\n- Challenges during training:\\n   - Bucket collisions\\n   - Inconsistent loss spikes beyond 5k steps\\n   - Sparse gradient updates during LSH attention\\n- Solutions tried:\\n   - Gradient clipping\\n   - Warmup scheduler\\n   - Memory-efficient attention modules')],\n",
       " 'yt_docs': [Document(id='0588adb9-280c-44df-b4b0-65e488acc08a', metadata={'source': 'youtube'}, page_content='\\n    This video explains how agentic AI systems rely on feedback loops, memory, and tool use.\\n    It compares them to traditional pipeline-based LLMs. Temporal reasoning and autonomous tasking are emphasized.\\n    ')],\n",
       " 'wiki_context': 'Page: Large language model\\nSummary: A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. The largest and most capable LLMs are generative pre-trained transformers (GPTs) that provide the core capabilities of modern chatbots. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\\nThey consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational agents, code generation, knowledge retrieval, and automated reasoning that previously required bespoke systems.\\nLLMs evolved from earlier statistical and recurrent neural network approaches to language modeling. The transformer architecture, introduced in 2017, replaced recurrence with self-attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. This innovation enabled models like GPT, BERT, and their successors, which demonstrated emergent behaviors at scale, such as few-shot learning and compositional reasoning.\\nReinforcement learning, particularly policy gradient algorithms, has been adapted to fine-tune LLMs for desired behaviors beyond raw next-token prediction. Reinforcement learning from human feedback (RLHF) applies these methods to optimize a policy, the LLM\\'s output distribution, against reward signals derived from human or automated preference judgments. This has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance.\\nBenchmark evaluations for LLMs have evolved from narrow linguistic assessments toward comprehensive, multi-task evaluations measuring reasoning, factual accuracy, alignment, and safety. Hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements.\\n\\nPage: Superintelligence\\nSummary: A superintelligence is a hypothetical agent that possesses intelligence surpassing that of the most gifted human minds. Philosopher Nick Bostrom defines superintelligence as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\".\\nTechnological researchers disagree about how likely present-day human intelligence is to be surpassed. Some argue that advances in artificial intelligence (AI) will probably result in general reasoning systems that lack human cognitive limitations. Others believe that humans will evolve or directly modify their biology to achieve radically greater intelligence. Several future study scenarios combine elements from both of these possibilities, suggesting that humans are likely to interface with computers, or upload their minds to computers, in a way that enables substantial intelligence amplification. The hypothetical creation of the first superintelligence may or may not result from an intelligence explosion or a technological singularity.\\nSome researchers believe that superintelligence will likely follow shortly after the development of artificial general intelligence. The first generally intelligent machines are likely to immediately hold an enormous advantage in at least some forms of mental capability, including the capacity of perfect recall, a vastly superior knowledge base, and the ability to multitask in ',\n",
       " 'arxiv_context': 'A Comprehensive Survey of Self-Evolving AI Agents\\nA New Paradigm Bridging Foundation Models and Lifelong Agentic Systems\\nJinyuan Fangâˆ—1, Yanwen Pengâˆ—2, Xi Zhangâˆ—1, Yingxu Wang3, Xinhao Yi1, Guibin Zhang4, Yi Xu5, Bin Wu6,\\nSiwei Liu7, Zihao Li1, Zhaochun Ren8, Nikos Aletras2, Xi Wang2, Han Zhou5, Zaiqiao Meng1âœ‰\\n1University of Glasgow, 2University of Sheffield, 3Mohamed bin Zayed University of Artificial\\nIntelligence, 4National University of Singapore, 5University of Cambridge, 6University College\\nLondon, 7University of Aberdeen, 8Leiden University\\nâˆ—Equal Contributor, âœ‰Corresponding Author\\nRecent advances in large language models (LLMs) have sparked growing interest in AI agents capable\\nof solving complex, real-world tasks. However, most existing agent systems rely on manually crafted\\nconfigurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving\\nenvironments. To address this limitation, recent research has explored agent evolution techniques that\\naim to automatically enhance agent systems based on interaction data and environmental feedback. This\\nemerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of\\nfoundation models with the continuous adaptability required by lifelong agentic systems. In this survey,\\nwe provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically,\\nwe first introduce a unified conceptual framework that abstracts the feedback loop underlying the design\\nof self-evolving agentic systems. The framework highlights four key components: System inputs, Agent\\nSystem, Environment, and Optimisers, serving as a foundation for understanding and comparing different\\nstrategies. Based on this framework, we systematically review a wide range of self-evolving techniques\\nthat target different components of the agent system, including foundation models, agent prompts,\\nmemory, tools, workflows, and communication mechanisms across agents. We also investigate domain-\\nspecific evolution strategies developed for specialised fields such as biomedicine, programming, and\\nfinance, where agent behaviour and optimisation objectives are tightly coupled with domain constraints.\\nIn addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for\\nself-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey\\naims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents,\\nlaying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.\\nGithub: https://github.com/EvoAgentX/Awesome-Self-Evolving-Agents\\n1\\nIntroduction\\nRecent progress in large language models (LLMs) has significantly advanced the development of artificial\\nintelligence (AI). Owing to progress in large-scale pretraining, supervised fine-tuning and reinforcement\\nlearning, LLMs have demonstrated remarkable capabilities in planning, reasoning, and natural language\\nunderstanding (Zhao et al., 2023; Grattafiori et al., 2024; Yang et al., 2025a; Guo et al., 2025). These advances\\nhave sparked growing interest in LLM-based agents (a subclass of AI agents in which an LLM serves as the\\ndecision/policy module) (Wang et al., 2024c; Luo et al., 2025a), which are autonomous systems that leverage\\nLLMs as the core reasoning components for understanding inputs, planning actions, and generating outputs\\nin open-ended, real-world environments (Wang et al., 2024c; Xi et al., 2025; Luo et al., 2025a). A typical AI\\nagent consists of several components that enable it to perform complex, goal-oriented tasks in an autonomous\\nmanner. The foundation model (e.g. an LLM) is the core, responsible for interpreting goals, making plans, and\\nexecuting actions. To support these capabilities, additional modules, such as perception (Shridhar et al., 2021;\\nZheng et al., 2024), planning (Yao et al., 2023a,b; Besta et al., 2024), memory (Modarressi et al., 2023; Zhong\\net al., 2024), and tools (Schick et al., 2023; Gou et al., 2024; Liu et al., 2025g), are integrated to help the agent\\nperceive inputs, decompose tasks, retain contextual information, and interact with tools (Wang et al., 2024c).\\n1\\narXiv:2508.07407v2  [cs.AI]  31 Aug 2025\\nFigure 1 LLM-centric learning is evolving from learning purely from static data, to interacting with dynamic environments,\\nand ultimately towards lifelong learning through multi-agent collaboration and self-evolution.\\nWhile single-agent systems have demonstrated strong generalisation and adaptability in various tasks, they often\\nstruggle with task specialisation and coordination in dynamic and complex environments (Wu et al., 2024a; Qian\\net al., 2024). These limitations have led to the development of multi-agent systems (MAS) (Hong et al., 2024;\\nGuo et al., 2024c; Zhou et al., 2025a), where multiple agents collaborate to solve complex problems. Compared\\nwith single-agent systems, MAS enables functional specialisation, with each agent designed for a specific subtask\\nor domain of expertise. Moreover, agents can interact, exchange information, and coordinate their behaviour to\\nachieve shared goals. Such collaboration enables the system to tackle tasks beyond the capability of a single\\nagent, while simulating more realistic, dynamic, and interactive environments. LLM-based agent systems have\\nbeen successfully applied to a wide range of real-world tasks, ranging from code generation (Jiang et al., 2024),\\nscientific research (Lu et al., 2024a), web navigation (Lai et al., 2024a), to domain-specific applications in\\nbiomedicine (Kim et al., 2024) and finance (Tian et al., 2025).\\nDespite the notable progress in agent systems, most of them, whether single- or multi-agent, continue to\\nrely extensively on manually designed configurations. Once deployed, these systems typically maintain static\\narchitectures and fixed functionalities. However, real-world environments are dynamic and continuously evolving,\\ne.g., user intents shift, task requirements change, and external tools or information sources may vary over\\ntime. For instance, an agent assisting in customer service may need to handle newly introduced products,\\nupdated company policies, or unfamiliar user intents. Similarly, a scientific research assistant may be required\\nto incorporate a newly published algorithm, or integrate a novel analysis tool. In such settings, manually\\nreconfiguring the agent system is time-consuming, labour-intensive, and difficult to scale.\\nThese challenges have motivated recent efforts to explore the new paradigm of Self-Evolving AI Agents, a novel\\nclass of agent systems capable of autonomous adaptation and continuous self-improvement, bridging foundation\\nmodels with lifelong learning agentic systems.\\nDefinition\\nSelf-evolving AI agents are autonomous systems that continuously and systematically optimise their internal\\ncomponents through interaction with environments, with the goal of adapting to changing tasks, contexts\\nand resources while preserving safety and enhancing performance.\\n2\\nInspired by Isaac Asimovâ€™s Three Laws of Robotics1, we propose a set of guiding principles for safe and effective\\nself-evolution of AI agents:\\nThree Laws of Self-Evolving AI Agents\\nI. Endure (Safety Adaptation)\\nSelf-evolving AI agents must maintain safety and stability during any modification;\\nII. Excel (Performance Preservation)\\nSubject to the First law, self-evolving AI agents must preserve or enhance existing task performance;\\nIII. Evolve (Autonomous Evolution)\\nSubject to the First and Second law, self-evolving AI agents must be able to autonomously optimise\\ntheir internal components in response to changing tasks, environments, or resources.\\nWe characterise the emergence of self-evolving AI agents as part of a broader paradigm shift in the development\\nof LLM-based systems. This shift spans from early-stage Model Offline Pretraining (MOP) and Model Online\\nAdaptation (MOA), to more recent trends in Multi-Agent Orchestration (MAO), and ultimately, to Multi-Agent\\nSelf-Evolving (MASE). As summarised in Figure 1 and Table 1, each paradigm builds on the previous one,\\nmoving from a static, frozen foundation model to fully autonomous, self-evolving agentic systems.\\nâ€¢ MOP (Model Offline Pretraining). The initial stage focuses on pretraining foundation models on large-scale,\\nstatic corpora and then deploying them in a fixed, frozen state, without further adaptation.\\nâ€¢ MOA (Model Online Adaptation). Building on MOP, this stage introduces post-deployment adaptation,\\nwhere the foundation models can be updated through techniques such as supervised fine-tuning, low-\\nrank adapters (Pfeiffer et al., 2021; Hu et al., 2022), or reinforcement learning from human feedback\\n(RLHF) (Ouyang et al., 2022), using labels, ratings, or instruction prompts.\\nâ€¢ MAO (Multi-Agent Orchestration). Extending beyond a single foundation model, this stage coordinates\\nmultiple LLM agents that communicate and collaborate via message exchange or debate prompts (Li et al.,\\n2024g; Zhang et al., 2025h), to solve complex tasks without modifying the underlying model parameters.\\nâ€¢ MASE (Multi-Agent Self-Evolving).\\nFinally, MASE introduces a lifelong, self-evolving loop where a\\npopulation of agents continually refines their prompts, memory, tool-use strategies and even their\\ninteraction patterns based on environmental feedback and meta-rewards (Novikov et al., 2025; Zhang\\net al., 2025i).\\nThe evolution from MOP to MASE represents a fundamental shift in the development of LLM-based systems,\\nfrom static, manually configured architectures to adaptive, data-driven systems that can evolve in response to\\nchanging requirements and environments. Self-evolving AI agents bridge the static capabilities of foundation\\nmodels with the continuous adaptability required by lifelong agentic systems, offering a path toward more\\nautonomous, resilient, and sustainable AI.\\nDespite self-evolving AI agents representing an ambitious vision for future AI systems, achieving this level of\\nautonomy remains a long-term goal. Current systems are still far from exhibiting the full capabilities required\\nfor safe, robust and open-ended self-evolution. In practice, current progress towards this vision is achieved\\nthrough agent evolution and optimisation techniques, which provide practical means for enabling agent systems\\nto iteratively refine their components based on interaction data and environmental feedback, thereby enhancing\\ntheir effectiveness in real-world tasks. Recent research has explored several key directions in this area. One line\\nof work focuses on enhancing the underlying LLM itself to improve the core capabilities, such as planning (Qiao\\net al., 2024), reasoning (Zelikman et al., 2022; Tong et al., 2024), and tool use (Feng et al., 2025a). Another line\\nof research targets the optimisation of auxiliary components within agent systems, including prompts (Xu et al.,\\n2022; Prasad et al., 2023; Yang et al., 2024a; Wang et al., 2025i), tools (Yuan et al., 2025b; Qu et al., 2025),\\n1Introduced in his stories â€œRunaroundâ€ (1942) and â€œI, Robotâ€ (1950). These laws are hierarchical: the Second cannot override\\nthe First, and the Third cannot override the First or Second. Although conceived as fictional moral constraints, they have become\\ninfluential in AI ethics research. Therefore, we articulate the â€œThree Laws of Self-Evolving AI Agentsâ€, advocating that AI agents,\\nas the core of embodied AI, prioritise compliance and safety before pursuing autonomous evolution.\\n3\\nParadigm\\nInteraction & Feedback\\nKey Techniques\\nDiagram\\nModel Offline\\nPretraining (MOP)\\nModel â‡”Static data\\n(loss/backprop)\\nâ€¢ Transformer Pretraining (Causal\\nLM, Masked LM, NSP)\\nâ€¢ BPE / SentencePiece\\nâ€¢ MoE & Pipeline Parallelism\\nStatic data\\nModel\\nloss\\nModel Online\\nAdaptation (MOA)\\nModel â‡”Supervision\\n(labels/scores/rewards)\\nâ€¢ Task Fine-tuning\\nâ€¢ Instruction Tuning\\nâ€¢ LoRA / Adapters / Prefix-Tuning\\nâ€¢ RLHF (RLAIF, DPO, PPO)\\nâ€¢ Multi-Modal Alignment\\nâ€¢ Human Alignment\\nModel\\nModel A\\nModel B\\nModel C\\nA\\nSFT\\nB\\nLoRA\\nC\\nRLHF\\nMulti-Agent\\nOrchestration (MAO)\\nAgent1 â‡”Agent2\\n(message exchange)\\nâ€¢ Multi-Agent Systems\\nâ€¢ Self-Reflection\\nâ€¢ Multi-Agent Debate\\nâ€¢ Chain-of-Thought Ensemble\\nâ€¢ Function / Tool Calling / MCP\\nÃ†\\nÃ†\\nÃ†\\nMulti-Agent\\nSelf-Evolving (MASE)\\nAgents â‡”Environment\\n(signals from env.)\\nâ€¢ Behaviour Optimisation\\nâ€¢ Prompt Optimisation\\nâ€¢ Memory Optimisation\\nâ€¢ Tool Optimisation\\nâ€¢ Agentic Workflow Optimisation\\nEnv.\\nÃ†\\nÃ†\\nÃ†\\nTable1 Comparison of four LLM-centric learning paradigms â€“ Model Offline Pretraining (MOP), Model Online Adaptation\\n(MOA), Multi-Agent Orchestration (MAO), and Multi-Agent Self-Evolving (MASE), highlighting each paradigmâ€™s\\ninteraction & feedback mechanisms, core techniques, and illustrative diagrams to trace the progression from static model\\ntraining to dynamic, autonomous agent evolution.\\nmemory (Zhong et al., 2024; Lee et al., 2024d), and etc., allowing the agents to better generalise to new tasks\\nand dynamic environments. Furthermore, in multi-agent systems, recent work investigates the optimisation of\\nagent topologies and communication protocols (Bo et al., 2024; Chen et al., 2025h; Zhang et al., 2025j; Zhou\\net al., 2025a), aiming to identify agent structures that are best suited to the current task and improve the\\ncoordination and information sharing among agents.\\nExisting surveys on AI agents either focus on the general introduction of agent architectures and functionali-\\nties (Wang et al., 2024c; Guo et al., 2024c; Xi et al., 2025; Luo et al., 2025a; Liu et al., 2025a,d), or target\\nspecific components such as planning (Huang et al., 2024b), memory (Zhang et al., 2024d), collaboration\\nmechanism (Tran et al., 2025), and evaluation (Yehudai et al., 2025). Other surveys investigate domain-specific\\napplications of agents, such as operating system agents (Hu et al., 2025b) and healthcare agents (Sulis et al.,\\n2023). While these surveys provide valuable insights into various aspects of agent systems, recent advances\\nin agent self-evolution and continual adaptation have not been sufficiently covered, which corresponds to the\\ncapabilities of agents that are central to the development of lifelong, autonomous AI systems. This leaves\\na critical gap in the literature for researchers and practitioners seeking a holistic understanding of the new\\nlearning paradigm that underpins adaptive and self-evolving agentic systems.\\nTo bridge this gap, this survey provides a focused and systematic review of techniques that enable agents to\\nevolve and improve themselves based on interaction data and environmental feedback. Specifically, we introduce\\na unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic\\nsystems. This framework identifies four core components: System Inputs, Agent System, Environment, and\\nOptimisers, highlighting the evolution loop of agent systems. Building on this framework, we systematically\\nexamine a wide range of evolution and optimisation techniques that target different components of the agent\\nsystems, including the LLM, prompts, memory, tools, workflow topologies, and communication mechanisms.\\nMoreover, we also investigate domain-specific evolution strategies developed for specialised fields. In addition,\\nwe provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic\\n4\\n2023\\n2024\\nSingle-Agent Optimisation\\nDomain-specific Optimisation\\nUnified Optimisation\\n(topology, prompt, workflow,\\ntool, memory, etc.)\\nAgent-centric\\nTopology\\n& Workflow\\n2025\\nTEMPERA\\nOPRO\\nMacNet\\nPrompt\\nMemory\\nTool\\nMulti-Agent \\nOptimisation\\nScience & Tech \\nResearch\\nMedical\\nPrompt\\nAgent\\nAPOHF\\nMIPRO\\nStraGO\\nSPO\\nTextGrad\\nPrompt\\nBreeder\\nMemo\\nChat\\nCOMEDY\\nRead\\nAgent\\nMoT\\nStruc\\nRAG\\nMemory\\nBank\\nRefle\\nxion\\nA-Mem\\nMem\\nGPT\\nMem0\\nAWESOME\\nHippoRAG\\nGraph\\nReader\\nChatDB\\nAWM\\nMem1\\nTool\\nLLM\\nGPT4\\nTool\\nGlorilla\\nAlita\\nCreator\\nLATM\\nCRAFT\\nAgent\\nOptimizer\\nPyVision\\nReTool\\nToolRL\\nSwiRL\\nNem\\notron\\nEasyTool\\nPlay2\\nPrompt\\nAuto\\nAgents\\nMorph\\nAgent\\nEvo\\nAgent\\nADAS\\nAgent\\nSquare\\nGPT\\nSwarm\\nG-Desi\\ngner\\nDyLAN\\nMASS\\nMaAS\\nMAS-\\nZero\\nAutoFlow\\nG-Safe\\nguard\\nMermaid\\nFlow\\nCode\\nAgent\\nMedAg\\nentSim\\nPath\\nFinder\\nMDTeam\\nGPT\\nMedAgentPro\\nAgent\\nCourts\\nAgent\\nCourt\\nFlow\\nReasoner\\nEvoFlow\\nHetero\\nSwarms\\nAgentGro\\nupChat\\nAgent\\nOrchestra\\nWork-\\nforce\\nMemAgent\\nAPE\\nGRIPS\\nRetro\\nformer\\nEvo\\nPropmt\\nG-Memory\\nAFlow\\nAgent\\nPrune\\nDsPy\\nLawLuo\\nFinRobot\\nPEER\\nAgent\\nCoder\\nScoreFlow\\nMAS\\n-GPT\\nMAS\\nRouter\\nAgent\\nHospital\\nFinCon\\nMMedAgent\\nLLM-RDF\\nCACTUS\\nUni\\nDebug\\nSelf\\nDebugging\\nPyCap\\nsule\\nCode\\nCoR\\nMD\\nAgents\\nChem\\nAgent\\nLIDDIA\\nOSDA\\nAgent\\nDrug\\nAgent\\nOpen\\nDevin\\nFigure 2 A visual taxonomy of AI agent evolution and optimisation techniques, categorised into three major directions:\\nsingle-agent optimisation, multi-agent optimisation, and domain-specific optimisation. The tree structure illustrates the\\ndevelopment of these approaches from 2023 to 2025, including representative methods within each branch.\\nsystems, which are critical to ensuring their effectiveness and reliability. As a concurrent work, Gao et al.\\n(2025b) surveys self-evolving agents organised around three foundational dimensions: what to evolve, when to\\nevolve, and how to evolve. While their taxonomy offers valuable insights, our survey aims to provide a more\\ncomprehensive and integrative perspective, i.e., the unified conceptual framework, on the mechanisms and\\nchallenges associated with building lifelong, self-evolving agentic systems.\\nThis survey aims to provide a comprehensive and systematic review of existing techniques for self-evolving\\nagentic systems, thereby offering researchers and practitioners valuable insights and guidelines for developing\\nmore effective and sustainable agentic systems. Figure 2 presents a visual taxonomy of existing agent evolu-\\ntion strategies across single-agent, multi-agent, and domain-specific optimisation, highlighting representative\\napproaches in each direction. Our main contributions are as follows:\\nâ€¢ We formalise the Three Laws of Self-Evolving AI Agents and map the evolution of LLM-centric\\nlearning paradigms from static pretraining to fully autonomous, lifelong self-evolving agentic systems.\\nâ€¢ We introduce a unified conceptual framework that abstracts the feedback loop underlying self-evolving\\nagentic systems, and provides a foundation for systematically understanding and comparing different\\nevolution and optimisation approaches.\\nâ€¢ We conduct a systematic review of existing evolution and optimisation techniques across single-agent,\\nmulti-agent, and domain-specific settings.\\nâ€¢ We provide a comprehensive review of evaluation, safety, and ethical considerations for self-evolving agentic\\nsystems, emphasising their critical role in ensuring the effectiveness, safety, and responsible deployment of\\nthese systems.\\nâ€¢ We identify key open challenges and outline promising research directions in agent self-evolution, aiming to\\nfacilitate future exploration and advance the development of more adaptive, autonomous, and self-evolving\\n5\\nagentic systems.\\nThe remainder of this survey is organised as follows. Section 2 presents preliminaries on AI agents and\\nmulti-agent systems, including their definitions, key components, representative architectures, and the broader\\nvision of autonomous and self-evolving agent systems. Section 3 introduces a unified conceptual framework\\nfor agent evolution approaches, outlining the key elements such as system inputs, evolution objectives, agent\\nstructures, and optimisers. Section 4 focuses on the optimisation of single-agent systems. It discusses several\\nkey aspects such as the optimisation of reasoning strategies, prompt formulation, memory mechanisms, and tool\\nusage. Section 5 focuses on multi-agent systems and review methods for optimising agent workflows, topologies,\\nand inter-agent communication strategies. Section 6 highlights domain-specific agent optimisation techniques\\nand their applications, while Section 7 discusses evaluation methodologies and benchmarks for assessing agent\\nsystems. Section 8 presents existing challenges in the agent evolution and optimisation field and outlines some\\npromising future research directions. Finally, we conclude the survey in Section 9.\\n2\\nFoundation of AI Agent Systems\\nTo facilitate a clear understanding of agent evolution and optimisation, this section provides an overview\\nof existing AI agent systems. We begin by introducing single-agent systems in Section 2.1, outlining their\\ndefinitions and core components. We then turn to multi-agent systems (MAS) in Section 2.2, highlighting their\\nmotivations, structural paradigms, and collaboration mechanisms. Finally, we present the vision of lifelong,\\nself-evolving agentic systems in Section 2.3.\\n2.1\\nAI Agents\\nAn AI agent refers to an autonomous system capable of perceiving its inputs, reasoning about goals, and\\ninteracting with the environment to complete tasks (Luo et al., 2025a). In this section, we focus on single-agent\\nsystems, which serve as the foundation of AI agent research. While our goal here is to provide only a brief\\noverview, readers may refer to existing surveys for more comprehensive discussions of AI agent architectures\\nand capabilities (Guo et al., 2024c; Xi et al., 2025; Luo et al., 2025a; Liu et al., 2025a).\\nAn AI agent is typically composed of multiple components that work together to enable autonomous decision-\\nmaking and execution. The core component of an agent is the Foundation Model, most commonly an LLM2,\\nwhich serves as the central reasoning engine responsible for interpreting instructions, generating plans, and\\nproducing actionable responses. In addition, there are also some supporting modules that enhance the agentâ€™s\\nability in complex and dynamic environments:\\n(1) Perception Module. The perception module is responsible for acquiring and interpreting information from\\nthe environment (Li et al., 2024f). This includes processing textual inputs, audio signals, video frames, or\\nother sensory-like data to build a representation suitable for reasoning.\\n(2) Planning Module. The planning module enables the agent to decompose complex tasks into actionable\\nsub-tasks or sequences of operations and guide their execution across multiple steps (Huang et al., 2024b).\\nThis process facilitates hierarchical reasoning and ensures coherent task completion. One of the simplest\\nforms of planning involves linear task decomposition, where a problem is broken down into multiple\\nintermediate steps, and the LLM follows these steps to address the problem. This is exemplified by\\nmethods such as chain-of-thought prompting (Wei et al., 2022). Beyond static planning, more dynamic\\napproaches interleave planning and execution in an iterative loop. For instance, the ReAct (Yao et al.,\\n2023b) framework combines reasoning with actions, allowing the agent to revise its plans based on real-time\\nfeedback. In addition to linear planning, some methods adopt a branching strategy, where each step may\\nlead to multiple possible continuations. Representative examples are Tree-of-Thought (Yao et al., 2023a)\\nand Graph-of-Thought (Besta et al., 2024), which enable the agent to explore multiple reasoning paths.\\n(3) Memory Module. The memory module enables the agent to retain and recall past experience, enabling\\ncontext-aware reasoning and long-term consistency. Broadly, memory can be categorised into short-term\\nand long-term memory. Short-term memory typically stores the context and interactions generated during\\n2While this survey focuses on LLMs, the backbone can be any foundation model (e.g., visionâ€“language models, protein\\nsequence/structure models), and the core agentic principles we discuss readily generalise to such backbones.\\n6\\nthe execution of the current task. Once the task is completed, the short-term memory will be removed. In\\ncontrast, long-term memory persists over time and may store accumulated knowledge, past experiences,\\nor reusable information across tasks. To access relevant long-term memory, many agent systems adopt a\\nretrieval-augmented generation (RAG) module (Zhang et al., 2024d), where the agent retrieves relevant\\ninformation from the memory and incorporates them into the input context for the LLM. Designing an\\neffective memory module involves several challenges, including how to structure memory representations,\\nwhen and what to store, how to retrieve relevant information efficiently, and how to integrate it into the\\nreasoning process Zeng et al. (2024a). For a more comprehensive review of memory mechanisms in AI\\nagents, we refer readers to the survey by Zhang et al. (2024d).\\n(4) Tool Use. The ability to use external tools is a key factor for AI agents to effectively operate in real-world\\nscenarios. While LLMs are powerful in language understanding and generation, their capabilities are\\ninherently limited by their static knowledge and reasoning capabilities. By using external tools, agents can\\nextend their functional scope, allowing them to better interact with real-world environments. Typical tools\\ninclude web search engines (Li et al., 2025g), code interpreters or execution environments (Islam et al.,\\n2024), and browser automation framework (MÃ¼ller and Å½uniÄ, 2024). The design of the tool-use component\\noften involves selecting tools, constructing tool-specific inputs, invoking API calls, and integrating tool\\noutputs back into the reasoning process.\\n2.2\\nMulti-Agent Systems\\nWhile single-agent systems have demonstrated strong capabilities in various tasks, many real-world tasks\\ndemand specialisation and coordination that exceed the capabilities of a single agent. This limitation has\\nmotivated the development of Multi-Agent Systems (MAS), which mirror the distributed intelligence found in\\nbiological and social systems.\\nMAS are formally defined as a collection of autonomous agents that interact within a shared environment\\nto achieve goals that are beyond the capabilities of a single agent. In contrast to single-agent systems that\\nrely solely on individual reasoning and capabilities, MAS focuses on achieving collective intelligence through\\nstructured coordination and collaboration among different agents (Tran et al., 2025). A fundamental mechanism\\nenabling such coordination is the concept of agent topology, the structural configuration that defines how\\nagents are connected and communicate within the system. The topology determines the information flow and\\ncollaboration strategies among agents, directly influencing how tasks are distributed and executed. Therefore,\\nMAS is often realised as a multi-agent workflow, where the systemâ€™s topology orchestrates the interactions\\namong agents to accomplish complex, shared goals. The key insight is that when multiple agents collaborate\\nthrough such workflows, the systemâ€™s overall performance can exceed the sum of the individual capabilities of\\nall agents within the system (Lin et al., 2025; Luo et al., 2025a).\\nMAS brings several notable advantages over single-agent systems.\\nFirst, MAS can decompose complex\\ntasks into manageable sub-tasks and assign them to specialised agents, which is helpful to improve the\\noverall performance (Krishnan, 2025; Sarkar and Sarkar, 2025). This approach mirrors human organisational\\ncollaboration, enabling MAS to handle tasks that are beyond the capacity of a single agent. Second, MAS\\nsupports parallel execution, allowing multiple agents to work simultaneously to complete the task. This\\nfeature is particularly advantageous for time-sensitive applications, as it greatly accelerates the problem-solving\\nprocess (Zhang et al., 2025k; Liu et al., 2025a; Li et al., 2025h). Third, the decentralized nature of MAS\\nenhances robustness: when one agent fails, other agents can dynamically redistribute tasks and compensate for\\nthe failure, ensuring graceful degradation rather than a complete system breakdown (Huang et al., 2024a; Yang\\net al., 2025b). Fourth, MAS offers inherent scalability, as new agents can be seamlessly integrated without\\nredesigning the entire system (Han et al., 2024; Chen et al., 2025g). Finally, collaborative mechanisms like\\ndebate and iterative refinement enable MAS to generate more innovative and reliable solutions by leveraging\\ndiverse perspectives and critical evaluation among agents (Guo et al., 2024c; Lin et al., 2025). Frameworks such\\nas CAMEL and AutoGen have further streamlined the development of MAS by providing modular architectures,\\nrole-playing patterns, and automated orchestration capabilities that reduce engineering overhead (Li et al.,\\n2023a; Wu et al., 2024a).\\n7\\n2.2.1\\nSystem Architecture\\nThe architectural design of MAS fundamentally determines how agents organise, coordinate, and execute\\ntasks. These structures range from rigid hierarchies to flexible peer-to-peer networks, each embodying different\\nphilosophies about control, autonomy, and collaboration.\\n(1) Hierarchical Structure. These systems employ static hierarchical organisations, typically linear or tree-\\nbased, where tasks are explicitly decomposed and sequentially assigned to specific agents. For instance,\\nMetaGPT (Hong et al., 2024) introduces Standard Operating Procedures (SOPs) to streamline software\\ndevelopment, while HALO (Hou et al., 2025) incorporates Monte Carlo Tree Search to enhance reasoning\\nperformance. This highly customised approach offers modularity, ease of development, and domain-specific\\noptimisation, making it prevalent in software development, medicine, scientific research, and social\\nsciences (Zheng et al., 2023b; Park et al., 2023; Qian et al., 2024; Li et al., 2024c; Cheng et al., 2025).\\n(2) Centralised Structure. This architecture follows a manager-follower paradigm where a central agent\\nor higher-level coordinator handles planning, task decomposition, and delegation, while subordinate\\nagents execute assigned subtasks. This design effectively balances global planning with specific task\\nexecution (Fourney et al., 2024; Roucher et al., 2025; CAMEL-AI, 2025). However, the central node\\ncreates performance bottlenecks and introduces single-point-of-failure vulnerabilities that compromise\\nsystem robustness (Ko et al., 2025).\\n(3) Decentralised Structure.\\nIn this architecture, agents collaborate as peers in a distributed network,\\nwidely adopted in world simulation applications. The absence of central control prevents single-point\\nfailuresâ€”damage to any node does not paralyse the entire system, eliminating bottlenecks and enhancing\\nrobustness (Lu et al., 2024b; Yang et al., 2025b). However, this introduces challenges in information\\nsynchronisation, data security, and increased collaboration costs (Ko et al., 2025). Recent work explores\\nblockchain technology to address these coordination challenges (Geren et al., 2024; Yang et al., 2025d).\\n2.2.2\\nCommunication Mechanisms\\nThe effectiveness of MAS largely depends on how agents exchange information and coordinate actions. Commu-\\nnication methods in MAS have evolved from simple message passing to sophisticated protocols that balance\\nexpressiveness, efficiency, and interoperability.\\n(1) Structured Output. This approach employs formats like JSON (Li et al., 2024e; Chen et al., 2025g),\\nXML (Zhang et al., 2025b; Kong et al., 2025), and executable code (Roucher et al., 2025) for inter-agent\\ncommunication. The explicit structure and well-defined parameters ensure high machine readability and\\ninterpretability, while standardised formats facilitate cross-platform collaboration (Chen et al., 2025g).\\nThese characteristics make structured communication ideal for applications demanding precision and\\nefficiency, such as problem-solving and reasoning tasks. The compact information representation further\\nenhances computational efficiency (Wang et al., 2024h).\\n(2) Natural Language. Natural language communication preserves rich contextual and semantic details, making\\nit particularly suitable for creative tasks, world simulation, and creative writing scenarios (Liu et al.,\\n2025a). This expressiveness enables nuanced interactions that capture subtle meanings and intentions.\\nHowever, it introduces challenges including ambiguity, potential misinterpretation, and reduced execution\\nefficiency compared to structured formats (Guo et al., 2024c; Yang et al., 2025c; Kong et al., 2025).\\n(3) Standardised Protocols. Recent advances have introduced specialised protocols designed to standardise\\nMAS communication, creating more inclusive and interoperable agent ecosystems: A2A (LLC and\\nContributors) standardises horizontal communication through a structured, peer-to-peer task delegation\\nmodel, enabling agents to collaborate on complex, long-running tasks while maintaining execution opacity.\\nANP (Chang and Contributors) implements secure, open horizontal communication for a decentralised\\n\"agent internet\" through a hierarchical architecture with built-in Decentralised Identity (DID) and\\ndynamic protocol negotiation.\\nMCP (PBC and Contributors) standardises vertical communication\\nbetween individual agents and external tools or data resources through a unified client-server interface.\\nAgora (Marro and Contributors) functions as a meta-protocol for horizontal communication, enabling\\nagents to dynamically negotiate and evolve their communication methods, seamlessly switching between\\nflexible natural language and efficient structured routines.\\n8\\n2.3\\nThe Vision of Lifelong, Self-Evolving Agentic Systems\\nThe trajectory from Model Offline Pretraining (MOP) through Model Online Adaptation (MOA) and Multi-\\nAgent Orchestration (MAO) has steadily reduced the degree of manual configuration in LLM-based systems.\\nYet, even the most advanced multi-agent frameworks today often depend on handcrafted workflows, fixed\\ncommunication protocols, and human-curated toolchains (Talebirad and Nadiri, 2023; Zhao et al., 2024; Luo\\net al., 2025a; Tran et al., 2025). These static elements constrain adaptability, making it difficult for agents to\\nsustain performance in dynamic, open-ended environments where requirements, resources, and goals evolve over\\ntime.\\nThe emerging paradigm of Multi-Agent Self-Evolving (MASE) systems addresses these limitations by closing the\\nloop between deployment and continual improvement. In a MASE system, a population of agents is equipped\\nto autonomously refine their prompts, memory, tool-use strategies, and even their interaction topology â€“ guided\\nby feedback from the environment and higher-level meta-rewards (Novikov et al., 2025; Zhang et al., 2025i).\\nThis continuous optimisation process enables agents not merely to adapt once, but to evolve over their lifetime\\nin response to shifting tasks, domains, and operational constraints.\\nLifelong, self-evolving agentic systems aim to overcome these constraints by embedding a continuous improvement\\nloop into the core of the architecture. Guided by the Three Laws of Self-Evolving AI Agents â€“ Endure\\n(safety adaptation), Excel (performance preservation), and Evolve (autonomous optimisation) â€“ these systems\\nare designed to:\\n(I) Monitor their own performance and safety profile during operation;\\n(II) Preserve or enhance capabilities through controlled, incremental updates;\\n(III) Autonomously adapt prompts, memory structures, tool-use strategies, and even inter-agent topologies in\\nresponse to shifting tasks, environments, and resources.\\nRather than requiring human designers to handcraft every interaction pattern, a lifelong self-evolving system\\ncan generate, evaluate, and refine its own agentic configurations, closing the loop between environment feedback,\\nmeta-level reasoning, and structural adaptation. This transforms agents from static executors into continually\\nlearning, co-evolving participants in their operational ecosystems.\\nThis vision has far-reaching implications. In scientific discovery, self-evolving agent ecosystems could au-\\ntonomously generate hypotheses, design experiments, and iterate on research workflows. In software engineering,\\nthey could co-evolve development pipelines, integrating new tools as they emerge. In humanâ€“AI collaboration,\\nthey could learn individual preferences and continually personalise interaction styles. Extending beyond the\\ndigital realm, such systems could interface with the physical world through robotics, IoT devices, and cy-\\nberâ€“physical infrastructures, perceiving environmental changes, acting upon them, and incorporating real-world\\nfeedback into their evolutionary loop. By treating agents as reconfigurable computational entities capable of\\nself-evolving, coordination, and long-term adaptation, MASE offers a pathway toward scalable, sustainable, and\\ntrustworthy AI â€“ AI that is not just trained once, but that lives, learns, and lasts.\\n3\\nA Conceptual Framework of MASE\\nTo provide a comprehensive overview of self-evolving agentic systems, we propose a high-level conceptual\\nframework that abstracts and summarises the key elements underlying the design and implementation of agent\\nevolution and optimisation methods. This framework provides an abstract yet generalisable view of most\\nexisting optimisation approaches, thereby enabling a comprehensive understanding of the field and facilitating\\ncomparative analysis across different approaches.\\n3.1\\nOverview of the Self-Evolving Process\\nWe begin with an overview of the self-evolving process in agent systems, which in practice is often realised\\nthrough iterative optimisation. In this process, the agent system is iteratively updated based on feedback\\nsignals obtained from performance evaluations and environmental interactions. As illustrated in Figure 3, the\\nprocess begins with a task specification, which may include a high-level description, input data, contextual\\ninformation, or concrete examples. These elements constitute the system inputs, which define the problem\\n9\\n- Setup -\\n- Mutation -\\n- Refine -\\nEnvironment\\nScenarios\\n- Feedback -\\nSystem Inputs\\nTask-level\\nInstance-level\\nAgent System\\nSingle-agent\\nMulti-agent\\nPrompt\\nTools\\nMemory\\nAgents\\nCommunication\\nTopology\\nTopology\\nOptimiser\\nSearch Space\\nOptimisation Algorithm\\nPrompt Template\\nTool Selection\\nLLM Parameters\\nArchitectural\\nRule-base Heuristics\\nGradient Descent\\nBayesian & MCTS & RL\\nLearning-based Policy\\nProxy Metrics\\nAccuracy, F1\\nSuccess rate\\nLLM-based\\xa0\\nEvaluators\\nCoding\\nLegal\\nResearch\\nMedicine\\n- Execution -\\nFigure 3 Conceptual framework of the self-evolving process in agent systems. The process forms an iterative optimisation\\nloop comprising four components: System Inputs, Agent System, Environment, and Optimiser. System inputs define the\\ntask setting (e.g., task-level or instance-level). The agent system (in single- or multi-agent form) executes the specified\\ntask. The environment (depending on different scenarios) provides feedback via proxy metrics. The optimiser updates\\nthe agent system through a defined search space and optimisation algorithm until performance goals are met.\\nsetting for the agent system. The agent system, either following a single-agent or multi-agent architecture, is\\nthen deployed to perform the task within an environment. The environment provides the operating context and\\ngenerates feedback signals, which are derived from predefined evaluation metrics, that measure the systemâ€™s\\neffectiveness and guide subsequent optimisation. Based on feedback from the environment, the optimiser\\napplies specific algorithms and strategies to update the agent system, such as adjusting the LLM parameters,\\nmodifying prompts, or refining the systemâ€™s structure. In some cases, the optimiser may also refine the system\\ninputs by synthesising training examples to augment existing datasets, thereby expanding the data available for\\nsubsequent optimisation cycle. The updated agent system is then redeployed to the environment, initialising the\\nnext iteration. This process forms an iterative, closed feedback loop in which the agent system is progressively\\nrefined and optimised over multiple iterations. The loop terminates once a predefined performance threshold is\\nreached or convergence criteria are satisfied. Building on the conceptual framework of MASE, EvoAgentX is\\nthe first open-source framework to apply this self-evolving agent process, designed to automate the generation,\\nexecution, evaluation, and optimisation of agentic systems (Wang et al., 2025i).\\nBuilding on the overview above, there are four key components within the agent optimisation process: system\\ninputs, agent systems, environment and optimisers. In what follows, we provide an introduction to each\\ncomponent, highlighting their individual roles, characteristics and interactions within the optimisation framework.\\n3.2\\nSystem Inputs\\nSystem inputs refer to the contextual information and data provided to the optimisation process. Formally, we\\ndenote the set of system inputs as I, which may consist of one or more elements that specify task requirements,\\nconstraints, and available data. These inputs define the problem setting for the agent system and determine\\nthe scope of optimisation. Depending on the scenario, I can take different forms:\\n10\\nâ€¢ Task-Level Optimisation. The most common setting in existing research focuses on improving the agent\\nsystemâ€™s overall performance on a specific task. In this case, the system inputs I may include a task\\ndescription T and a training dataset Dtrain used for training or validation: I = {T , Dtrain}. A separate\\ntest dataset Dtest can also be incorporated to evaluate the optimised agentâ€™s performance. In some\\nscenarios, task-specific labeled data, i.e., Dtrain, are unavailable. To enable optimisation in such settings,\\nrecent approaches (Huang et al., 2025; Zhao et al., 2025a; Liu et al., 2025b) propose to dynamically\\nsynthesise training examples, often through LLM-based data generation, to create a surrogate dataset for\\niterative improvement.\\nâ€¢ Instance-Level Optimisation. Recent studies also explore a more fine-grained setting, where the objective\\nis to enhance the agent systemâ€™s performance on a specific example (Sun et al., 2024a; Novikov et al.,\\n2025). In this case, the system inputs may consist of an input-output pair (x, y), along with optional\\ncontextual information C, i.e., I = {x, y, C}.\\n3.3\\nAgent Systems\\nThe agent system is the core component within the feedback loop that is subject to optimisation. It defines\\nthe decision-making process and functionality of the agent(s) in response to given inputs. Formally, we denote\\nthe agent system as A, which may consist of a single agent or a collection of collaborating agents. The agent\\nsystem A can be further decomposed into several components, such as the underlying LLM, prompting strategy,\\nmemory module, tool-use policy, etc. Optimisation methods may focus on one or more of these components\\ndepending on the intended scope. In most existing works, optimisation is performed on a single component of\\nA, such as finetuning the LLM to enhance reasoning and planning capabilities (Zelikman et al., 2022; Tong\\net al., 2024; Lai et al., 2024b), or tuning the prompts and selecting appropriate tools to improve task-specific\\nperformance without modifying the LLM itself (Yang et al., 2024a; Yuan et al., 2025b). Moreover, recent\\nresearch has also explored joint optimisation of multiple components with A. For example, in single-agent\\nsystems, some approaches jointly optimise LLM and prompting strategy to better align model behaviour\\nwith task requirements (Soylu et al., 2024). In multi-agent systems, existing studies have explored the joint\\noptimisation of prompts and inter-agent topology to improve overall effectiveness (Zhang et al., 2025j; Zhou\\net al., 2025a).\\n3.4\\nEnvironments\\nThe environment serves as the external context in which the agent system operates and generates outputs.\\nSpecifically, the agent system interacts with the environment by perceiving its inputs, executing actions, and\\nreceiving corresponding outcomes. Depending on the task, the environment can vary from a benchmark dataset\\nto a fully dynamic, real-world setting (Liu et al., 2023a). For example, in code generation tasks, the environment\\nmay include code execution and verification components such as compilers, interpreters, and test cases. In\\nscientific research, it may consist of literature databases, simulation platforms, or laboratory equipment.\\nBeyond providing the operational context, the environment also plays a critical role in generating feedback signals\\nthat inform and guide the optimisation process. These signals are typically derived from evaluation metrics\\nthat quantify the effectiveness or efficiency of the agent system. In most cases, such metrics are task-specific,\\ne.g., accuracy, F1, or success rate, which provide quantitative measures of performance. However, in settings\\nwhere labelled data or ground-truth are unavailable, LLM-based evaluators are often employed to estimate\\nperformance (Yehudai et al., 2025). These evaluators can generate proxy metrics or provide textual feedback\\nby assessing aspects such as correctness, relevance, coherence, or alignment with task instructions. A more\\ndetailed discussion of evaluation strategies across different applications is presented in Section 7.\\n3.5\\nOptimisers\\nOptimisers (P) are the core component of the self-evolving feedback loop, responsible for refining the agent\\nsystem A based on performance feedback from the environment. Their objective is to search, via specialised\\nalgorithms and strategies, for the agent configuration that achieves the best performance under the given\\nevaluation metric. Formally, this can be expressed as:\\nAâˆ—= arg max\\nAâˆˆS O(A; I),\\n(1)\\n11\\nwhere S denotes the search space of configurations, O(A; I) âˆˆR is the evaluation function that maps the\\nperformance of A on the given system inputs I to a scalar score, and Aâˆ—denotes the optimal agent configuration.\\nAn optimiser is typically defined by two core components: (1) search space (S): This defines the set of agent\\nconfigurations that can be explored and optimised. The granularity of S depends on which part(s) of the\\nagent system are subject to optimisation, ranging from agent prompts or tool selection strategies to continuous\\nLLM parameters or architectural structures. (2) optimisation algorithm (H): This specifies the strategy used to\\nexplore S and select or generate candidate configurations. It can include rule-based heuristics, gradient descent,\\nBayesian optimisation, Monte Carlo Tree Search (MCTS), reinforcement learning, evolutionary strategies, or\\nlearning-based policies. Together, the pair (S, H) defines the behaviour of the optimiser and determines how\\nefficiently and effectively it can adapt the agent system toward better performance.\\nIn the following sections, we introduce typical optimisers in three different settings: single-agent systems\\n(Section 4), multi-agent systems (Section 5), and domain-specific agent systems (Section 6). Each setting\\nexhibits distinct characteristics and challenges, leading to different designs and implementations of optimisers.\\nIn single-agent optimisation, the focus is on improving an individual agentâ€™s performance by tuning LLM\\nparameters, prompts, memory mechanisms, or tool-use policies. In contrast, multi-agent optimisation extends\\nthe scope to optimising not only individual agents but also their structural designs, communication protocols, and\\ncollaboration capabilities. Domain-specific agent optimisation presents additional challenges, where optimisers\\nmust account for specialised requirements and constraints inherent to particular domains, leading to tailored\\noptimiser designs. A comprehensive hierarchical categorisation of these optimisation settings and representative\\nmethods is provided in Figure 5.\\n4\\nSingle-Agent Optimisation\\nPrompt Optimisation\\nâ¦¿ Edit-Based Optimisation\\nâ¦¿ Generative Optimisation\\nâ¦¿\\xa0Text Gradient-Based\\xa0Optimisation\\nâ¦¿\\xa0Evolutionary\\xa0Optimisation\\nMemory Optimisation\\nâ¦¿ Short-term Memory Optimisation\\nâ¦¿ Long-term Memory Optimisation\\nTool Optimisation\\nâ¦¿ Training-Based Optimisation\\nâ¦¿ Inference-Time Optimisation\\nâ¦¿\\xa0Prompt-Based Tool Optimisation\\nâ¦¿\\xa0Reasoning-Based Tool Optimisation\\nSingle-agent\\nMemory\\nTools\\nLLM\\nPrompt\\nLLM Behaviour Optimisation\\nâ¦¿ Reasoning Behaviour Optimisation\\nâ¦¿ Test-Time Scaling\\xa0Optimisation\\n\\xa0 \\xa0 \\xa0â€¢\\xa0Feedback-based Strategy\\n\\xa0 \\xa0 \\xa0â€¢\\xa0Search-based Strategy\\nFigure 4 An overview of single-agent optimisation approaches, categorised by the targeted component within the agent\\nsystem: prompt, memory, and tool.\\nSingle-agent optimisation focuses on improving the performance of a single-agent system. According to the\\noptimisation feedback loop introduced earlier, the key challenge lies in the design of optimisers for updating\\nthe system. This involves identifying the specific components of the agent system to optimise (i.e., search\\nspace), determining the particular capabilities to enhance, and choosing appropriate optimisation strategies to\\neffectively achieve these improvements (i.e., optimisation algorithm).\\nIn this section, we organise single-agent optimisation approaches based on the targeted component within the\\nagent system, as this determines both the structure of the search space and the choice of optimisation methods.\\nSpecifically, we focus on four major categories: (1) LLM Behaviour optimisation, which aims to improve the\\nLLMâ€™s reasoning and planning capabilities through either parameter tuning or test-time scaling techniques; (2)\\nPrompt optimisation, which focuses on adapting prompts to guide the LLM towards producing more accurate\\nand task-relevant outputs; (3) Memory optimisation, which aims to enhance the agentâ€™s ability to store, retrieve,\\nand reason over historical information or external knowledge; (4) Tool optimisation, which focuses on enhancing\\nthe agentâ€™s ability to effectively leverage existing tools, or autonomously create or configure new tools to\\naccomplish complex tasks. Figure 4 shows the major categories of single-agent optimisation approaches.\\n12\\nAgentic\\nSelf-Evolution\\nSingle-Agent\\nOptimisation\\nBehaviour\\nOptimisation\\nSFT\\nSTaR (Zelikman et al., 2022); ToRA (Gou\\net al., 2024); NExT (Ni et al., 2024);\\nRL\\nSelf-Rewarding (Yuan et al., 2024d); DeepSeek-Prover-\\nv1.5 (Xin et al., 2025); Absolute-Zero (Zhao et al., 2025a)\\nVerifier Module\\nBaldur (First et al., 2023); Math-Shepherd (Wang\\net al., 2024d); Rewarding Progress (Setlur et al., 2025);\\nSearch-Based\\nCoT-SC (Wang et al., 2023b); Tree-of-Thoughts (Yao\\net al., 2023a); Graph-of-Thoughts (Besta et al., 2024);\\nPrompt\\nOptimisation\\nEdit-Based\\nGPS (Xu et al., 2022); GrIPS (Prasad et al.,\\n2023); TEMPERA (Zhang et al., 2023b);\\nGeneration-Based\\nAPE (Zhou et al., 2023c); PromptAgent (Wang et al., 2024i);\\nOPRO (Yang et al., 2024a); APOHF (Lin et al., 2024a);\\nRETROFORMER (Yao et al., 2024); MIPRO (Opsahl-Ong\\net al., 2024); StraGo (Wu et al., 2024b); SPO (Xiang et al., 2025);\\nText Gradient-\\nBased\\nProTeGi (Pryzant et al., 2023); TextGrad (Yuksekgonul et al., 2024);\\nEvolution-Based\\nEvoPrompt (Guo et al., 2024b); Promptbreeder (Fernando et al., 2024);\\nMemory\\nOptimisation\\nShort-term\\nMemory\\nCOMEDY (Chen et al., 2025f);ReadAgent (Lee et al., 2024d); MoT (Li and\\nQiu, 2023);StructRAG (Li et al., 2025i);MemoryBank (Zhong et al., 2024);\\nLong-term\\nMemory\\nEWE (Chen et al., 2025d); A-MEM (Xu et al., 2025);\\nMem0 (Chhikara et al., 2025); GraphReader (Li et al., 2024d);\\nAWM (Wang et al., 2024l); MyAgent (Hou et al., 2024);\\nTool Optimisation\\nTraining-Based\\nToolLLM (Qin et al., 2024); Confucius (Gao et al., 2024a); Re-\\nTool (Feng et al., 2025a); ToolRL (Qian et al., 2025a); SWiRL (Goldie\\net al., 2025); Nemotron-Research-Tool-N1 (Zhang et al., 2025m);\\nPrompt-Based\\nEASYTOOL (Yuan et al., 2025b); PLAY2PROMPT (Fang et al.,\\n2025); DRAFT (Qu et al., 2025); JointOptim (Wu et al., 2025a);\\nTool Creation\\nCREATOR (Qian et al., 2023); LATM (Cai et al., 2024); CRAFT (Yuan\\net al., 2024a); AgentOptimizer (Zhang et al., 2024b); Alita (Qiu et al., 2025);\\nMulti-Agent\\nOptimisation\\nPrompt Op-\\ntimisation\\nAutoAgents (Chen et al., 2024b); DSPy (Singhvi et al., 2023);\\nMIPRO (Opsahl-Ong et al., 2024); PromptWizard (Agarwal et al., 2024)\\nTopology\\nOptimisation\\nCode-level\\nWorkflow\\nAutoFlow (Li et al., 2024h); AFlow (Zhang et al., 2025j);\\nScoreFlow (Wang et al., 2025j); MAS-GPT (Ye et al., 2025);\\nCommunication\\nGraph\\nGPTSwarm (Zhuge et al., 2024a); DynaSwarm (Leong and Wu,\\n2025); G-Designer (Zhang et al., 2024a); NetSafe (Yu et al.,\\n2024a); AgentPrune (Zhang et al., 2025g); AGP (Li et al., 2025a);\\nUnified\\nOptimisation\\nCode-Based\\nADAS (Hu et al., 2025a); FlowReasoner (Gao et al., 2025a);\\nSearch-Based\\nEvoAgent (Yuan et al., 2025a); MASS (Zhou et al., 2025a); DebFlow (Su\\net al., 2025); EvoFlow (Gao et al., 2025a); MAS-ZERO (Ke et al., 2025);\\nLearning-Based\\nMaAS (Zhang et al., 2025f); ANN (Ma et al., 2025);\\nLLM Backbone\\nOptimisation\\nReasoning-\\nOriented\\nAutoFlow (Li et al., 2024h); AFlow (Zhang et al., 2025j);\\nScoreFlow (Wang et al., 2025j); MAS-GPT (Ye et al., 2025);\\nCollaboration-\\nOriented\\nCOPPER (Bo et al., 2024); OPTIMA (Chen\\net al., 2025h); MaPoRL (Park et al., 2025);\\nDomain-Specific\\nOptimisation\\nBiomedicine\\nMedical Diagnosis\\nMedAgentSim (Almansoori et al., 2025); PathFinder (Ghezloo et al.,\\n2025); MDAgents (Kim et al., 2024); MDTeamGPT (Chen et al., 2025c);\\nMMedAgent (Li et al., 2024a); MedAgent-Pro (Wang et al., 2025l);\\nMolecular\\nDiscovery\\nCACTUS (McNaughton et al., 2024); LLM-RDF (M. Bran et al.,\\n2024); ChemAgent (Tang et al., 2025a); OSDA Agent (Hu et al.,\\n2025c); DrugAgent (Inoue et al., 2025);LIDDIA (Averly et al., 2025);\\nProgramming\\nCode Refinement\\nSelf-Refine (Madaan et al., 2023); AgentCoder (Huang et al.,\\n2023a); CodeAgent (Tang et al., 2024); CodeCoR (Pan\\net al., 2025b); OpenHands (Wang et al., 2025g);\\nCode Debugging\\nSelf-Debugging (Chen et al., 2024c); Self-Edit (Zhang et al.,\\n2023a); PyCapsule (Adnan et al., 2025); RGD (Jin et al., 2024);\\nFinancial and\\nLegal Research\\nFinancial\\nDecision-Making\\nFinCon (Yu et al., 2024b); PEER (Wang\\net al., 2024j); FinRobot (Yang et al., 2024b);\\nLegal Reasoning\\nLawLuo (Sun et al., 2024b);AgentCourt (Chen\\net al., 2025a);LegalGPT (Shi et al., 2024b);\\nFigure 5 A comprehensive hierarchical categorisation of Agentic Self-Evolution methods, encompassing single-agent,\\nmulti-agent and domain-specific optimisation categories, illustrated with selected representative works.\\n13\\n4.1\\nLLM Behaviour Optimisation\\nBackbone LLMs lay the foundation for single-agent systems, serving as the primary component responsible\\nfor planning, reasoning, and task execution. Therefore, enhancing the planning and reasoning capabilities of\\nthe LLM is central to improving the overall effectiveness of the agent system. Recent efforts in this direction\\nbroadly fall into two categories: (1) training-based methods, which directly update the modelâ€™s parameters to\\nimprove reasoning ability and task performance; (2) test-time methods, which aim to improve LLMâ€™s behaviour\\nduring inference without modifying its parameters. In the following, we review and summarise representative\\napproaches from both categories.\\n4.1.1\\nTraining-Based Behaviour Optimisation\\nWhile LLMs have demonstrated strong linguistic capabilities (Zhao et al., 2023), recent research (Wu et al.,\\n2024c) highlights a notable gap between their fluency in natural language and their ability to perform complex\\nreasoning. This discrepancy limits the effectiveness of LLM-based agents in tasks that require multi-step\\ninference and complex decision-making. To address this, recent work has explored reasoning-oriented training\\nmethods, using supervised fine-tuning (SFT) and reinforcement learning (RL) to help models systematically\\nevaluate and refine their responses.\\nSupervised Fine-tuning.\\nThe core idea of supervised fine-tuning is to train agents using annotated data that\\ncontains detailed reasoning steps, allowing the model to learn a complete mapping from the input question,\\nthrough intermediate reasoning processes, to the final answer. This approach typically relies on carefully\\nconstructed reasoning trajectories, which can typically be constructed from (1) rollouts generated by the agent\\nitselfduringexecution, and (2) demonstrationsproducedbystrongerteacheragents. By imitating these trajectories,\\nthe agent acquires the ability to perform step-by-step reasoning in a structured manner. STaR (Zelikman\\net al., 2022) proposes an iterative fine-tuning procedure, where the model is trained on instances it has solved\\ncorrectly and refines incorrect traces to generate better trajectories. Based on this idea, NExT (Ni et al., 2024)\\nuses self-generated trajectories filtered by unit test correctness to self-evolve agents for program repair tasks.\\nSimilarly, Deepseek-Prover (Xin et al., 2024) progressively evolve the agent by iteratively training the policy\\nmodel with verified proofs, enabling it to generate increasingly accurate formal proofs for theorem-proving tasks.\\nAnother line of work fine-tunes agents on trajectories generated by proprietary LLMs, across domains such\\nas mathematics (Gou et al., 2024; Yin et al., 2024) and science (Ma et al., 2024). Beyond agentic capability,\\nMin et al. (2024); Huang et al. (2024c); Labs (2025) train models based on trajectories generated by OpenAI\\no1 (Jaech et al., 2024) to replicate its thinking capability, aiming to further improve the agent backboneâ€™s\\nreasoning ability.\\nReinforcement Learning.\\nRL treats reasoning as a sequential decision-making process where the model is\\nrewarded for producing correct or high-quality reasoning paths. One of the strategies is preference-based\\noptimisation, where DPO (Rafailov et al., 2023) is applied using preference pairs generated from various sources,\\nsuch as test case performance, correctness of final outcomes, or pseudo-labels from trained process reward\\nmodels (PRMs) (Hui et al., 2024; Min et al., 2024; Jiao et al., 2024; Liu et al., 2025f). Yuan et al. (2024d)\\nfurther introduce a self-evolving framework where the policy model uses its own judgments to iteratively refine\\nits reasoning ability. Similarly, Agent Q (Putta et al., 2024) combines MCTS-guided search and a self-critique\\nmechanism to iteratively improve agentsâ€™ decision making in web environments via DPO, leveraging both\\nsuccessful and failed trajectories. In another line of work, TÃ¼lu 3 (Lambert et al., 2024) applies reinforcement\\nlearning with verifiable rewards across mathematical and instruction-following tasks without any learned reward\\nmodel. Notably, DeepSeek-R1 (Guo et al., 2025) further demonstrates the feasibility of pure RL with Group\\nRelative Policy Optimisation (Shao et al., 2024) when ground-truth verification is possible. Building on this\\ndirection, Xin et al. (2025) extend the idea to enhance DeepSeek-Prover by incorporating reinforcement learning\\nfrom proof assistant feedback. Liu et al. (2025e) further explore self-evolving training in the multimodal setting\\nby introducing MSTAR, a framework that leverages RL to overcome performance saturation and enhance\\nreasoning capabilities through iterative self-improvement. Beyond using verifiable rewards in a fixed dataset,\\nAbsolute Zero (Zhao et al., 2025a) trains a single model that alternates between task proposer and solver roles,\\nself-evolving by generating and solving its own problems. Similarly, R-Zero (Huang et al., 2025) employs a\\ndual-mode framework in which a challenger generates tasks tailored to the solverâ€™s current competence, enabling\\nboth to evolve iteratively without external supervision.\\n14\\n4.1.2\\nTest-Time Behaviour Optimisation\\nAs training resources become increasingly constrained and API-based models cannot be fine-tuned, test-time\\ncompute emerges as a solution to these limitations by enabling models to refine or extend their reasoning\\ncapabilities during inference without additional training. By increasing the inference budget, models are able to\\nâ€œthink longerâ€.\\nScaling test-time capabilities can be achieved through two primary strategies. The first involves guiding inference\\nthrough the incorporation of external feedback, which facilitates the modelâ€™s refinement of its responses. The\\nsecond strategy focuses on generating multiple candidate outputs using more efficient sampling algorithms.\\nThis is followed by a selection process where a verifier identifies the most suitable output. Notably, these two\\napproaches are in fact closely related. The feedback used to guide generation in the former can naturally serve\\nas a verifier in the latter.\\nFeedback-based Strategy.\\nA natural method is to adjust a modelâ€™s behaviour based on the quality of its\\ngenerated outputs. This process typically relies on feedback from a verifier, which provides either an exact or\\nestimated score to guide the model. We categorise feedback into two types. Outcome-level feedback provides a\\nsingle score or signal based on the final output, regardless of the number of reasoning steps taken. For tasks\\nwhere ground-truth is easily accessible, the verifier can be implemented as an external tool to provide accurate\\nfeedback. For example, CodeT (Chen et al., 2023) and LEVER (Ni et al., 2023) leverage a compiler to execute\\nthe generated code and validate its correctness against test cases. START (Li et al., 2025c) and CoRT (Li et al.,\\n2025b) employ hint-based tool invocation to enhance long CoT reasoning. Similarly, Baldur (First et al., 2023)\\nleverages error messages from proof assistants to further repair incorrect proofs generated by LLMs. However,\\nfor most tasks, ground-truth is not always available at inference time. As a result, a more general approach\\nis to train a model to serve as the verifier that assigns a score to each candidate response (Liu et al., 2024a,\\n2025c), allowing them to be ranked based on predicted quality. However, this form of feedback is relatively\\nsparse, as it evaluates only the final output. In contrast, step-level feedback evaluates each intermediate step\\nduring the generation process, offering finer-grained supervision. Relying solely on outcome feedback often\\nleads to the unfaithful reasoning problem (Turpin et al., 2023), where incorrect reasoning chains may still result\\nin correct final answers. To address this, recent work (Wang et al., 2024d; Jiao et al., 2024; Setlur et al., 2025)\\nincreasingly focuses on training process reward models to detect and correct errors throughout the reasoning\\nprocess, generally yielding better improvement than using outcome-level feedback.\\nSearch-based Strategy.\\nComplex reasoning tasks often admit multiple valid paths leading to the correct solution.\\nSearch-based approaches take advantage of this property by exploring several candidate reasoning trajectories in\\nparallel, enabling the model to better navigate the solution space. With the help of critic models, various search\\nstrategies have been developed to guide the decoding process. For example, CoT-SC (Wang et al., 2023b) adopts\\na best-of-N approach: it generates multiple reasoning paths and selects the final answer based on the majority\\nvote over outcomes. DBS (Zhu et al., 2024) proposes the use of beam search in combination with step-level\\nfeedback to refine intermediate reasoning steps, while CoRe (Zhu et al., 2023) and Tree-of-Thoughts (Yao et al.,\\n2023a) explicitly model the reasoning process as a tree structure, using Monte Carlo Tree Search (MCST)\\nfor a balance between exploration and exploitation during searching. Forest-of-Thought (Bi et al., 2025)\\nfurther generalises this idea by enabling multiple trees to make independent decisions and applying a sparse\\nactivation mechanism to filter and select outputs from the most relevant trees. Beyond tree-based methods,\\nother approaches have also explored alternative structural formulations of reasoning. Graph-of-Thoughts (Besta\\net al., 2024) organises intermediate thoughts as nodes in a graph and applies graph-based operations to support\\nflexible reasoning and information flow. Buffer-of-Thoughts (Yang et al., 2024c) introduces a dynamic memory\\nbuffer to store and instantiate meta-level thoughts during reasoning.\\n4.2\\nPrompt Optimisation\\nIn single-agent systems, prompts play a critical role in defining the agentâ€™s goals, behaviour, and task-specific\\nstrategies. They typically contain instructions, illustrative demonstrations, and contextual information that\\nguide the underlying LLM in generating appropriate outputs. However, it is well-known that LLMs are highly\\nsensitive to prompts; even minor variations in phrasing, formatting, or word ordering can lead to significant\\nchanges in the LLMsâ€™ behaviour and output (Loya et al., 2023; Zhou et al., 2024b). This sensitivity makes it\\n15\\ndifficult to design robust and generalisable AI agent systems, motivating the development of prompt optimisation\\ntechniques to automatically search for high-quality prompts. Prompt optimisation methods can be categorised\\nbased on the strategies used to navigate the prompt space and identify high-quality prompts that enhance model\\nperformance. In this section, we review and summarise four representative categories: edit-based methods,\\ngenerative methods, text gradient-based methods, and evolutionary methods.\\n4.2.1\\nEdit-Based Prompt Optimisation\\nEarlier attempts in prompt optimisation focus on edit-based approaches, which iteratively refine human-written\\nprompts through predefined editing operations, such as token insertion, deletion or substitution (Prasad et al.,\\n2023; Pan et al., 2024a; Lu et al., 2024c; Zhang et al., 2023b; Zhou et al., 2023a; Agarwal et al., 2024). These\\nmethods treat prompt optimisation as a local search problem over prompt space, aiming to gradually improve\\nprompt quality while preserving the core semantics of the original instruction. For example, GRIPS (Prasad\\net al., 2023) splits instructions into phrases and applies phrase-level edit operations: delete, swap, paraphrase,\\nand addition, to progressively improve prompt quality. Plum (Pan et al., 2024a) extends GRIPS by incorporating\\nmetaheuristic strategies such as simulated annealing, mutation, and crossover. TEMPERA (Zhang et al., 2023b)\\nfurther frames the editing process as a reinforcement learning problem, training a policy model to perform\\ndifferent editing techniques to construct query-dependent prompts efficiently.\\n4.2.2\\nGenerative Prompt Optimisation\\nIn contrast to edit-based methods that apply local modifications to prompts, generative approaches leverage\\nLLMs to iteratively generate entirely new prompts, conditioned on a base prompt and various optimisation\\nsignals. Compared to local edits, generative methods can explore a broader region of the prompt space and\\nproduce more diverse and semantically rich candidates.\\nThe prompt generation process is typically driven by a variety of optimisation signals that guide the LLM\\ntowards producing improved prompts. These signals may include predefined rewriting rules (Xu et al., 2022;\\nZhou et al., 2024a), input-output examplars (Zhou et al., 2023c; Xu et al., 2024b), and dataset or program\\ndescriptions (Opsahl-Ong et al., 2024). Additional guidance can come from prior prompts along with their\\nevaluation scores (Yang et al., 2024a), meta-prompts that specify task objectives and constraints (Ye et al.,\\n2023; Hsieh et al., 2024; Wang et al., 2024i; Xiang et al., 2025), as well as signals that indicate the desired\\ndirection of change (Fernando et al., 2024; Guo et al., 2024b; Opsahl-Ong et al., 2024). Moreover, some methods\\nalso leverage success and failure examples to highlight effective or problematic prompt patterns (Wu et al.,\\n2024b; Yao et al., 2024). For example, ORPO (Yang et al., 2024a) generates new instructions by prompting the\\nLLM with previously generated candidates and their evaluation scores. StraGo (Wu et al., 2024b) leverages\\ninsights from both successful and failure cases to identify critical factors for obtaining high-quality prompts.\\nThe optimisation signals can be further integrated into advanced search strategies, such as Gibbs sampling (Xu\\net al., 2024b), Monte Carlo tree search (MCTS) (Wang et al., 2024i), Bayesian optimisation (Opsahl-Ong et al.,\\n2024; Lin et al., 2024b; Hu et al., 2024; Schneider et al., 2025; Wan et al., 2025), and neural bandit-based\\nmethods (Lin et al., 2024b; Shi et al., 2024a; Lin et al., 2024a). These search strategies enable more efficient and\\nscalable exploration of the prompt space. For instance, PromptAgent (Wang et al., 2024i) formulates prompt\\noptimisation as a strategic planning problem and leverages MCTS to efficiently navigate the expert-level prompt\\nspace. MIPRO (Opsahl-Ong et al., 2024) employs Bayesian optimisation to efficiently search for the optimal\\ncombination of instruction candidates and few-shot demonstrations.\\nWhile most generative approaches use a frozen LLM to generate new prompts, recent work has explored the use\\nof reinforcement learning to train a policy model for prompt generation (Deng et al., 2022; Sun et al., 2024a;\\nYao et al., 2024; Wang et al., 2025k). For example, Retroformer (Yao et al., 2024) trains a policy model to\\niteratively refine prompts by summarising the root cause of previous failed cases.\\n4.2.3\\nText Gradient-Based Prompt Optimisation\\nIn addition to editing and generating prompts directly, a more recent line of work explores the use of text\\ngradients to guide prompt optimisation (Pryzant et al., 2023; Yuksekgonul et al., 2024; Wang et al., 2024g; Austin\\nand Chartock, 2024; YÃ¼ksekgÃ¶nÃ¼l et al., 2025; Tang et al., 2025c; Zhang et al., 2025l). These methods draw\\ninspiration from gradient-based learning in neural networks, but instead of computing numerical gradients over\\n16\\nmodel parameters, they generate natural language feedback, which is referred to as â€œtext gradientâ€, that guides\\nhow a prompt should be updated to optimise a given objective. Once the text gradient is obtained, the prompt\\nis updated according to the feedback. The key components within such approaches lie in how the text gradients\\nare generated and how they are subsequently used to update the prompt. For example, ProTeGi (Pryzant\\net al., 2023) generates text gradients by criticising the current prompt. Subsequently, it edits the prompt in the\\nopposite semantic direction of the gradient. Such â€œgradient descentâ€ steps are guided by a beam search and\\nbandit selection procedure to find optimal prompts efficiently. Similarly, TextGrad (Yuksekgonul et al., 2024;\\nYÃ¼ksekgÃ¶nÃ¼l et al., 2025) generalises this idea to a broader framework for compound AI systems. It treats\\ntextual feedback as a form of â€œautomatic differentiationâ€ and uses LLM-generated suggestions to iteratively\\nimprove components such as prompts, code, or other symbolic variables. Another work (Zhou et al., 2024c)\\nproposes agent symbolic learning, a data-centric framework that models language agents as symbolic networks\\nand enables them to autonomously optimise their prompts, tools, and workflows via symbolic analogues of\\nback-propagation and gradient descent. Recent work (Wu et al., 2025c) also explores the prompt optimisation in\\ncompound AI systems, where its goal is to automatically optimise the configuration across a heterogeneous set\\nof components and parameters, e.g., model parameters, prompts, model selection choice, and hyperparameters.\\n4.2.4\\nEvolutionary Prompt Optimisation\\nIn addition to the above optimisation techniques, evolutionary algorithms have also been explored as a flexible\\nand effective approach for prompt optimisation (Guo et al., 2024b; Fernando et al., 2024). These approaches\\ntreat prompt optimisation as an evolutionary process, maintaining a population of candidate prompts that\\nare iteratively refined through evolutionary operators such as mutation, crossover, and selection. For example,\\nEvoPrompt (Guo et al., 2024b) leverages two widely used evolutionary algorithms: Genetic Algorithm (GA) and\\nDifferential Evolution (DE), to guide the optimisation process to find the high-performing prompts. It adapts\\nthe core evolutionary operations, namely mutation and crossover, to the prompt optimisation setting, where\\nnew candidate prompts are generated by combining segments from two parent prompts and introducing random\\nalternation to specific elements. Similarly, Promptbreeder (Fernando et al., 2024) also iteratively mutates a\\npopulation of task-prompts to evolve these prompts. A key feature is its use of mutation prompts, which are\\ninstructions that specify how task-prompts should be modified during the mutation process. These mutation\\nprompts can be either predefined or generated dynamically by the LLM itself, enabling a flexible and adaptive\\nmechanism for guiding prompt evolution.\\n4.3\\nMemory Optimisation\\nMemory is essential for enabling agents to reason, adapt, and operate effectively over extended tasks. However,\\nAI agents frequently face limitations arising from constrained context windows and forgetting, which can result\\nin phenomena such as context drift and hallucination (Liu et al., 2024b; Zhang et al., 2024c,d). These limitations\\nhave driven increasing interest in memory optimisation to enable generalisable and consistent behaviours in\\ndynamic environments. In this survey, we focus on inference-time memory strategies that enhance memory\\nutilisation without modifying model parameters. In contrast to training-time techniques such as fine-tuning or\\nknowledge editing (Cao et al., 2021; Mitchell et al., 2022), inference-time approaches dynamically decide what\\nto retain, retrieve, and discard during the reasoning process.\\nWe categorise existing methods into two optimisation objectives: short-term memory, which focuses on\\nmaintaining coherence within the active context, and long-term memory, which supports persistent retrieval\\nacross sessions. This optimisation-oriented perspective shifts the focus from static memory formats (e.g.,\\ninternal vs. external) to dynamic memory control, with an emphasis on how memory is scheduled, updated,\\nand reused to support decision-making. In the following subsections, we present representative methods within\\neach category, emphasising their impact on reasoning fidelity and effectiveness in long-horizon settings.\\n4.3.1\\nShort-term Memory Optimisation\\nShort-term memory optimisation focuses on managing limited contextual information within the LLMâ€™s working\\nmemory (Liu et al., 2024b). This typically includes recent dialogue turns, intermediate reasoning traces,\\nand task-relevant content from the immediate context. As the context expands, memory demands increase\\nsignificantly, making it impractical to retain all information within a fixed context window. To address this,\\nvarious techniques have been proposed to compress, summarise, or selectively retain key information (Zhang\\n17\\net al., 2024d; Wang et al., 2025d). Common strategies encompass summarisation, selective retention, sparse\\nattention, and dynamic context filtering. For example, Wang et al. (2025d) proposes recursive summarisation\\nto incrementally construct compact and comprehensive memory representations, enabling consistent responses\\nthroughout extended interactions. MemoChat (Lu et al., 2023) maintains dialogue-level memory derived from\\nconversation history to support coherent and personalised interaction. COMEDY (Chen et al., 2025f) and\\nReadAgent (Lee et al., 2024d) further incorporate extracted or compressed memory traces into the generation\\nprocess, allowing agents to maintain context over long conversations or documents. In addition to summarisation,\\nother methods dynamically adjust the context or retrieve intermediate state traces to facilitate multi-hop\\nreasoning. For example, MoT (Li and Qiu, 2023) and StructRAG (Li et al., 2025i) retrieve self-generated or\\nstructured memory to guide intermediate steps. MemoryBank (Zhong et al., 2024), inspired by the Ebbinghaus\\nforgetting curve (Murre and Dros, 2015), hierarchically summarises events and updates memory based on\\nrecency and relevance. Reflexion (Shinn et al., 2023) enables agents to reflect on task feedback and store\\nepisodic insights, promoting self-improvement over time.\\nThese methods significantly improve local coherence and context efficiency. However, short-term memory alone\\nis insufficient for retaining knowledge across sessions or enabling generalisation over long horizons, highlighting\\nthe need for complementary long-term memory mechanisms.\\n4.3.2\\nLong-term Memory Optimisation\\nLong-term memory optimisation mitigates the limitations of short context windows by providing persistent\\nand scalable storage that extends beyond the immediate input scope of the language model. It enables agents\\nto retain and retrieve factual knowledge, task histories, user preferences, and interaction trajectories across\\nsessions (Du et al., 2025), thereby supporting coherent reasoning and decision-making over time. A key objective\\nin this area is to manage increasingly complex and expanding memory spaces while preserving a clear separation\\nbetween memory storage and the reasoning process (Zhang et al., 2024d). External memory can be either\\nunstructured or organised into structured formats such as tuples, databases, or knowledge graphs (Zeng et al.,\\n2024b), and may span a wide range of sources and modalities.\\nA critical paradigm of long-term memory optimisation is Retrieval-Augmented Generation (RAG), which\\nincorporates relevant external memory into the reasoning process via retrieval (Wang et al., 2023a; Efeoglu\\nand Paschke, 2024; Gao et al., 2025c). For instance, EWE (Chen et al., 2025d) augments a language model\\nwith an explicit working memory that dynamically holds latent representations of retrieved passages, focusing\\non combining static memory entries at each decoding step. In contrast, A-MEM (Xu et al., 2025) constructs\\ninterconnected knowledge networks through dynamic indexing and linking, enabling agents to form evolving\\nmemory. Another prominent direction involves agentic retrieval, where agents autonomously determine when\\nand what to retrieve, alongside trajectory-level memory, which utilises past interactions to inform future\\nbehaviour.\\nSupporting techniques such as efficient indexing, memory pruning, and compression further\\nenhance scalability (Zheng et al., 2023a; Alizadeh et al., 2024). For example, Wang et al. (2024e) propose\\na lightweight unlearning framework based on the RAG paradigm. By altering the external knowledge base\\nused for retrieval, the system can simulate forgetting effects without modifying the underlying LLM. Similarly,\\nXu et al. (2025) introduce a self-evolving memory system that maintains long-term memory without relying\\non predefined operations. In addition to retrieval policies and memory control mechanisms, the structure\\nand encoding of memory itself significantly affect system performance. Vector-based memory systems encode\\nmemory in dense latent spaces and support fast, dynamic access. For instance, MemGPT (Packer et al.,\\n2023), NeuroCache (Safaya and Yuret, 2024), G-Memory (Zhang et al., 2025e), and AWESOME (Cao and\\nWang, 2024) enable consolidation and reuse across tasks. Mem0 (Chhikara et al., 2025) further introduces\\na production-ready memory-centric architecture for continuous extraction and retrieval. Other approaches\\ndraw inspiration from biological or symbolic systems to improve interpretability. HippoRAG (Gutierrez et al.,\\n2024) implements hippocampus-inspired indexing via lightweight knowledge graphs. GraphReader (Li et al.,\\n2024d) and Mem0g (Chhikara et al., 2025) use graph-based structures to capture conversational dependencies\\nand guide retrieval. In the symbolic domain, systems like ChatDB (Hu et al., 2023) issue SQL queries over\\nstructured databases, while Wang et al. (2024f) introduces a neurosymbolic framework that stores facts and\\nrules in both natural and symbolic form, supporting precise reasoning and memory tracking.\\nRecent work has also emphasised the importance of memory control mechanisms during inference (Zou et al., 2024;\\nChen et al., 2025d), which determine what, when, and how to store, update, or discard memory (Jin et al., 2025).\\n18\\nFor instance, MATTER (Lee et al., 2024b) dynamically selects relevant segments from multiple heterogeneous\\nmemory sources to support question answering, and AWM (Wang et al., 2024l) enables continuous memory\\nupdates in both online and offline settings. MyAgent (Hou et al., 2024) endows agents with memory-aware\\nrecall mechanisms for generation, addressing the temporal cognition limitations of LLMs. MemoryBank (Zhong\\net al., 2024) proposes a cognitively inspired update strategy, where periodic revisiting of past knowledge\\nmitigates forgetting and enhances long-term retention. Reinforcement learning and prioritisation policies have\\nalso been employed to guide memory dynamics (Zhou et al., 2025b; Yan et al., 2025; Long et al., 2025). For\\nexample, MEM1 (Zhou et al., 2025c) leverages reinforcement learning to maintain an evolving internal memory\\nstate, selectively consolidating new information while discarding irrelevant content. A-MEM (Xu et al., 2025)\\npresents an agentic memory architecture that autonomously organises, updates, and prunes memory based on\\nusage. MrSteve (Park et al., 2024) incorporates episodic â€œwhat-where-whenâ€ memory to hierarchically structure\\nlong-term knowledge, enabling goal-directed planning and task execution. These approaches allow agents to\\nproactively manage memory and complement short-term mechanisms. Meanwhile, MIRIX (Wang and Chen,\\n2025) introduces an agent memory system with six specialised memory types in collaborative settings, enabling\\ncoordinated retrieval and achieving state-of-the-art performance in long-horizon tasks, while Agent KB (Tang\\net al., 2025b) leverages a shared knowledge base with a teacher-student dual-phase retrieval mechanism to\\ntransfer cross-domain problem-solving strategies and execution lessons across agents, significantly enhancing\\nperformance through hierarchical strategic guidance and refinement.\\n4.4\\nTool Optimisation\\nTools are critical components within agent systems, serving as interfaces that allow agents to perceive and interact\\nwith the real world. They enable access to external information sources, structured databases, computational\\nresources, and APIs, thereby enhancing the agentâ€™s ability to solve complex, real-world problems (Patil et al.,\\n2024; Yang et al., 2023; Guo et al., 2024d). As a result, tool use has become a core competence of AI agents,\\nespecially for tasks that require external knowledge and multi-step reasoning. However, simply exposing agents\\nto tools is not sufficient. Effective tool use requires the agent to recognise when and how to invoke the right\\ntools, interpret tool outputs, and integrate them into multi-step reasoning. Consequently, recent research has\\nfocused on tool optimisation, which aims to enhance the agentâ€™s ability to use tools intelligently and efficiently.\\nExisting research on tool optimisation largely falls into two complementary directions. The first, which has\\nbeen more extensively explored, focuses on enhancing the agentâ€™s ability to interact with tools. This is achieved\\nthrough different approaches, including training strategies, prompting techniques, and reasoning algorithms,\\nthat aim to improve the agentâ€™s ability to understand, select, and execute tools effectively. The second, which\\nis more recent and still emerging, focuses on optimising the tools themselves by modifying existing tools or\\ncreating new ones that better align with the functional requirements of the target tasks.\\n4.4.1\\nTraining-Based Tool Optimisation\\nTraining-based tool optimisation aims to enhance an agentâ€™s ability to use tools by updating the underlying\\nLLMâ€™s parameters through learning. The motivation behind this approach stems from the fact that LLMs\\nare pretrained purely on text generation tasks, without any exposure to tool usage or interactive execution.\\nTherefore, they lack an inherent understanding of how to invoke external tools and interpret tool outputs.\\nTraining-based methods aim to address this limitation by explicitly teaching the LLMs how to interact with\\ntools, thereby embedding tool-use capabilities directly into the agentâ€™s internal policy.\\nSupervised Fine-Tuning for Tool Optimisation.\\nEarlier efforts in this line of work rely on supervised fine-tuning\\n(SFT), which trains the LLM on high-quality tool-use trajectories to explicitly demonstrate how tools should\\nbe invoked and integrated into task execution (Schick et al., 2023; Du et al., 2024; Liu et al., 2025g; Wang\\net al., 2025e). A central focus of these methods lies in the collection of high-quality tool-use trajectories, which\\ntypically consist of input queries, intermediate reasoning steps, tool invocations and final answers. These\\ntrajectories serve as explicit supervision signals for the agent, teaching it how to plan tool usage, execute\\ncalls, and incorporate results into its reasoning process. For example, approaches such as ToolLLM (Qin\\net al., 2024) and GPT4Tools (Yang et al., 2023) leverage more powerful LLMs to generate both instructions\\nand corresponding tool-use trajectories. Inspired by the human learning process, STE (Wang et al., 2024a)\\nintroduces simulated trial-and-error interactions to collect tool-use examples, while TOOLEVO (Chen et al.,\\n19\\n2025b) employs MCTS to enable more active exploration and collect higher-quality trajectories. T3-Agent (Gao\\net al., 2025d) further extends this paradigm to the multimodal setting by introducing a data synthesis pipeline\\nthat generates and verifies high-quality multimodal tool-use trajectories for tuning visionâ€“language models.\\nMoreover, recent work (Yao et al., 2025) indicates that even advanced LLMs face challenges with tool use in\\nmulti-turn interactions, especially when these interactions involve complex function calls, long-term dependencies,\\nor requesting missing information. To generate high-quality training trajectories on multi-turn tool calling,\\nMagnet (Yin et al., 2025) proposes to synthesise a sequence of queries and executable function calls from tools,\\nand employs a graph to build a reliable multi-turn query. BUTTON (Chen et al., 2025e) generates synthetic\\ncompositional instruction tuning data via a two-stage process, where a bottom-up stage composes atomic tasks\\nto construct the instructions and a top-down stage employs a multi-agent system to simulate the user, assistant,\\nand tool to generate the trajectory data. To enable more realistic data generation, APIGen-MT (Prabhakar\\net al., 2025) proposes a two-phase framework that first generates tool call sequences and then transforms them\\ninto complete multi-turn interaction trajectories through simulated human-agent interplay.\\nOnce the tool-use trajectories are collected, they are used to fine-tune the LLM through standard language\\nmodelling objectives, enabling the model to learn successful patterns of tool invocation and integration. In\\naddition to this common paradigm, some studies have also explored more advanced training strategies to\\nfurther enhance tool-use capabilities. For example, Confucius (Gao et al., 2024a) introduces an easy-to-difficult\\ncurriculum learning paradigm that gradually exposes the model to increasingly complex tool-use scenarios.\\nGorilla (Patil et al., 2024) proposes integrating a document retriever into the training pipeline, allowing the\\nagent to dynamically adapt to evolving toolsets by grounding tool usage in retrieved documentation.\\nReinforcement Learning for Tool Optimisation.\\nWhile supervised fine-tuning has proven effective for teaching\\nagents to use tools, its performance is often constrained by the quality and coverage of the training data.\\nLow-quality trajectories can lead to diminished performance gains. Moreover, fine-tuning on limited datasets\\nmay hinder generalisation, particularly when agents encounter unseen tools or task configurations at inference\\ntime. To address these limitations, recent research has turned to reinforcement learning (RL) as an alternative\\noptimisation paradigm for tool use. By enabling agents to learn through interaction and feedback, RL facilitates\\nthe development of more adaptive and robust tool-use strategies. This approach has shown promising results\\nin recent work such as ReTool (Feng et al., 2025a) and Nemotron-Research-Tool-N1 (Tool-N1) (Zhang et al.,\\n2025m), both of which demonstrate how lightweight supervision in an interactive environment can lead to more\\ngeneralisable tool-use capabilities. Tool-Star (Dong et al., 2025a) enhances RL-based tool use by combining\\nscalable tool-integrated data synthesis with a two-stage training framework to improve autonomous multi-tool\\ncollaborative reasoning. SPORT (Li et al., 2025d) extends RL-based tool optimisation to the multimodal setting\\nthrough step-wise preference optimisation, enabling agents to self-synthesise tasks, explore and verify tool\\nusage without human annotations. Building on these foundations, further studies have focused on improving\\nRL algorithms for tool use, including ARPO (Dong et al., 2025b), which balances long-horizon reasoning\\nand multi-turn tool interactions via an entropy-based adaptive rollout mechanism and stepwise advantage\\nattribution, as well as methods that design more effective reward functions (Qian et al., 2025a) and leverage\\nsynthetic data generation and filtering to enhance training stability and efficiency (Goldie et al., 2025).\\n4.4.2\\nInference-Time Tool Optimisation\\nIn addition to training-based approaches, another line of work focuses on enhancing tool-use capabilities during\\ninference, without modifying LLM parameters. These methods typically operate by optimising tool-related\\ncontextual information within prompts or guiding the agentâ€™s decision-making process through structured\\nreasoning at test time. There are two major directions within this paradigm: (1) prompt-based methods, which\\nrefine the representation of tool documentation or instructions to facilitate better understanding and utilisation\\nof tools; (2) reasoning-based methods, which leverage test-time reasoning strategies, such as MCTS and other\\ntree-based algorithms to enable more effective exploration and selection of tools during inference.\\nPrompt-Based Tool Optimisation.\\nTool-related information is typically provided to agents through tool\\ndocumentation within prompts. These documents describe tool functionalities, potential usage, and invocation\\nformats, helping the agent understand how to interact with external tools to solve complex tasks. Therefore,\\ntool documentation within prompts serves as a crucial bridge between the agent and its available tools, directly\\n20\\ninfluencing the quality of tool-use decisions. Recent efforts have focused on optimising how this documentation\\nis presented, either by restructuring source documents or refining them through interactive feedback (Qu\\net al., 2025). For instance, EASYTOOL (Yuan et al., 2025b) transforms different tool documentation into\\nunified, concise instructions, making them easier for LLMs to use. In contrast, approaches such as DRAFT (Qu\\net al., 2025) and PLAY2PROMPT (Fang et al., 2025) draw inspiration from human trial-and-error processes,\\nintroducing interactive frameworks that iteratively refine tool documentation based on feedback.\\nBeyond these methods, a more recent direction explores the joint optimisation of both tool documentation and\\nthe instructions provided to the LLM agent. For example, Wu et al. (2025a) propose an optimisation framework\\nthat simultaneously refines the agentâ€™s prompt instructions and the tool descriptions, collectively referred to\\nas the context, to enhance their interaction. The optimised context has been shown to reduce computational\\noverhead and improve tool-use efficiency, highlighting the importance of context design in effective inference-time\\ntool optimisation.\\nReasoning-Based Tool Optimisation.\\nTest-time reasoning and planning techniques have demonstrated strong\\npotential for improving tool-use capabilities in AI agents. Early work such as ToolLLM (Qin et al., 2024)\\nhas validated the effectiveness of the ReAct (Yao et al., 2023b) framework in tool-use scenarios, and further\\nproposed a depth-first tree search algorithm that enables agents to quickly backtrack to the last successful state\\nrather than restarting from scratch, which significantly improves efficiency. ToolChain (Zhuang et al., 2024)\\nintroduces a more efficient tree-based search algorithm by employing a cost function to estimate the future\\ncost of a given branch. This allows agents to prune low-value paths early and avoid the inefficient rollouts\\ncommonly associated with traditional MCTS. Similarly, Tool-Planner (Liu et al., 2025h) clusters tools with\\nsimilar functionalities into toolkits and leverages a tree-based planning method to quickly reselect and adjust\\ntools from these toolkits. MCP-Zero (Fei et al., 2025) introduces an active agent framework that empowers\\nLLMs to autonomously identify capability gaps and request tools on demand.\\n4.4.3\\nTool Functionality Optimisation\\nBeyond optimising the agentâ€™s behaviour, a complementary line of work focuses on modifying or generating\\ntools themselves to better support task-specific reasoning and execution. Inspired by the human practice of\\ncontinuously developing tools to meet task requirements, these approaches aim to extend the agentâ€™s action\\nspace by adapting the toolset to the task, rather than adapting the task to a fixed toolset (Wang et al., 2024k).\\nFor instance, CREATOR (Qian et al., 2023) and LATM (Cai et al., 2024) introduce frameworks that generate\\ntool documentation and executable code for novel tasks. CRAFT (Yuan et al., 2024a) leverages reusable code\\nsnippets from prior tasks to create new tools for unseen scenarios. AgentOptimiser (Zhang et al., 2024b) treats\\ntools and functions as learnable weights, allowing the agent to iteratively refine them using LLM-based updates.\\nA more recent work, Alita (Qiu et al., 2025), extends tool creation into a Multi-Component Program (MCP)\\nformat, which enhances reusability and environment management. Moreover, CLOVA (Gao et al., 2024b)\\nintroduces a closed-loop visual assistant framework with inference, reflection, and learning phases, enabling\\ncontinual adaptation of visual tools based on human feedback.\\n5\\nMulti-Agent Optimisation\\nThe multi-agent workflow defines how multiple agents collaborate to solve complex tasks through structured\\ntopologies and interaction patterns. The field has witnessed a fundamental shift: from manually designed\\nagent architectures, where researchers explicitly specify collaboration patterns and communication protocols, to\\nself-evolving systems that automatically discover effective collaboration strategies. This evolution reframes\\nworkflow design as a search problem over three interconnected spaces: the structural space of possible agent\\ntopologies, the semantic space of agent roles and instructions, and the capability space of LLM backbones.\\nRecent approaches explore these spaces using a range of optimisation techniques, from evolutionary algorithms\\nto reinforcement learning, each offering different trade-offs in balancing multiple optimisation targets (e.g.,\\naccuracy, efficiency, and safety).\\nThis section traces the progression of multi-agent workflow optimisation across four key dimensions. Our\\nstarting point examines manually designed paradigms that establish foundational principles. From there, we\\nconsider prompt-level optimisation, which refines agent behaviours within fixed topologies. We subsequently\\n21\\naddress topology optimisation, which focuses on discovering the most effective architectures for multiple agents\\nto accomplish a given task. We also discuss comprehensive approaches that simultaneously consider multiple\\noptimisation spaces, jointly optimising prompts, topologies, and other system parameters in an integrated\\nmanner. Additionally, we investigate LLM-backbone optimisation, which enhances the fundamental reasoning\\nand collaborative capabilities of the agents themselves through targeted training. Through this lens, we show\\nhow the field progressively expands its conception of what constitutes a searchable and optimisable parameter\\nin multi-agent systems, from agent instructions and communication structures to the core competencies of the\\nunderlying models. Figure 6 provides an overview of multi-agent workflow optimisation across its core elements\\nand key dimensions.\\n\\xa0Optimisation\\xa0Method\\n\\xa0Optimisation\\xa0Space\\nLLM\\nTree Search\\nRL\\nGradient\\nText\\nCode\\nGraph\\nDistribution\\nMulti-Agent\\xa0Optimisation\\nElements\\n\\xa0Optimisation\\xa0Target\\nAccuracy\\nEfficiency\\nSafety\\nMulti-Agent Optimisation\\n\\xa0Dimensions\\nPrompt\\nTopolodgy\\nLLM Backbone\\nUnified\\nCode-level Workflows\\nCommunication-graphs\\nTopolodgies\\nCode-based Approaches\\nSearch-based Approaches\\nLearning-based Approaches\\nReasoning-oriented Optimisation\\nCollaboration-oriented\\nOptimisation\\nPrompt Optimisation\\nFigure 6 An overview of multi-agent systems optimisation approaches, with core optimisation elements (space, methods,\\nand targets) on the left and optimisation dimensions (prompt, topology, unified, and LLM backbone) on the right.\\n5.1\\nManually Designed Multi-Agent Systems\\nManually designed workflows form the foundation of multi-agent collaboration research. These architectures\\nencode researchersâ€™ insights about task decomposition, agent capabilities, and coordination mechanisms into\\nexplicit interaction patterns. By examining these handcrafted paradigms, we can understand the design\\nprinciples that guide agent collaboration and the engineering considerations that shape system architecture.\\nParallel Workflows.\\nParallel workflows employ concurrent execution followed by collective decision-making.\\nThe simplest form involves multiple independent agents generating solutions in parallel, followed by majority\\nvoting to select the final output. Empirical evidence shows that parallel generation with small LLMs can\\nmatch or even outperform single large LLMs (Verga et al., 2024; Wang et al., 2025a). Multi-layer aggregation\\nfurther reduces error bounds and improves robustness (Zhang et al., 2025d). Recent extensions incorporate\\ndynamic task graphs and asynchronous threads to enable near-linear scaling and lower decision latency (Yu\\net al., 2025; Gu et al., 2025; Wang et al., 2025c). However, while computational throughput scales horizontally,\\nthe engineering costs of managing coordination and consistency grow exponentially.\\nHierarchical Workflows.\\nWhen subtasks exhibit strict contextual dependencies, hierarchical (Zhang et al.,\\n2024c; Qian et al., 2024) workflows offer a structured alternative. These frameworks organise agents into\\nmulti-level top-down structures or sequential pipelines. The system decomposes tasks across layers, with each\\nlayer responsible for different subtasks. This design excels in complex goal-driven tasks such as deep research\\nand code generation (Hong et al., 2024; Zhang et al., 2025n). However, its fixed topology limits adaptability,\\nespecially when facing dynamic goals or resource constraints.\\nMulti-Agent Debate.\\nTo balance accuracy with interpretability, researchers have developed the debate paradigm,\\nwhere agents engage in adversarial-negotiation-arbitration cycles to discuss and correct reasoning errors. Early\\n22\\nwork explored symmetric debater mechanisms (Li et al., 2024g). More recent studies extend this framework\\nby introducing role asymmetry, adjustable debate intensity, and persuasiveness-oriented strategies (Yin et al.,\\n2023; Liang et al., 2024; Khan et al., 2024; Chang, 2024). In addition, confidence-gated debate strategies\\ndemonstrate that triggering multi-agent debates only when a single model exhibits low confidence can sharply\\nreduce inference costs without hindering performance (Eo et al., 2025).\\nDespite the success of manually designed workflows and structured multi-agent paradigms, recent empirical\\nstudies reveal that single large LLMs with well-crafted prompts can match the performance of complex multi-\\nagent discussion frameworks on multiple reasoning benchmarks (Pan et al., 2025a). This finding, coupled\\nwith the high implementation and maintenance costs of handcrafted multi-agent workflows (Li et al., 2024h;\\nZhang et al., 2025j), has driven the development of self-evolving multi-agent systems that can automatically\\nlearn, adapt, and restructure their workflows over time, rather than relying on fixed architectures and static\\ncoordination protocols.\\n5.2\\nSelf-Evolving Multi-Agent System\\nThe high engineering costs and limited adaptability of manually designed multi-agent workflows have motivated\\na shift towards automated, self-evolving systems. These systems can automatically design, evaluate, and refine\\nagent workflows by adapting their prompts, topologies, and collaborative strategies based on performance\\nfeedback. Instead of relying on hard-coded configurations, they treat workflow optimisation as a search problem,\\nwhere the system explores and optimises over a space of possible configurations. The search space spans multiple\\nlevels, from local prompts to global topology structures.\\nTo effectively navigate the search space, various search algorithms have been introduced. These methods\\nrange from reinforcement learning, Monte Carlo Tree Search, and generative models that enable efficient\\nexploration, to evolutionary operators that provide robust search capabilities. Moreover, the optimisation\\nobjectives have expanded from improving performance to balancing multi-dimensional goals, including task\\naccuracy, computational efficiency, and safety. This evolution reveals that as search capabilities advance, the\\ncore challenge shifts from finding optimal solutions to defining what optimality means in dynamic multi-agent\\ncontexts.\\n5.2.1\\nMulti-Agent Prompt Optimisation\\nOne promising direction for achieving such self-evolution is through prompt optimisation, where prompts define\\nboth agent roles and their corresponding task instructions. Recent approaches treat these prompt-encoded\\nconfigurations as a formal search space for systematic refinement. In fact, prompt optimisation in multi-agent\\nworkflows often builds upon the single-agent techniques discussed in Section 4.2, but extends them to coordinate\\nmultiple agents and task dependencies. For example, DSPy (Singhvi et al., 2023) Assertions introduces runtime\\nself-evolution, where the search space encompasses possible intermediate outputs from pipeline modules, using\\nassertion-driven backtracking with explicit feedback to guide LLMs in self-correcting outputs that violate\\nprogrammatic constraints. AutoAgents (Chen et al., 2024b) extends prompt optimisation from single-agent\\nsettings to entire multi-agent team configurations, optimising specialised agent roles and execution plans through\\nstructured dialogue between dedicated meta-agents.\\n5.2.2\\nTopology Optimisation\\nTopology optimisation represents a paradigm shift in multi-agent system design: rather than treating communi-\\ncation structure as a fixed constraint, it recognises topology itself as a powerful optimisation target. This insight\\nemerged from a fundamental observationâ€”even the best prompts cannot compensate for poor architectural\\nchoices. Viewed through a representation-centred lens, existing work clusters into two complementary families:\\nprogram/code-level workflow topologies and communication-graph topologies; this classification foregrounds\\nwhat is being optimisedâ€”the chosen representation of topology. This marks not just technical progress but a\\nconceptual shiftâ€”the medium (topology) matters as much as the message (prompts).\\nCode-level workflows.\\nRepresenting workflows as executable programs or typed code graphs makes agent\\ncoordination explicit and verifiable, enabling compositional reuse and automated checking. AutoFlow (Li et al.,\\n2024h) sets the search space to natural-language programs (CoRE) and trains a generator LLM with reinforcement\\n23\\nlearning, supporting both fine-tuning and in-context use. Compared with AutoFlow, AFlow (Zhang et al.,\\n2025j) replaces the NL program space with typed, reusable operators to form code graphs; Monte Carlo Tree\\nSearch with LLM-guided expansion and soft probabilistic selection provides a more structured, sample-efficient\\nexploration of the vast design space than RL over CoRE. Pushing beyond these discrete search schemes,\\nScoreFlow (Wang et al., 2025j) lifts code representations into a continuous space and applies gradient-based\\noptimisation with Score-DPO (a direct preference optimisation variant incorporating quantitative feedback)\\nto improve the workflow generator. This addresses the exploration inefficiency inherent to RL/MCTS and\\nenables task-level adaptive workflow generation. Orthogonal to search-based optimisation, MAS-GPT (Ye et al.,\\n2025) uses supervised fine-tuning on a consistency-oriented corpus (inter- and intra-consistency) so that a single\\ninference aims to produce a complete, executable MAS codebase, trading broad search coverage for one-shot\\nefficiency and stronger dependence on data quality.\\nCommunication-graph topologies.\\nUnlike code-level programs, this line treats the workflow as a multi-agent\\ncommunication graph whose connections are the optimisation target (Liu et al., 2025i). GPTSwarm (Zhuge\\net al., 2024a) defines its search space as connections within a computational graph of agents. It relaxes this\\ndiscrete space into continuous edge probabilities, also employing RL to learn optimal connection schemes.\\nBuilding on GPTSwarm, DynaSwarm (Leong and Wu, 2025) extends the search space from a single optimised\\ngraph to a portfolio of graph structures with Actorâ€“Critic (A2C) optimisation and a lightweight graph selector\\nfor per-instance topology selection, addressing the key observation that different queries require different\\ngraph structures for optimal performance. Rather than masking edges in a fixed space, G-Designer (Zhang\\net al., 2024a) employs a variational graph autoencoder to directly generate task-adaptive communication\\ngraphs, modulating structural complexity to balance quality and token cost. MermaidFlow (Zheng et al., 2025)\\nrepresents topology as a typed, declarative graph with static verification and explores only semantically valid\\nregions via safety-constrained evolutionary operators.\\nBeyond static graph synthesis, some approaches dynamically modulate the communication graph during\\nexecution. DyLAN (Liu et al., 2023b) treats the search space as active agents across layers with an early-\\nstopping time axis; it prunes low-value agents via an LLM ranker and performs automated team optimisation\\nwith an Agent Importance Score using propagationâ€“aggregationâ€“selection. Captain Agent (Song et al., 2024)\\ndefines the search space as subtask-specific sets of agents and tools (retrieved, filtered, and, when needed,\\ngenerated); nested group conversations and reflection iteratively refine team composition in situ rather than\\nsynthesising a fixed graph from scratch. Flow (Niu et al., 2025) contrasts with DyLANâ€™s pruning and Captain\\nAgentâ€™s team recomposition by dynamically adjusting the AOV graph structure: it selects an initial graph via\\nparallelism/dependency metrics and then refines it online through workflow refinement and subtask reassignment,\\nachieving modular concurrency with minimal coordination cost.\\nOrthogonal to graph synthesis, pruning methods optimise by removing redundant or risky communications\\nwhile preserving essential collaboration. AgentPrune (Zhang et al., 2025g) treats the search space as a spatial-\\ntemporal communication graph where both intra-dialogue (spatial) and inter-dialogue (temporal) edges are\\npruning targets; it employs a trainable low-rank-guided graph mask to identify and eliminate redundant\\ncommunications via one-shot pruning, optimizing for token economy. Building on this pruning paradigm,\\nAGP (Adaptive Graph Pruning) (Li et al., 2025a) extends the search space to include both agent quantity\\n(hard pruning) and communication edges (soft pruning). It employs a two-stage training strategy that jointly\\noptimises these dimensions on a per-task basis, dynamically determining the optimal number of agents and\\ntheir connections for task-specific topology generation. While the above methods prune for efficiency and\\nadaptability, G-Safeguard (Wang et al., 2025f) applies pruning for securityâ€”it operates on communication\\nedges as the search space, using a GNN to flag risky nodes and deterministic rules to cut outward edges under a\\nmodel-driven threshold for defence against adversarial attacks. Relatedly, NetSafe (Yu et al., 2024a) summarises\\ntopological safety risks and proposes graph-based detection and intervention principles as a complementary\\nsafety lens.\\n5.2.3\\nUnified Optimisation\\nUnified optimisation emerges from a key insight: prompts and topology are not independent design choices but\\ndeeply interconnected aspects of agent systems (Zhou et al., 2025a). A well-crafted prompt cannot function\\neffectively in a poor communication structure, while an elegant topology yields little benefit with poorly\\n24\\ninstructed agents. This interdependence has driven the field along three distinct technical paths: code-based\\nunification, structured optimisation methods, and learning-driven architectures. Each approach tackles the joint\\noptimisation challenge from a unique angle, revealing different trade-offs between efficiency and performance.\\nCode-based Approaches.\\nThe most direct approach to unified optimisation treats code as a universal repre-\\nsentation for both prompts and topology. ADAS (Hu et al., 2025a) pioneered this approach through its Meta\\nAgent Search framework, representing prompts, workflows, and tool use as Python code to enable iterative\\nagent generation and evaluation. This code-centric view allows natural co-evolution, modifying agent logic\\ninherently affects both instructional and structural aspects. FlowReasoner (Gao et al., 2025a) advanced the\\ncode-based paradigm by focusing on query-level adaptation, generating one MAS per query rather than per\\ntask. After distilling reasoning abilities from DeepSeek-R1, it employs GRPO with external execution feedback\\nto enhance its meta-agent, optimising for performance and efficiency. Together, these methods demonstrate\\nthat code provides a flexible substrate for joint optimisation, though at different granularities of adaptation.\\nSearch-based Approaches.\\nRather than relying on implicit co-evolution through code, another line of work\\ndevelops explicit mechanisms for coordinating prompt and topology design. EvoAgent (Yuan et al., 2025a)\\ndefined search spaces as textual agent settings (roles, skills, prompts) and employed evolutionary algorithms\\nwith mutation, crossover, and selection operators to generate diverse agent populations. Compared with implicit\\ncode-based co-evolution, EvoAgent explicitly evolves configuration-level characteristics rather than synthesising\\nprograms. Relative to EvoAgentâ€™s text-centric configuration search, EvoFlow (Gao et al., 2025a) likewise adopts\\nevolutionary search but over operator-node workflow graphs. It introduces predefined composite operators (e.g.\\nCoT, debate) and uses an operator library with tag selection to constrain mutation/crossover and narrow the\\nsearch space. EvoFlow further treats LLM selection as a decision variable to balance performance and cost;\\ndiversity-aware selection preserves population variety, and a multi-objective fitness drives costâ€“performance\\nPareto optimisation.\\nComplementary to evolutionary searches, MASS (Zhou et al., 2025a) proposes a three-stage, conditionally\\ncoupled optimisation framework: it first locally tunes each agentâ€™s prompts, then searches the workflow topology\\nin a pruned space, and finally performs global prompt optimisation on the selected topology; the procedure\\nalternates rather than fully decoupling, serving as a practical approximation to joint optimisation. Most\\nrecently, DebFlow (Su et al., 2025) represents search spaces as workflow graphs of operator nodes and employs\\nmulti-agent debate for optimisation. Guided by reflexion on execution failures, it avoids exhaustive search while\\npioneering debate mechanisms in automated agent design. These structured approaches trade some flexibility\\nfor more targeted optimisation strategies. Building on the operator node representation, MAS-ZERO (Ke et al.,\\n2025) casts unified optimisation as a purely inference-time search, iteratively restructuring agent teams and\\ntask decompositions through solvability-guided refinement without any gradient updates or offline training.\\nLearning-based Approaches.\\nThe latest wave of research applies sophisticated learning paradigms to jointly\\noptimise prompts and topology. MaAS (Zhang et al., 2025f) shifts from optimising single architectures to\\nlearning agentic supernetsâ€”probabilistic distributions over multi-agent systems. Its controller network samples\\nquery-specific architectures with Monte Carlo and textual gradient optimisation, achieving superior performance\\nwith dramatically reduced inference costs. ANN (Ma et al., 2025) conceptualises multi-agent collaboration as\\nlayered neural networks, where each layer forms specialised agent teams. It employs a two-phase optimisation\\nprocess: forward task decomposition and backward textual gradient refinement. This approach jointly evolves\\nagent roles, prompts, and inter-layer topologies, enabling post-training adaptation to novel tasks.\\n5.2.4\\nLLM Backbone Optimisation\\nThe evolution of the LLM backbone behind agents is a critical aspect of multi-agent evolution, particularly how\\nagents improve their cooperative or reasoning abilities through interaction.\\nReasoning-oriented Optimisation.\\nA prominent line of work focuses on enhancing the backbone LLMâ€™s\\nreasoning capacity via multi-agent collaboration. For instance, multi-agent finetuning (Subramaniam et al.,\\n2025) leverages high-quality cooperative trajectories sampled from multi-agent debates for supervised fine-tuning,\\nenabling (1) role-specific specialisation of agents and (2) improved reasoning capabilities of the underlying\\nbackbone model. Similarly, Sirius (Zhao et al., 2025c) and MALT (Motwani et al., 2024) employ self-play to\\n25\\ncollect high-quality cooperative trajectories and train agents within their respective multi-agent collaboration\\nframeworks. While both approaches leverage failed trajectories to some extent, they differ in methodology:\\nSirius relies solely on SFT and integrates incorrect trajectories via self-correction into the training dataset,\\nwhereas MALT adopts DPO, naturally utilising negative samples. These methods provide early evidence of the\\npotential for self-improvement in multi-agent systems, though they are primarily applied in relatively simple\\nsettings (e.g., multi-agent debate or â€œgenerator-verifier-answererâ€ system). Moving forward, MaPoRL (Park\\net al., 2025) introduces task-specific reward shaping to explicitly incentivise inter-agent communication and\\ncooperation through reinforcement learning. MARFT (Liao et al., 2025) establishes a comprehensive bridge\\nbetween conventional multi-agent reinforcement learning (MARL) and LLM-based multi-agent reinforcement\\ntuning. Building on this, MARTI (Liao et al., 2025) proposes a more customizable framework for reinforced\\nmulti-agent fine-tuning, supporting flexible design of both agentic structures and reward functions. Empirical\\nresults show that LLM backbones exhibit considerable improvements in cooperative capabilities during their\\ncooperative training.\\nCollaboration-oriented Optimisation.\\nBeyond reasoning, a smaller body of work focuses on enhancing commu-\\nnication and collaboration abilities within multi-agent systems. The core assumption is that LLM agents are\\nnot inherently effective team players, and their collaborative communication skills require targeted training. An\\nearly example is COPPER (Bo et al., 2024), which employs PPO to train a shared reflector that generates\\nhigh-quality, role-aware personalised reflections for multi-agent collaboration trajectories. OPTIMA (Chen\\net al., 2025h) more directly targets communication efficiency in multi-agent systems (measured by token usage\\nand communication readability) and explores achieving an effectiveness-efficiency trade-off via SFT, DPO,\\nand hybrid methods. It reports a 2.8Ã— performance gain with less than 10% of the token cost on tasks\\ndemanding intensive information exchange, which vividly demonstrates the promising potential of scaling\\nagentsâ€™ collaborative capabilities. Further, MaPoRL (Park et al., 2025) argues that the prevalent paradigm\\nof prompting out-of-the-box LLMs and relying solely on their innate collaborative abilities is questionable.\\nInstead, it introduces carefully designed reinforcement learning signals within a multi-agent debate framework\\nto explicitly elicit collaborative behaviours, encouraging agents to communicate more frequently and with higher\\nquality.\\n6\\nDomain-Specific Optimisation\\nWhile previous sections have focused on agent optimisation and evolution techniques in general-domain settings,\\ndomain-specific agent systems introduce unique challenges that require tailored optimisation strategies. These\\ndomains, such as biomedicine (Almansoori et al., 2025), programming (Tang et al., 2024), scientific research (Pu\\net al., 2025), game-playing (Belle et al., 2025), computer use (Sun et al., 2025), and finance & legal research, are\\noften characterised by specialised task structures, domain-specific knowledge bases, distinct data modalities, and\\noperational constraints. Such factors can significantly influence how agents are designed, optimised, and evolved.\\nIn this section, we survey recent advances in domain-specific agent optimisation and evolution, highlighting\\neffective techniques that have been developed to meet the unique demands of each domain.\\n6.1\\nDomain-Specific Optimisation in Biomedicine\\nIn the biomedical domain, agent optimisation focuses on aligning agent behaviours with the procedural and\\noperational requirements of real-world clinical settings. Recent studies have demonstrated the effectiveness of\\ndomain-specific agent design in two key application areas: medical diagnosis (Donner-Banzhoff, 2018; Almansoori\\net al., 2025; Zhuang et al., 2025) and molecular discovery (M. Bran et al., 2024; Inoue et al., 2025). In what\\nfollows, we examine representative agent optimisation strategies within these two domains.\\n6.1.1\\nMedical Diagnosis\\nMedical diagnosis requires determining a patientâ€™s condition based on clinical information such as symptoms,\\nmedical history, and diagnostic test results (Kononenko, 2001; Donner-Banzhoff, 2018). Recent research has\\nincreasingly explored the use of autonomous agents in this context, enabling systems to automatically conduct\\ndiagnostic dialogues, pose clarifying questions, and generate plausible diagnostic hypotheses (Li et al., 2024c;\\nChen et al., 2025i; Zuo et al., 2025; Ghezloo et al., 2025). These agents often operate under uncertain conditions,\\n26\\nmaking decisions based on incomplete or ambiguous patient information (Chen et al., 2025i). The diagnostic\\nprocess typically involves multi-turn interactions, during which agents elicit missing information through\\nfollow-up enquiries (Chen et al., 2025i). Moreover, to support robust clinical reasoning, agents often require\\nintegrating external knowledge bases or interacting with specialised medical tools for information retrieval and\\nevidence-based reasoning (Feng et al., 2025b; Fallahpour et al., 2025).\\nGiven these domain-specific requirements, recent studies have focused on developing agent architectures\\nspecifically optimised for medical diagnosis (Li et al., 2024a; Almansoori et al., 2025; Ghezloo et al., 2025;\\nWang et al., 2025l). One promising research direction focuses on multi-agent systems, which have shown\\nstrong potential for modelling the complexity and multi-step reasoning involved in medical diagnosis. These\\napproaches can be broadly classified into two categories: simulation-driven and collaborative designs. Simulation-\\ndriven systems aim to reproduce real clinical settings by assigning specific roles to agents and enabling them\\nto learn diagnostic strategies through interactions within a simulated medical environment. For instance,\\nMedAgentSim (Almansoori et al., 2025) introduces a self-evolving simulation framework that integrates\\nexperience replay, chain-of-thought ensembling, and CLIP-based semantic memory to support diagnostic\\nreasoning. PathFinder (Ghezloo et al., 2025) targets histopathological analysis by orchestrating multiple agents\\nto emulate expert diagnostic workflows on gigapixel-scale medical images. In contrast, collaborative multi-agent\\nsystems focus on collective decision-making and collaboration among agents. For example, MDAgents (Kim\\net al., 2024) enables adaptive collaboration among multiple agents, where a moderator agent is responsible for\\nintegrating diverse suggestions and consulting external knowledge sources as needed. MDTeamGPT (Chen\\net al., 2025c) extends this paradigm to multidisciplinary consultation, supporting self-evolving, team-based\\ndiagnostic processes through reflective discussion mechanisms.\\nAnother line of work on agent optimisation for diagnosis focuses on tool integration and multimodal reasoning.\\nFor instance, MMedAgent (Li et al., 2024a) addresses the generalisability limitations of existing multimodal\\nLLMs by dynamically incorporating specialised medical tools across different modalities. To improve clinical\\nreliability, MedAgent-Pro (Wang et al., 2025l) introduces diagnostic planning guided by established clinical\\ncriteria and integrates multimodal evidence via task-specific tool agents. In contrast to fixed agent architectures,\\nrecent work has explored more flexible designs that adapt based on diagnostic performance. For example,\\nZhuang et al. (2025) proposes a graph-based agent framework where the reasoning process is continuously\\nadjusted using feedback from diagnostic results. These approaches highlight specialisation, multimodality, and\\ninteractive reasoning as key principles for developing agent-based systems in medical diagnosis.\\n6.1.2\\nMolecular Discovery and Symbolic Reasoning\\nMolecular discovery within biomedical domains demands precise symbolic reasoning over chemical structures,\\nreaction pathways, and pharmacological constraints (Bilodeau et al., 2022; Makke and Chawla, 2024; M. Bran\\net al., 2024). To support molecular discovery, recent agent-based systems have introduced tailored techniques\\nsuch as integrating chemical analysis tools, enhancing memory for knowledge retention, and enabling multi-\\nagent collaboration (McNaughton et al., 2024; Inoue et al., 2025). One key approach is domain-specific tool\\nintegration, which allows agents to perform chemical reasoning through interaction with executable chemical\\noperations. For instance, CACTUS (McNaughton et al., 2024) equips agents with cheminformatics tools such\\nas RDKit (Landrum, 2013) to ensure the generation of chemically valid outputs. By grounding reasoning in\\ndomain-specific toolsets, CACTUS achieves significantly better performance than agents without tool integration.\\nSimilarly, LLM-RDF (M. Bran et al., 2024) automates chemical synthesis by coordinating specialised agents,\\neach responsible for a specific task and equipped with corresponding tools for literature mining, synthesis\\nplanning, or reaction optimisation.\\nAnother prominent line of research leverages memory-enabled reasoning (Hu et al., 2025c; Inoue et al., 2025),\\nwhere agents learn from prior experience by recording how previous problems were solved. ChemAgent (Tang\\net al., 2025a) breaks down complex chemical tasks into smaller subtasks, which are stored within a structured\\nmemory module, enabling efficient retrieval and refinement. OSDA Agent (Hu et al., 2025c) extends this approach\\nby introducing a self-reflective mechanism, where failed molecule proposals are abstracted into structured\\nmemory updates that inform and enhance future decision-making. In parallel, multi-agent coordination provides\\nadditional benefits. DrugAgent (Inoue et al., 2025) introduces a coordinator architecture that integrates\\nevidence from machine learning-based predictors, biomedical knowledge graphs, and literature search agents. It\\nemploys Chain-of-Thought and ReAct (Yao et al., 2023b) frameworks to support interpretable, multi-source\\n27\\nreasoning. LIDDIA (Averly et al., 2025) generalises this design by assigning modular roles, i.e., reasoner,\\nexecutor, evaluator, and memory, which collectively emulate iterative workflows in medicinal chemistry and\\nfacilitate multi-objective molecule evaluation.\\n6.2\\nDomain-Specific Optimisation in Programming\\nIn the programming domain, agent optimisation focuses on aligning agent behaviours with the procedural and\\noperational requirements of established software engineering workflows. Recent studies have demonstrated the\\neffectiveness of domain-specific agent design in two key application areas: code refinement (Rasheed et al., 2024;\\nTang et al., 2024; Pan et al., 2025b) and code debugging (Lee et al., 2024a; Puvvadi et al., 2025; Adnan et al.,\\n2025). In what follows, we examine representative agent optimisation strategies within these two domains.\\n6.2.1\\nCode Refinement\\nCode refinement involves the iterative improvement of code quality, structure, and correctness while preserving\\nits original functionality (Yang et al., 2024d; He et al., 2025; Islam et al., 2025). Recent studies have increasingly\\ninvestigated agent-based systems that support domain-specific optimisation for this task, focusing on self-\\nimprovement, collaborative workflows, and integration with programming tools (Madaan et al., 2023; Tang et al.,\\n2024; Rahman et al., 2025). These systems are designed to emulate human-in-the-loop refinement processes,\\nenforce adherence to software engineering best practices, and ensure that code remains robust, readable, and\\nmaintainable throughout iterative development cycles. One critical optimisation strategy involves self-feedback\\nmechanisms, where agents critique and revise their own outputs. For example, Self-Refine (Madaan et al.,\\n2023) introduces a lightweight framework in which a language model generates natural language feedback on its\\nown outputs and subsequently revises the code accordingly. Similarly, CodeCriticBench (Zhang et al., 2025a)\\npresents a comprehensive benchmark designed to assess the self-critiquing and refinement capabilities of LLMs,\\nwhere agents are evaluated on their ability to identify, explain, and revise code defects through structured\\nnatural language feedback. LLM-Surgeon (van der Ouderaa et al., 2023) proposes a systematic framework\\nin which a language model diagnoses structural and semantic issues within its own code outputs and applies\\ntargeted edits based on learned repair patterns, thereby optimising code quality while preserving functionality.\\nThese approaches eliminate the need for task-specific retraining, providing consistent improvements in code\\nquality.\\nAnother line of research explores experience-driven learning, where agents improve their problem-solving\\ncapabilities by relying on memory-enabled reasoning, systematically recording and reusing solutions to previously\\nencountered tasks (Wang et al., 2025g; Tang et al., 2024; Pan et al., 2025b). For example, AgentCoder (Huang\\net al., 2023a) and CodeAgent (Tang et al., 2024) simulate collaborative development workflows by assigning\\nspecialised roles to individual agents, such as coder, reviewer, and tester, which iteratively improve code\\nthrough structured dialogue cycles. These systems support collective evaluation and revision, promoting role\\nspecialisation and deliberative decision-making. Additionally, tool-enhanced frameworks such as CodeCoR (Pan\\net al., 2025b) and OpenHands (Wang et al., 2025g) incorporate external tools and modular agent interactions\\nto facilitate dynamic code pruning, patch generation, and context-aware refinement. VFlow (Wei et al., 2025b)\\nreformulates the workflow optimisation problem of Verilog code generation task as a search task on a graph of\\nLLM nodes with code-based representations, employing a Cooperative Evolution with Past Experience MCTS\\n(CEPE-MCTS) algorithm. These developments highlight iterative feedback, modular design, and interactive\\nreasoning as essential principles for building adaptive agent-based systems for code refinement.\\n6.2.2\\nCode Debugging\\nCode debugging presents intricate challenges that require precise fault localisation, execution-aware reasoning,\\nand iterative correction. These capabilities are typically absent in general-purpose LLMs (Puvvadi et al.,\\n2025; Mannadiar and Vangheluwe, 2010). To address these challenges, domain-specific optimisation focuses on\\naligning agent roles and workflows with the structured reasoning patterns and tool usage observed in human\\ndebugging practices. A key strategy involves leveraging runtime feedback to facilitate self-correction. For\\nexample, Self-Debugging (Chen et al., 2024c) and Self-Edit (Zhang et al., 2023a) exemplify this approach by\\nincorporating execution traces into the debugging process. These agents operate through internal cycles of fault\\nidentification, natural language-based reasoning, and targeted code revision, enabling autonomous debugging\\nwithout external supervision.\\n28\\nRecent research has explored modular agent architectures specifically designed to support the multi-stage\\nstructure of debugging workflows. For instance, PyCapsule (Adnan et al., 2025) introduces a separation of\\nresponsibilities between a programmer agent and an executor agent, thereby distinguishing code generation from\\nsemantic validation. More advanced systems, including Self-Collaboration (Dong et al., 2024) and RGD (Jin\\net al., 2024), employ collaborative pipelines in which agents are assigned specialised roles such as tester, reviewer,\\nor feedback analyser, mirroring professional debugging practices. Additionally, FixAgent (Lee et al., 2024a)\\nextends this paradigm through hierarchical agent activation, dynamically dispatching different agents based on\\nbug complexity and required depth of analysis.\\n6.3\\nDomain-Specific Optimisation in Financial and Legal Research\\nIn financial and legal domains, agent optimisation focuses on tailoring multi-agent architectures, reasoning\\nstrategies, and tool integration to the procedural and operational demands of domain-specific workflows (Sun\\net al., 2024b; He et al., 2024; Li et al., 2025f). Recent studies have demonstrated the effectiveness of such\\ndomain-specific designs in two key application areas: financial decision-making (Li et al., 2023c; Yu et al., 2024b;\\nWang et al., 2024j) and legal reasoning (Di Martino et al., 2023; Chen et al., 2025a), where modular design,\\ncollaborative interaction, and rule-grounded reasoning are essential for reliable performance. In what follows,\\nwe examine representative agent optimisation strategies within these two domains.\\n6.3.1\\nFinancial Decision-Making\\nFinancial decision-making requires agents to operate under uncertain and rapidly changing conditions, reason\\nover volatile market dynamics, and integrate heterogeneous information sources such as numerical indicators,\\nnews sentiment, and expert knowledge (Li et al., 2023c; Sarin et al., 2024; Chudziak and Wawer, 2025). In\\nresponse to these domain-specific demands, recent research has focused on developing multi-agent architectures\\ntailored to the procedural and cognitive requirements of financial environments (Fatemi and Hu, 2024; Luo et al.,\\n2025b). One critical strategy involves conceptual and collaborative agent design. For instance, FinCon (Yu et al.,\\n2024b) proposes a synthesised multi-agent system built on LLMs, employing conceptual verbal reinforcement\\nand domain-adaptive fine-tuning to enhance decision stability and policy alignment in dynamic markets.\\nPEER (Wang et al., 2024j) extends this paradigm through a modular agent architecture comprising expert,\\nretriever, and controller roles, which interact under a unified tuning mechanism to balance task specialisation\\nwith general adaptability. FinRobot (Yang et al., 2024b) further advances this line of work by integrating\\nexternal tools for model-grounded reasoning, enabling agents to connect high-level strategies with executable\\nfinancial models and real-time data streams.\\nAnother line of work on agent optimisation for financial decision-making focuses on sentiment analysis and\\nreporting (Xing, 2025; Tian et al., 2025; Raza et al., 2025). Heterogeneous LLM agent architectures (Xing, 2025)\\nenhance robustness in financial reporting by combining specialised sentiment modules with rule-based validators\\nto ensure compliance with domain-specific guidelines. Similarly, template-based reporting frameworks (Tian\\net al., 2025) decompose report generation into agent-driven retrieval, validation, and synthesis stages, enabling\\niterative refinement through real-world feedback. These approaches demonstrate the potential of self-evolving\\nmulti-agent systems to provide reliable, interpretable, and context-aware decision support in complex financial\\nenvironments.\\n6.3.2\\nLegal Reasoning\\nLegal reasoning requires agents to interpret structured legal rules, analyse case-specific evidence, and produce\\noutputs that are consistent with institutional regulations and judicial standards (Xu and Ju, 2023; Yuan\\net al., 2024c; Jiang and Yang, 2025). To address these domain-specific demands, recent research has explored\\nmulti-agent systems tailored to the procedural and interpretive requirements of legal settings (Di Martino et al.,\\n2023; Hu and Shu, 2023; Chen et al., 2025a). One significant direction involves collaborative agent frameworks\\nthat simulate judicial processes and support structured argumentation. For instance, LawLuo (Sun et al., 2024b)\\nintroduces a co-run multiagent architecture in which legal agents are assigned specialised roles such as document\\ndrafting, legal argument generation, and compliance validation, all operating under the supervision of a central\\ncontroller to ensure procedural consistency and legal correctness. Multi-Agent Justice Simulation (Di Martino\\net al., 2023) and AgentCourt (Chen et al., 2025a) extend this paradigm to model adversarial trial procedures,\\nenabling agents to participate in role-based interactions that emulate real-world courtroom dynamics. In\\n29\\nparticular, AgentCourt incorporates self-evolving lawyer agents that refine their strategies through reflective\\nself-play, leading to improved debate quality and enhanced procedural realism.\\nAnother line of work focuses on structured legal reasoning and domain-grounded interpretability. LegalGPT (Shi\\net al., 2024b) integrates a legal chain-of-thought framework within a multi-agent system, guiding legal reasoning\\nthrough interpretable and rule-aligned steps. Similarly, AgentsCourt (He et al., 2024) combines courtroom\\ndebate simulation with legal knowledge augmentation, enabling agents to perform judicial decision-making\\ngrounded in codified rules and case precedents. These approaches highlight the importance of rule grounding,\\nmodular role design, and collaborative reasoning in the development of robust, transparent, and legally reliable\\nagent systems.\\n7\\nEvaluation\\nThe rapid emergence of autonomous LLM-based agents has underscored the need for rigorous, multidimensional\\nevaluation frameworks. As these agents are deployed across increasingly diverse tasks and environments, recent\\nresearch has introduced a range of benchmarks and methodologies to assess not only task completion but also\\nreasoning quality, generalisation ability, and compliance with safety and alignment standards. Evaluation is no\\nlonger viewed as a static endpoint but as a dynamic feedback mechanism: fine-grained performance signals are\\nnow used to guide agent optimisation, prompt refinement, and dataset augmentation, enabling self-evolving\\nsystems that continuously acquire new capabilities and address failure cases. Current evaluation paradigms\\nencompass structured benchmark tasks with standardised metrics, safety- and alignment-oriented audits, and\\nLLM-as-a-judge approaches that leverage large models as flexible, scalable evaluators.\\n7.1\\nBenchmark-based Evaluation\\n7.1.1\\nTool and API-Driven Agents\\nTool-augmented agents are evaluated based on their ability to invoke external APIs and functions to solve\\nproblems that exceed the scope of their intrinsic knowledge. Benchmarks such as ToolBench (Xu et al., 2023),\\nAPI-Bank (Li et al., 2023b), MetaTool (Huang et al., 2023b), and ToolQA (Zhuang et al., 2023) define tasks\\nthat require tool usage and assess both the correctness and efficiency of API calls. Many of these evaluations\\nemploy simulated APIs or sandboxed environments, measuring task success alongside interaction efficiency.\\nEarly studies have shown that agents often overfit to specific tool schemas, exhibiting limited generalisation to\\npreviously unseen APIs. To address this limitation, recent benchmarks such as GTA (Wang et al., 2024b) and\\nAppWorld (Trivedi et al., 2024) introduce more realistic, multi-step tasks that require planning and coordination\\nacross multiple tools, while placing greater emphasis on process-oriented evaluation metrics. This trend reflects\\na broader shift towards richer, reasoning-aware evaluations that assess not only final outcomes but also the\\nquality of the decision-making process.\\n7.1.2\\nWeb Navigation and Browsing Agents\\nWeb agents are evaluated on their ability to interact with websites, extract information, and complete real-\\nworld online tasks.\\nBenchmarks such as BrowseComp (Wei et al., 2025a), MM-BrosweComp (Li et al.,\\n2025e), WebArena (Zhou et al., 2023b), VisualWebArena (Koh et al., 2024), WebCanvas (Pan et al., 2024b),\\nWebWalker (Wu et al., 2025b), and AgentBench (Liu et al., 2023a) have progressively increased the realism\\nand diversity of web-based evaluations, spanning simulated and live environments. These benchmarks test\\nnavigation skills, adaptability to interface changes, and the integration of textual and visual information.\\nRecent work incorporates intermediate metrics (e.g., sub-goal completion) and robustness assessments, though\\nreproducibility and generalisation remain challenging due to the dynamic nature of the web.\\n7.1.3\\nMulti-Agent Collaboration and Generalists\\nAs agents become more general-purpose, new benchmarks target multi-agent coordination and cross-domain\\ncompetence. MultiAgentBench (Zhu et al., 2025) and SwarmBench (Ruan et al., 2025) evaluate collaboration,\\ncompetition, and decentralised coordination among LLM agents, assessing both task completion and the quality\\nof communication and strategy. Generalist benchmarks such as GAIA (Mialon et al., 2023) and AgentBench (Liu\\n30\\net al., 2023a) test adaptability across diverse environments, from web navigation to coding and database queries.\\nRecent work, Wang et al. (2025b) further explores the GAIA benchmark to analyse the efficiencyâ€“effectiveness\\ntrade-off in agentic systems, proposing Efficient Agents, a framework that achieves competitive performance\\nwith significantly reduced operational costs. These evaluations highlight challenges in aggregating metrics across\\nheterogeneous tasks, risks of overfitting to narrow scenarios, and the need for unified, holistic leaderboards.\\n7.1.4\\nGUI and Multimodal Environment Agents\\nGUI and multimodal benchmarks challenge agents to operate in rich, interactive environments that combine\\ntextual and visual inputs. Mobile-Bench (Deng et al., 2024), AndroidWorld (Rawles et al., 2024), CRAB (Xu\\net al., 2024a), GUI-World (Chen et al., 2024a), and OSWorld (Xie et al., 2024) simulate realistic apps and\\noperating systems, requiring complex action sequences. Tasks often combine natural language understanding,\\nvisual perception, and API invocation. Evaluations measure task success, state management, perception\\naccuracy, and adaptability to GUI changes. However, the diversity of GUI environments makes standardisation\\nand reproducibility difficult, and agents remain brittle when faced with interface variability.\\n7.1.5\\nDomain-Specific Task Agents\\nDomain-focused benchmarks in coding (SWE-bench (Jimenez et al., 2024)), data science (DataSciBench (Zhang\\net al., 2025c), MLGym (Nathani et al., 2025)), enterprise productivity (WorkBench (Styles et al., 2024)), and\\nscientific research (OpenAGI (Ge et al., 2023), SUPER (Bogin et al., 2024)) assess specialised competencies that\\nintegrate planning, tool use, and adherence to domain norms. SWE-bench, for example, evaluates code-editing\\nagents on real GitHub repositories, while AgentClinic (Schmidgall et al., 2024) and MMedAgent (Li et al.,\\n2024a) test multimodal reasoning in clinical settings. Evaluation criteria have expanded from binary success\\nmeasures to encompass metrics such as test pass rates, policy adherence, and conformity to domain-specific\\nconstraints. Despite these advances, inconsistencies in metric definitions and persistent gaps in generalisation\\nremain significant challenges.\\n7.2\\nLLM-based Evaluation\\n7.2.1\\nLLM-as-a-Judge\\nThe LLM-as-a-Judge paradigm refers to employing large language models to assess the quality of outputs\\ngenerated by AI systems, such as text, code, or conversational responses, via structured prompts (Arabzadeh\\net al., 2024; Li et al., 2024b; Qian et al., 2025b). This approach has attracted attention as a scalable and\\ncost-effective alternative to conventional evaluation methods, including human judgment and automatic metrics\\n(e.g., BLEU, ROUGE), which often fail to capture semantic depth or coherence (Arabzadeh et al., 2024). LLM\\njudges typically operate in two modes: pointwise evaluation (Ruan et al., 2024), where outputs are scored\\ndirectly against criteria such as factuality and helpfulness, and pairwise comparison, where two outputs are\\ncompared and the preferred one is selected with justification (Li et al., 2024b; Zhao et al., 2025b).\\nRecent studies demonstrate that LLM-based evaluations can correlate with human judgments, in some cases\\nreaching parity with inter-annotator agreement levels (Arabzadeh et al., 2024). However, these methods are\\nsensitive to prompt design and susceptible to biases introduced by subtle instructional variations (Arabzadeh\\net al., 2024; Zhao et al., 2025b). Furthermore, single-step, output-focused evaluations may overlook the\\nreasoning depth in multi-step processes (Zhuge et al., 2024b; Wang et al., 2025h). To address these limitations,\\nenhancements have been proposed, including multi-agent deliberation frameworks such as CollabEval (Qian\\net al., 2025b) and structured meta-evaluation benchmarks to calibrate and improve the reliability of LLM\\njudges (Li et al., 2024b; Zhao et al., 2025b).\\n7.2.2\\nAgent-as-a-Judge\\nThe Agent-as-a-Judge framework extends LLM-based evaluation by employing full-fledged agentic systems\\ncapable of multi-step reasoning, state management, and tool use to critique other AI agents (Zhuge et al.,\\n2024b; Zhao et al., 2025b; Qian et al., 2025b). Different from traditional LLM judges, which focus solely\\non final outputs, agent judges evaluate the entire reasoning trajectory, capturing decision-making processes\\nand intermediate actions (Zhuge et al., 2024b). For example, Zhuge et al. (2024b) applied an agent judge\\n31\\nto the DevAI benchmark for code-generation agents. The framework incorporated specialised modules to\\nanalyse intermediate artefacts, construct reasoning graphs, and validate hierarchical requirements, resulting in\\nevaluations that aligned more closely with human expert judgments than traditional LLM-based approaches.\\nAgent judges also delivered substantial efficiency gains, reducing evaluation time and cost relative to manual\\nreview (Zhuge et al., 2024b; Zhao et al., 2025b).\\nNevertheless, implementing the Agent-as-a-Judge methodology introduces additional complexity and raises\\nchallenges for generalisation to domains other than code generation.\\nCurrent research seeks to improve\\nadaptability and simplify deployment across a broader range of AI tasks (Zhao et al., 2025b; Qian et al., 2025b).\\n7.3\\nSafety, Alignment, and Robustness in Lifelong Self-Evolving Agents\\nIn the context of the Three Laws of Self-Evolving AI Agents, Endure, the maintenance of safety and\\nstability during any modification, forms the primary constraint on all other forms of adaptation. For lifelong,\\nself-evolving agentic systems, safety is not a one-off certification but an ongoing requirement: every evolution\\nstep, from prompt updates to topology changes, must be assessed for unintended or malicious behaviours. This\\nnecessitates evaluation protocols that are continuous, granular, and scalable, ensuring that agents can remain\\naligned while adapting over extended lifetimes.\\nRecent work has introduced diverse evaluation paradigms. Risk-focused benchmarks such as AgentHarm (An-\\ndriushchenko et al., 2025) measure an agentâ€™s propensity to comply with explicitly malicious multi-step\\nrequestsâ€”requiring coherent tool use to execute harmful objectives such as fraud or cybercrime, revealing that\\neven leading LLMs can be coaxed into complex unsafe behaviours with minimal prompting. Domain-specific\\nprobes such as RedCode (Guo et al., 2024a) (code security) and MobileSafetyBench (Lee et al., 2024c)\\n(mobile control) stress-test agents in realistic, sandboxed environments. Behavioural probes like MACHI-\\nAVELLI (Pan et al., 2023) explore whether agents develop unethical, power-seeking strategies under reward\\noptimisation, highlighting the interplay between Endure and Excel, safe adaptation must not degrade core task\\ncompetence.\\nMeta-evaluation approaches, e.g., Agent-as-a-Judge (Zhuge et al., 2024b), AgentEval (Arabzadeh et al.,\\n2024), and R-Judge (Yuan et al., 2024b) â€“ position LLM agents themselves as evaluators or safety monitors,\\noffering scalable oversight but also exposing the limitations of current â€œrisk awareness.â€ These studies underline\\nthe multi-dimensional nature of safety, where accuracy alone is insufficient; over-reliance on correctness\\nmetrics can conceal epistemic risks and systemic biases (Li et al., 2025j). Legal alignment tests such as\\nSafeLawBench (Cao et al., 2025) further show that even state-of-the-art models struggle to satisfy established\\nlegal principles, reflecting the difficulty of codifying alignment in domains with open-textured norms.\\nDespite these advances, most current evaluations are snapshot-based, assessing agents at a single point in time.\\nFor MASE systems, safety evaluation must itself become dynamic â€“ continuously monitoring, diagnosing, and\\ncorrecting behaviours as the system evolves. Developing longitudinal, evolution-aware benchmarks that track\\nsafety, alignment, and robustness across the full lifecycle of an agent ecosystem remains an open and urgent\\nchallenge.\\n8\\nChallenges and Future Directions\\nDespite rapid advances, the evolution and optimisation of AI agents still face fundamental obstacles. These\\nchallenges are closely tied to the Three Laws of Self-Evolving AI Agents and need to be addressed to\\nrealise the vision of lifelong agentic systems. We group the key open problems accordingly.\\n8.1\\nChallenges\\n8.1.1\\nEndure â€“ Safety Adaptation\\n(1) Safety, Regulation, andAlignment. Most optimisation pipelines prioritise task metrics over safety constraints,\\nneglecting risks such as unintended behaviours, privacy breaches, and misaligned objectives. The dynamic\\nnature of evolving agents undermines existing legal frameworks (e.g., EU AI Act, GDPR), which assume\\nstatic models and fixed decision logic. This calls for new evolution-aware audit mechanisms, adaptive\\n32\\nlicences, provable-safety sandboxes, and legal protocols capable of tracking and constraining an agentâ€™s\\nself-directed evolutionary path.\\n(2) Reward Modelling and Optimisation Instability. Learned reward models for intermediate reasoning steps\\noften suffer from dataset scarcity, noisy supervision, and feedback inconsistency, leading to unstable or\\ndivergent agent behaviours. Stability is central to safety: even small perturbations in inputs or update\\nrules can undermine the trustworthiness of an evolving workflow.\\n8.1.2\\nExcel â€“ Performance Preservation\\n(1) Evaluation in Scientific and Domain-Specific Scenarios. In domains like biomedicine or law, reliable ground\\ntruth is often absent or disputed, complicating the construction of trustworthy feedback signals for\\noptimisation.\\n(2) Balancing Efficiency and Effectiveness in MAS Optimisation. Large-scale multi-agent optimisation improves\\ntask performance but incurs significant computational cost, latency, and instability. Designing methods\\nthat explicitly trade off effectiveness against efficiency remains unresolved.\\n(3) TransferabilityofOptimisedPromptsandTopologies. Optimised prompts or agent topologies are often brittle,\\nshowing poor generalisation across LLM backbones with differing reasoning abilities. This undermines\\nscalability and reusability in production settings.\\n8.1.3\\nEvolve â€“ Autonomous Optimisation\\n(1) Optimisation in Multimodal and Spatial Environments. Most optimisation algorithms are text-only, yet real-\\nworld agents must process multimodal inputs and reason in spatially grounded or continuous environments.\\nThis demands internal world models and perceptualâ€“temporal reasoning.\\n(2) Tool Use and Creation. Current methods typically assume a fixed toolset, overlooking the autonomous\\ndiscovery, adaptation, and co-evolution of tools alongside agents.\\n8.2\\nFuture Directions\\nLooking forward, many of these limitations point to promising research avenues. We highlight several directions\\nand link them to their role in the MOPâ†’MOAâ†’MAOâ†’MASE paradigm shift.\\n(1) Simulated Environments for Fully Autonomous Self-Evolution (MASE). Develop open-ended, interactive\\nsimulation platforms where agents can iteratively interact, receive feedback, and refine prompts, memory,\\ntools, and workflows via closed-loop optimisation.\\n(2) Advancing Tool Use and Creation (MAOâ†’MASE). Move beyond static tool invocation toward agents that\\nadaptively select, compose, or create tools. Incorporate reinforcement learning and feedback-driven\\nstrategies, paired with robust evaluation pipelines.\\n(3) Real-World Evaluation and Benchmarking (Cross-stage). Create benchmarks and protocols that reflect\\nreal-world complexity, support interaction-based and longitudinal assessment, and align with long-term\\nimprovement signals.\\n(4) Effectivenessâ€“Efficiency Trade-offs in MAS Optimisation (MAO). Design optimisation algorithms that jointly\\nmodel performance and resource constraints, enabling MAS deployment under strict latency, cost, or\\nenergy budgets.\\n(5) Domain-Aware Evolution for Scientific and Specialised Applications (MASE). Tailor evolution methods to\\ndomain-specific constraints in science, medicine, law, or education, integrating heterogeneous knowledge\\nsources, bespoke evaluation criteria, and regulatory compliance.\\nOutlook. Addressing these challenges will require optimisation pipelines that are not only high-performing\\nand domain-adaptive, but also safe, regulation-aware, and self-sustaining. Embedding these solutions within\\nthe MOPâ†’MOAâ†’MAOâ†’MASE trajectory, and grounding them in the Three Laws of Self-Evolving AI\\nAgents, offers a coherent roadmap toward truly lifelong, autonomous agentic systems â€“ systems that can\\nendure, excel, and evolve across the full span of their operational lifetimes.\\n33\\n9\\nConclusions\\nIn this survey, we have presented a comprehensive overview of the emerging paradigm of self-evolving AI agents,\\nwhich bridge the static capabilities of foundation models with the continuous adaptability required by lifelong\\nagentic systems. We situated this evolution within a unified four-stage trajectory, from Model Offline Pretraining\\n(MOP) and Model Online Adaptation (MOA), through Multi-Agent Orchestration (MAO), and ultimately to\\nMulti-Agent Self-Evolving (MASE), highlighting the progressive shift from static, human-configured models to\\ndynamic, autonomous ecosystems.\\nTo formalise this transition, we introduced a conceptual framework that abstracts the feedback loop underlying\\nagent evolution, with four key components: Inputs, Agent System, Objectives, and Optimisers, that together\\ndetermine how agents improve through continual interaction with their environment. Building on this, we\\nsystematically reviewed optimisation techniques across agent components, domain-specific strategies, and\\nevaluation methodologies critical for building adaptive and resilient agentic systems.\\nWe also proposed the Three Laws of Self-Evolving AI Agents, Endure (safety adaptation), Excel\\n(performance preservation), and Evolve (autonomous evolution), as guiding principles to ensure that lifelong\\nself-improvement remains safe, effective, and aligned. These laws are not mere principles but practical design\\nconstraints, ensuring that the path toward autonomy remains aligned with safety, performance, and adaptability.\\nThey serve as the guardrails for the MASE paradigm, guiding research from narrow, single-shot optimisation\\ntoward continuous, open-ended self-improvement.\\nLooking forward, the ability to endure, excel, and evolve will be decisive for agents operating in dynamic,\\nreal-world environments, whether in scientific discovery, software engineering, or humanâ€“AI collaboration.\\nAchieving this will demand breakthroughs in scalable optimisation algorithms, lifelong evaluation protocols,\\nsafe coordination in heterogeneous agent environments, and mechanisms for adapting to unforeseen domains.\\nWe hope this survey serves as both a reference point and a call to action to build an ecosystem of self-evolving\\nAI agents that do not simply execute tasks, but live, learn, and last. By aligning technical innovation with\\nprincipled self-evolution, we can pave the way toward truly autonomous, resilient, and trustworthy lifelong\\nagentic systems.\\nAcknowledgements\\nWe would like to thank Shuyu Guo for his valuable contributions to the early-stage exploration and literature\\nreview on agent optimisation.\\nReferences\\nMuntasir Adnan, Zhiwei Xu, and Carlos CN Kuhn. Large language model guided self-debugging code generation. arXiv\\npreprint arXiv:2502.02928, 2025.\\nEshaan Agarwal, Joykirat Singh, Vivek Dani, Raghav Magazine, Tanuja Ganu, and Akshay Nambi. Promptwizard:\\nTask-aware prompt optimization framework. arXiv preprint arXiv:2405.18369, 2024.\\nKeivan Alizadeh, Seyed-Iman Mirzadeh, Dmitry Belenko, S. Khatamifard, Minsik Cho, Carlo C. del Mundo, Mohammad\\nRastegari, and Mehrdad Farajtabar. LLM in a flash: Efficient large language model inference with limited memory. In\\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 12562â€“12584, 2024.\\nMohammad Almansoori, Komal Kumar, and Hisham Cholakkal. Self-evolving multi-agent simulations for realistic\\nclinical interactions. arXiv preprint arXiv:2503.22678, 2025.\\nMaksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks,\\nAndy Zou, J. Zico Kolter, Matt Fredrikson, Yarin Gal, and Xander Davies. AgentHarm: A benchmark for measuring\\nharmfulness of LLM agents. In The Thirteenth International Conference on Learning Representations, 2025.\\nNegar Arabzadeh, Julia Kiseleva, Qingyun Wu, Chi Wang, Ahmed Awadallah, Victor Dibia, Adam Fourney, and Charles\\nClarke. Towards better human-agent alignment: Assessing task utility in llm-powered applications. arXiv preprint\\narXiv:2402.09015, 2024.\\n34\\nDerek Austin and Elliott Chartock. GRAD-SUM: Leveraging gradient summarization for optimal prompt engineering.\\narXiv preprint arXiv:2407.12865, 2024.\\nReza Averly, Frazier N Baker, and Xia Ning. LIDDIA: Language-based intelligent drug discovery agent. arXiv preprint\\narXiv:2502.13959, 2025.\\nNikolas Belle, Dakota Barnes, Alfonso Amayuelas, Ivan Bercovich, Xin Eric Wang, and William Wang. Agents of change:\\nSelf-evolving llm agents for strategic planning. arXiv preprint arXiv:2506.04651, 2025.\\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda,\\nTomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with\\nlarge language models. In Proceedings of the AAAI conference on artificial intelligence, pages 17682â€“17690, 2024.\\nZhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, and Yunhe Wang. Forest-of-thought: Scaling test-time compute for\\nenhancing LLM reasoning. In Forty-second International Conference on Machine Learning, 2025.\\nCamille Bilodeau, Wengong Jin, Tommi Jaakkola, Regina Barzilay, and Klavs F Jensen. Generative models for molecular\\ndiscovery: Recent advances and challenges. Wiley Interdisciplinary Reviews: Computational Molecular Science, 12(5):\\ne1608, 2022.\\nXiaohe Bo, Zeyu Zhang, Quanyu Dai, Xueyang Feng, Lei Wang, Rui Li, Xu Chen, and Ji-Rong Wen. Reflective\\nmulti-agent collaboration based on large language models. Advances in Neural Information Processing Systems, 37:\\n138595â€“138631, 2024.\\nBen Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, and Tushar\\nKhot. SUPER: evaluating agents on setting up and executing tasks from research repositories. In Proceedings of the\\n2024 Conference on Empirical Methods in Natural Language Processing, pages 12622â€“12645, 2024.\\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. In The\\nTwelfth International Conference on Learning Representations, 2024.\\nCAMEL-AI. Workforce â€” camel-ai documentation. https://docs.camel-ai.org/key_modules/workforce, 2025. Accessed:\\n2025-08-09.\\nChuxue Cao, Han Zhu, Jiaming Ji, Qichao Sun, Zhenghao Zhu, Yinyu Wu, Josef Dai, Yaodong Yang, Sirui Han, and\\nYike Guo. SafeLawBench: Towards safe alignment of large language models. In Findings of the Association for\\nComputational Linguistics, pages 14015â€“14048, 2025.\\nNicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Proceedings of the 2021\\nConference on Empirical Methods in Natural Language Processing, pages 6491â€“6506, 2021.\\nShuyang Cao and Lu Wang. AWESOME: GPU memory-constrained long document summarization using memory\\nmechanism and global salient content. In Proceedings of the 2024 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 5925â€“5941,\\n2024.\\nEdward Y Chang. Socrasynth: Multi-llm reasoning with conditional statistics. arXiv preprint arXiv:2402.06634, 2024.\\nGaoWei Chang and Agent Network Protocol Contributors. Agent Network Protocol (ANP). https://github.com/\\nagent-network-protocol/AgentNetworkProtocol. MIT License, accessed 2025-07-31.\\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. CodeT: Code\\ngeneration with generated tests. In The Eleventh International Conference on Learning Representations, 2023.\\nDongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Huichi Zhou, Qihui Zhang, Zhigang He, Yilin Bai, Chujie Gao,\\nLiuyi Chen, et al. GUI-world: A video benchmark and dataset for multimodal GUI-oriented understanding. In The\\nThirteenth International Conference on Learning Representations, 2024a.\\nGuangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, BÃ¶rje Karlsson, Jie Fu, and Yemin Shi. AutoAgents:\\nA framework for automatic agent generation. In Proceedings of the Thirty-Third International Joint Conference on\\nArtificial Intelligence, pages 22â€“30, 2024b.\\nGuhong Chen, Liyang Fan, Zihan Gong, Nan Xie, Zixuan Li, Ziqiang Liu, Chengming Li, Qiang Qu, Hamid Alinejad-\\nRokny, Shiwen Ni, and Min Yang. AgentCourt: Simulating court with adversarial evolvable lawyer agents. In Findings\\nof the Association for Computational Linguistics, pages 5850â€“5865. Association for Computational Linguistics, 2025a.\\nGuoxin Chen, Zhong Zhang, Xin Cong, Fangda Guo, Yesai Wu, Yankai Lin, Wenzheng Feng, and Yasheng Wang. Learning\\nevolving tools for large language models. In The Thirteenth International Conference on Learning Representations,\\n2025b.\\n35\\nKai Chen, Xinfeng Li, Tianpei Yang, Hewei Wang, Wei Dong, and Yang Gao. MDTeamGPT: A self-evolving llm-based\\nmulti-agent framework for multi-disciplinary team medical consultation. arXiv preprint arXiv:2503.13856, 2025c.\\nMingda Chen, Yang Li, Karthik Padthe, Rulin Shao, Alicia Yi Sun, Luke Zettlemoyer, Gargi Ghosh, and Wen-tau Yih.\\nImproving factuality with explicit working memory. In Proceedings of the 63rd Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 11199â€“11213, 2025d.\\nMingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, KeerLu, Bin CUI, Wentao Zhang, Zenan Zhou, and\\nWeipeng Chen. Facilitating multi-turn function calling for LLMs via compositional instruction tuning. In The\\nThirteenth International Conference on Learning Representations, 2025e.\\nNuo Chen, Hongguang Li, Jianhui Chang, Juhua Huang, Baoyuan Wang, and Jia Li. Compress to impress: Unleashing\\nthe potential of compressive memory in real-world long-term conversations. In Proceedings of the 31st International\\nConference on Computational Linguistics, pages 755â€“773, 2025f.\\nWeize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu,\\nand Maosong Sun. Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence. In The\\nThirteenth International Conference on Learning Representations, 2025g.\\nWeize Chen, Jiarui Yuan, Chen Qian, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Optima: Optimizing effectiveness\\nand efficiency for llm-based multi-agent system. In Findings of the Association for Computational Linguistics, pages\\n11534â€“11557. Association for Computational Linguistics, 2025h.\\nXi Chen, Huahui Yi, Mingke You, WeiZhi Liu, Li Wang, Hairui Li, Xue Zhang, Yingman Guo, Lei Fan, Gang Chen,\\net al. Enhancing diagnostic capability with multi-agents conversational large language models. NPJ digital medicine,\\n8(1):159, 2025i.\\nXinyun Chen, Maxwell Lin, Nathanael SchÃ¤rli, and Denny Zhou. Teaching large language models to Self-Debug. In The\\nTwelfth International Conference on Learning Representations, 2024c.\\nYuyang Cheng, Yumiao Xu, Chaojia Yu, and Yong Zhao. HAWK: A hierarchical workflow framework for multi-agent\\ncollaboration. arXiv preprint arXiv:2507.04067, 2025.\\nPrateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building production-ready ai\\nagents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025.\\nJarosÅ‚aw A Chudziak and MichaÅ‚ Wawer. ElliottAgents: a natural language-driven multi-agent system for stock market\\nanalysis and prediction. arXiv preprint arXiv:2507.03435, 2025.\\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and\\nZhiting Hu. RLPrompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022\\nConference on Empirical Methods in Natural Language Processing, pages 3369â€“3391, 2022.\\nShihan Deng, Weikai Xu, Hongda Sun, Wei Liu, Tao Tan, Jianfeng Liu, Ang Li, Jian Luan, Bin Wang, Rui Yan, and\\nShuo Shang. Mobile-Bench: An evaluation benchmark for llm-based mobile agents. In Proceedings of the 62nd Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8813â€“8831, 2024.\\nBeniamino Di Martino, Antonio Esposito, and Luigi Colucci Cante. Multi agents simulation of justice trials to support\\ncontrol management and reduction of civil trials duration. Journal of Ambient Intelligence and Humanized Computing,\\n14(4):3645â€“3657, 2023.\\nGuanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou,\\nand Ji-Rong Wen. Tool-Star: Empowering llm-brained multi-tool reasoner via reinforcement learning. arXiv preprint\\narXiv:2505.16410, 2025a.\\nGuanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang\\nWang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025b.\\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. ACM Transactions on\\nSoftware Engineering and Methodology, 33(7):1â€“38, 2024.\\nNorbert Donner-Banzhoff. Solving the diagnostic challenge: a patient-centered approach. The Annals of Family Medicine,\\n16(4):353â€“358, 2018.\\nYiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, and Jeff Z\\nPan. Rethinking memory in ai: Taxonomy, operations, topics, and future directions. arXiv preprint arXiv:2505.00675,\\n2025.\\n36\\nYu Du, Fangyun Wei, and Hongyang Zhang. AnyTool: Self-reflective, hierarchical agents for large-scale API calls. In\\nForty-first International Conference on Machine Learning, 2024.\\nSefika Efeoglu and Adrian Paschke.\\nRetrieval-augmented generation-based relation extraction.\\narXiv preprint\\narXiv:2404.13397, 2024.\\nSugyeong Eo, Hyeonseok Moon, Evelyn Hayoon Zi, Chanjun Park, and Heuiseok Lim. Debate only when necessary:\\nAdaptive multiagent collaboration for efficient llm reasoning. arXiv preprint arXiv:2504.05047, 2025.\\nAdibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, and Bo Wang. Medrax: Medical reasoning agent for chest\\nx-ray. arXiv preprint arXiv:2502.02673, 2025.\\nWei Fang, Yang Zhang, Kaizhi Qian, James Glass, and Yada Zhu. Play2prompt: Zero-shot tool instruction optimization\\nfor llm agents via tool play. arXiv preprint arXiv:2503.14432, 2025.\\nSorouralsadat Fatemi and Yuheng Hu. Finvision: A multi-agent framework for stock market prediction. In Proceedings\\nof the 5th ACM International Conference on AI in Finance, pages 582â€“590, 2024.\\nXiang Fei, Xiawu Zheng, and Hao Feng. MCP-Zero: Active tool discovery for autonomous llm agents. arXiv preprint\\narXiv:2506.01056, 2025.\\nJiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and\\nWanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025a.\\nJinghao Feng, Qiaoyu Zheng, Chaoyi Wu, Ziheng Zhao, Ya Zhang, Yanfeng Wang, and Weidi Xie. M3builder: A\\nmulti-agent system for automated machine learning in medical imaging. arXiv preprint arXiv:2502.20301, 2025b.\\nChrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim RocktÃ¤schel. Promptbreeder:\\nSelf-referential self-improvement via prompt evolution. In Forty-first International Conference on Machine Learning,\\n2024.\\nEmily First, Markus N Rabe, Talia Ringer, and Yuriy Brun. Baldur: Whole-proof generation and repair with large\\nlanguage models. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium\\non the Foundations of Software Engineering, pages 1229â€“1241, 2023.\\nAdam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Friederike Niedtner, Grace Proebsting,\\nGriffin Bassman, Jack Gerrits, Jacob Alber, et al. Magentic-one: A generalist multi-agent system for solving complex\\ntasks. arXiv preprint arXiv:2411.04468, 2024.\\nHongcheng Gao, Yue Liu, Yufei He, Longxu Dou, Chao Du, Zhijie Deng, Bryan Hooi, Min Lin, and Tianyu Pang.\\nFlowReasoner: Reinforcing query-level meta-agents. arXiv preprint arXiv:2504.15257, 2025a.\\nHuan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan\\nQi, Yiran Wu, et al. A survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint\\narXiv:2507.21046, 2025b.\\nShen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie Ren, Zhumin Chen, Jun Ma, and Zhaochun\\nRen. Confucius: Iterative tool learning from introspection feedback by easy-to-difficult curriculum. In Proceedings of\\nthe AAAI Conference on Artificial Intelligence, pages 18030â€“18038, 2024a.\\nYunfan Gao, Yun Xiong, Yijie Zhong, Yuxi Bi, Ming Xue, and Haofen Wang. Synergizing RAG and reasoning: A\\nsystematic review. arXiv preprint arXiv:2504.15909, 2025c.\\nZhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, and Qing Li. CLOVA: A closed-loop\\nvisual assistant with tool usage and update. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,\\npages 13258â€“13268. IEEE, 2024b.\\nZhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, and Qing\\nLi. Multi-modal agent tuning: Building a vlm-driven agent for efficient tool usage. In The Thirteenth International\\nConference on Learning Representations, 2025d.\\nYingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang, et al. OpenAGI: When llm\\nmeets domain experts. Advances in Neural Information Processing Systems, 36:5539â€“5568, 2023.\\nCaleb Geren, Amanda Board, Gaby G. Dagher, Tim Andersen, and Jun Zhuang. Blockchain for large language model\\nsecurity and safety: A holisticsurvey. SIGKDD Explorations, 26(2):1â€“20, 2024.\\n37\\nFatemeh Ghezloo, Mehmet Saygin Seyfioglu, Rustin Soraki, Wisdom O Ikezogwo, Beibin Li, Tejoram Vivekanandan,\\nJoann G Elmore, Ranjay Krishna, and Linda Shapiro. Pathfinder: A multi-modal multi-agent system for medical\\ndiagnostic decision-making applied to histopathology. arXiv preprint arXiv:2502.08916, 2025.\\nAnna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, and Christopher D.Manning Manning. Synthetic data generation\\n& multi-step RL for reasoning & tool use. arXiv preprint arXiv:2504.04736, 2025.\\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. ToRA:\\nA tool-integrated reasoning agent for mathematical problem solving. In The Twelfth International Conference on\\nLearning Representations, 2024.\\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Let-\\nman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783,\\n2024.\\nZhouhong Gu, Xiaoxuan Zhu, Yin Cai, Hao Shen, Xingzhou Chen, Qingyi Wang, Jialin Li, Xiaoran Shi, Haoran Guo,\\nWenxuan Huang, et al. AgentGroupChat-V2: Divide-and-conquer is what llm-based multi-agent system need. arXiv\\npreprint arXiv:2506.15451, 2025.\\nChengquan Guo, Xun Liu, Chulin Xie, Andy Zhou, Yi Zeng, Zinan Lin, Dawn Song, and Bo Li. Redcode: Risky\\ncode execution and generation benchmark for code agents. Advances in Neural Information Processing Systems, 37:\\n106190â€“106236, 2024a.\\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang,\\nXiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\\narXiv:2501.12948, 2025.\\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang.\\nEvoPrompt: Connecting llms with evolutionary algorithms yields powerful prompt optimizers. In The Twelfth\\nInternational Conference on Learning Representations, 2024b.\\nTaicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang\\nZhang. Large language model based multi-agents: A survey of progress and challenges. In Proceedings of the\\nThirty-Third International Joint Conference on Artificial Intelligence, pages 8048â€“8057, 2024c.\\nZhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu.\\nStableToolBench: Towards stable large-scale benchmarking on tool learning of large language models. In Findings of\\nthe Association for Computational Linguistics, pages 11143â€“11156, 2024d.\\nBernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. HippoRAG: Neurobiologically inspired\\nlong-term memory for large language models. In Advances in Neural Information Processing Systems, 2024.\\nShanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, and Zhaozhuo Xu. LLM multi-agent systems: Challenges and\\nopen problems. arXiv preprint arXiv:2402.03578, 2024.\\nJunda He, Christoph Treude, and David Lo. LLM-based multi-agent systems for software engineering: Literature review,\\nvision, and the road ahead. ACM Transactions on Software Engineering and Methodology, 34(5):1â€“30, 2025.\\nZhitao He, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, and\\nJun Zhao. AgentsCourt: Building judicial decision-making agents with court debate simulation and legal knowledge\\naugmentation. arXiv preprint arXiv:2403.02959, 2024.\\nSirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang,\\nSteven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and JÃ¼rgen Schmidhuber.\\nMetaGPT: Meta programming for A multi-agent collaborative framework. In The Twelfth International Conference\\non Learning Representations, 2024.\\nYuki Hou, Haruki Tamoto, and Homei Miyashita. â€œmy agent understands me betterâ€: Integrating dynamic human-like\\nmemory recall and consolidation in llm-based agents. In Extended Abstracts of the CHI Conference on Human Factors\\nin Computing Systems, CHI â€™24, page 1â€“7. ACM, May 2024.\\nZhipeng Hou, Junyi Tang, and Yipeng Wang. Halo: Hierarchical autonomous logic-oriented orchestration for multi-agent\\nllm systems. arXiv preprint arXiv:2505.13516, 2025.\\nCho-Jui Hsieh, Si Si, Felix X. Yu, and Inderjit S. Dhillon. Automatic engineering of long prompts. In Findings of the\\nAssociation for Computational Linguistics, pages 10672â€“10685, 2024.\\n38\\nChenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. ChatDB: Augmenting llms with\\ndatabases as their symbolic memory. arXiv preprint arXiv:2306.03901, 2023.\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\\nChen. LoRA: Low-rank adaptation of large language models. In The Tenth International Conference on Learning\\nRepresentations, 2022.\\nShengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. In The Thirteenth International Conference\\non Learning Representations, 2025a.\\nWenyang Hu, Yao Shu, Zongmin Yu, Zhaoxuan Wu, Xiaoqiang Lin, Zhongxiang Dai, See-Kiong Ng, and Bryan\\nKian Hsiang Low. Localized zeroth-order prompt optimization. Advances in Neural Information Processing Systems,\\n37:86309â€“86345, 2024.\\nXueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu\\nZhao, et al. OS agents: A survey on mllm-based agents for computer, phone and browser use. In Proceedings of the\\n63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7436â€“7465,\\n2025b.\\nZhaolin Hu, Yixiao Zhou, Zhongan Wang, Xin Li, Weimin Yang, Hehe Fan, and Yi Yang. Osda agent: Leveraging large\\nlanguage models for de novo design of organic structure directing agents. In The Thirteenth International Conference\\non Learning Representations, 2025c.\\nZhiting Hu and Tianmin Shu. Language models, agent models, and world models: The law for machine reasoning and\\nplanning. arXiv preprint arXiv:2312.05230, 2023.\\nChengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi,\\nand Dong Yu. R-Zero: Self-evolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004, 2025.\\nDong Huang, Jie M Zhang, Michael Luck, Qingwen Bu, Yuhao Qing, and Heming Cui. AgentCoder: Multi-agent-based\\ncode generation with iterative testing and optimisation. arXiv preprint arXiv:2312.13010, 2023a.\\nJen-tse Huang, Jiaxu Zhou, Tailin Jin, Xuhui Zhou, Zixi Chen, Wenxuan Wang, Youliang Yuan, Michael R Lyu,\\nand Maarten Sap. On the resilience of llm-based multi-agent collaboration with faulty agents. arXiv preprint\\narXiv:2408.00989, 2024a.\\nXu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and\\nEnhong Chen. Understanding the planning of llm agents: A survey. arXiv preprint arXiv:2402.02716, 2024b.\\nYue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang\\nGong, et al. Metatool benchmark for large language models: Deciding whether to use tools and which to use. arXiv\\npreprint arXiv:2310.03128, 2023b.\\nZhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan,\\nand Pengfei Liu. O1 replication journeyâ€“part 2: Surpassing o1-preview through simple distillation, big progress or\\nbitter lesson? arXiv preprint arXiv:2411.16489, 2024c.\\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming\\nLu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024.\\nYoshitaka Inoue, Tianci Song, Xinling Wang, Augustin Luna, and Tianfan Fu. Drugagent: Multi-agent large language\\nmodel-based reasoning for drug-target interaction prediction. In ICLR 2025 Workshop on Machine Learning for\\nGenomics Explorations, 2025.\\nMd. Ashraful Islam, Mohammed Eunus Ali, and Md. Rizwan Parvez. MapCoder: Multi-agent code generation for\\ncompetitive problem solving. In Proceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics, pages 4912â€“4944, 2024.\\nMd. Ashraful Islam, Mohammed Eunus Ali, and Md. Rizwan Parvez. CodeSim: Multi-agent code generation and\\nproblem solving through simulation-driven planning and debugging. In Findings of the Association for Computational\\nLinguistics: NAACL, 2025.\\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander\\nMadry, Alex Beutel, Alex Carney, et al. Openai o1 system card. CoRR, 2024.\\nCong Jiang and Xiaolei Yang. Agentsbench: A multi-agent llm simulation framework for legal judgment prediction.\\nSystems, 13(8):641, 2025.\\n39\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. A survey on large language models for code\\ngeneration. arXiv preprint arXiv:2406.00515, 2024.\\nFangkai Jiao, Chengwei Qin, Zhengyuan Liu, Nancy F. Chen, and Shafiq Joty. Learning planning-based reasoning by\\ntrajectories collection and process reward synthesizing. In Proceedings of the 2024 Conference on Empirical Methods\\nin Natural Language Processing, pages 334â€“350, 2024.\\nCarlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan.\\nSWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on\\nLearning Representations, 2024.\\nHaolin Jin, Zechao Sun, and Huaming Chen. RGD: Multi-LLM based agent debugger via refinement and generation\\nguidance. In 2024 IEEE International Conference on Agents (ICA), pages 136â€“141. IEEE, 2024.\\nMingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang, Wenyue Hua, Ruixiang Tang, William Yang Wang, and Yongfeng\\nZhang. Disentangling memory and reasoning ability in large language models. In Proceedings of the 63rd Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1681â€“1701, 2025.\\nZixuan Ke, Austin Xu, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, and Shafiq Joty. MAS-ZERO: Designing\\nmulti-agent systems with zero supervision. arXiv preprint arXiv:2505.14996, 2025.\\nAkbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette,\\nSamuel R. Bowman, Tim RocktÃ¤schel, and Ethan Perez. Debating with more persuasive llms leads to more truthful\\nanswers. In Forty-first International Conference on Machine Learning, 2024.\\nYubin Kim, Chanwoo Park, Hyewon Jeong, Yik S Chan, Xuhai Xu, Daniel McDuff, Hyeonhoon Lee, Marzyeh Ghassemi,\\nCynthia Breazeal, and Hae W Park. MDAgents: An adaptive collaboration of llms for medical decision-making.\\nAdvances in Neural Information Processing Systems, 37:79410â€“79452, 2024.\\nRonny Ko, Jiseong Jeong, Shuyuan Zheng, Chuan Xiao, Tae-Wan Kim, Makoto Onizuka, and Won-Yong Shin. Seven\\nsecurity challenges that must be solved in cross-domain multi-agent llm systems. arXiv preprint arXiv:2505.23847,\\n2025.\\nJing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan\\nZhou, Russ Salakhutdinov, and Daniel Fried. VisualWebArena: Evaluating multimodal agents on realistic visual web\\ntasks. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), pages 881â€“905, 2024.\\nDezhang Kong, Shi Lin, Zhenhua Xu, Zhebo Wang, Minghao Li, Yufeng Li, Yilun Zhang, Hujin Peng, Zeyang Sha,\\nYuyuan Li, Changting Lin, Xun Wang, Xuan Liu, Ningyu Zhang, Chaochao Chen, Muhammad Khurram Khan, and\\nMeng Han. A survey of llm-driven ai agent communication: Protocols, security risks, and defense countermeasures,\\n2025.\\nIgor Kononenko. Machine learning for medical diagnosis: history, state of the art and perspective. Artificial Intelligence\\nin Medicine, 23(1):89â€“109, 2001. ISSN 0933-3657. doi: https://doi.org/10.1016/S0933-3657(01)00077-X.\\nNaveen Krishnan. Advancing multi-agent systems through model context protocol: Architecture, implementation, and\\napplications. arXiv preprint arXiv:2504.21030, 2025.\\nBespoke\\nLabs.\\nBespoke-stratos:\\nThe\\nunreasonable\\neffectiveness\\nof\\nreasoning\\ndistillation.\\nwww.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation, 2025. Accessed:\\n2025-01-22.\\nHanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang,\\nYuxiao Dong, and Jie Tang. AutoWebGLM: A large language model-based web navigating agent. In Proceedings of\\nthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5295â€“5306. ACM, 2024a.\\nXin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-DPO: Step-wise preference\\noptimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024b.\\nNathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V\\nMiranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training.\\narXiv preprint arXiv:2411.15124, 2024.\\nGreg Landrum. Rdkit documentation. Release, 1(1-79):4, 2013.\\nCheryl Lee, Chunqiu Steven Xia, Longji Yang, Jen-tse Huang, Zhouruixin Zhu, Lingming Zhang, and Michael R Lyu. A\\nunified debugging approach via llm-based multi-agent synergy. arXiv preprint arXiv:2404.17153, 2024a.\\n40\\nDongkyu Lee, Chandana Satya Prakash, Jack FitzGerald, and Jens Lehmann. MATTER: memory-augmented transformer\\nusing heterogeneous knowledge sources. In Findings of the Association for Computational Linguistics, pages 16110â€“\\n16121, 2024b.\\nJuyong Lee, Dongyoon Hahm, June Suk Choi, W Bradley Knox, and Kimin Lee. MobileSafetyBench: Evaluating safety\\nof autonomous agents in mobile device control. arXiv preprint arXiv:2410.17520, 2024c.\\nKuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John F. Canny, and Ian Fischer. A human-inspired reading agent with\\ngist memory of very long contexts. In Forty-first International Conference on Machine Learning, 2024d.\\nHui Yi Leong and Yuqing Wu. DynaSwarm: Dynamically graph structure selection for llm-based multi-agent system.\\narXiv preprint arXiv:2507.23261, 2025.\\nBinxu Li, Tiankai Yan, Yuanting Pan, Jie Luo, Ruiyang Ji, Jiayuan Ding, Zhe Xu, Shilong Liu, Haoyu Dong, Zihao Lin,\\net al. MMedAgent: Learning to use medical tools with multi-modal agent. arXiv preprint arXiv:2407.02483, 2024a.\\nBoyi Li, Zhonghan Zhao, Der-Horng Lee, and Gaoang Wang. Adaptive graph pruning for multi-agent communication.\\narXiv preprint arXiv:2506.02951, 2025a.\\nChengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang\\nWang, Junyang Lin, et al. CoRT: Code-integrated reasoning within thinking. arXiv preprint arXiv:2506.09820, 2025b.\\nChengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang\\nLin, and Dayiheng Liu. START: Self-taught reasoner with tools. arXiv preprint arXiv:2503.04625, 2025c.\\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\\nCAMEL:\\nCommunicative agents for â€mindâ€ exploration of large language model society. In Thirty-seventh Conference on Neural\\nInformation Processing Systems, 2023a.\\nHaitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. LLMs-as-Judges: a\\ncomprehensive survey on llm-based evaluation methods. arXiv preprint arXiv:2412.05579, 2024b.\\nJunkai Li, Yunghwei Lai, Weitao Li, Jingyi Ren, Meng Zhang, Xinhui Kang, Siyu Wang, Peng Li, Ya-Qin Zhang, Weizhi\\nMa, et al. Agent hospital: A simulacrum of hospital with evolvable medical agents. arXiv preprint arXiv:2405.02957,\\n2024c.\\nMinghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li.\\nAPI-Bank: A comprehensive benchmark for tool-augmented llms. In Proceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing, pages 3102â€“3116, 2023b.\\nPengxiang Li, Zhi Gao, Bofei Zhang, Yapeng Mi, Xiaojian Ma, Chenrui Shi, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun\\nZhu, et al. Iterative tool usage exploration for multimodal agents via step-wise preference tuning. arXiv preprint\\narXiv:2504.21561, 2025d.\\nShilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli\\nOuyang, Wenbo Su, and Bo Zheng. GraphReader: Building graph-based agent to enhance long-context abilities of\\nlarge language models. In Findings of the Association for Computational Linguistics: EMNLP, pages 12758â€“12786,\\n2024d.\\nShilong Li, Xingyuan Bu, Wenjie Wang, Jiaheng Liu, Jun Dong, Haoyang He, Hao Lu, Haozhe Zhang, Chenchen Jing,\\nZhen Li, et al. MM-BrowseComp: A comprehensive benchmark for multimodal browsing agents. arXiv preprint\\narXiv:2508.13186, 2025e.\\nXiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, and Xiangmin Xu. HedgeAgents: A balanced-aware multi-agent financial\\ntrading system. In Companion Proceedings of the ACM on Web Conference 2025, pages 296â€“305, 2025f.\\nXiaonan Li and Xipeng Qiu. MoT: Memory-of-thought enables chatgpt to self-improve. In Proceedings of the 2023\\nConference on Empirical Methods in Natural Language Processing, pages 6354â€“6374, 2023.\\nXiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou.\\nWebThinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776,\\n2025g.\\nXin Sky Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Chen Qian, Chuan Shi, and Cheng\\nYang. GraphTeam: Facilitating large language model-based graph analysis via multi-agent collaboration. arXiv\\npreprint arXiv:2410.18032, 2024e.\\nXinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. A survey on llm-based multi-agent systems: workflow, infrastructure,\\nand challenges. Vicinagearth, 1(1):9, 2024f.\\n41\\nYang Li, Yangyang Yu, Haohang Li, Zhi Chen, and Khaldoun Khashanah. TradingGPT: Multi-agent system with\\nlayered memory and distinct characters for enhanced financial trading performance. arXiv preprint arXiv:2309.03736,\\n2023c.\\nYaoru Li, Shunyu Liu, Tongya Zheng, and Mingli Song. Parallelized planning-acting for efficient LLM-based multi-agent\\nsystems. arXiv preprint arXiv:2503.03505, 2025h.\\nYunxuan Li, Yibing Du, Jiageng Zhang, Le Hou, Peter Grabowski, Yeqing Li, and Eugene Ie. Improving multi-agent\\ndebate with sparse communication topology. In Findings of the Association for Computational Linguistics: EMNLP,\\npages 7281â€“7294. Association for Computational Linguistics, 2024g.\\nZelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, and Yongfeng Zhang.\\nAutoFlow: Automated workflow generation for large language model agents. arXiv preprint arXiv:2407.12821, 2024h.\\nZhuoqun Li, Xuanang Chen, Haiyang Yu, Hongyu Lin, Yaojie Lu, Qiaoyu Tang, Fei Huang, Xianpei Han, Le Sun,\\nand Yongbin Li. StructRAG: Boosting knowledge intensive reasoning of llms via inference-time hybrid information\\nstructurization. In The Thirteenth International Conference on Learning Representations, 2025i.\\nZihao Li, Weiwei Yi, and Jiahong Chen. Accuracy paradox in large language models: Regulating hallucination risks in\\ngenerative ai. arXiv preprint, 2025j.\\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng\\nTu. Encouraging divergent thinking in large language models through multi-agen debate. In Proceedings of the 2024\\nConference on Empirical Methods in Natural Language Processing, pages 17889â€“17904, 2024.\\nJunwei Liao, Muning Wen, Jun Wang, and Weinan Zhang. MARFT: Multi-agent reinforcement fine-tuning. arXiv\\npreprint arXiv:2504.16129, 2025.\\nXiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-Kiong Ng, Patrick Jaillet, and Bryan Kian Hsiang Low. Prompt\\noptimization with human feedback. arXiv preprint arXiv:2405.17346, 2024a.\\nXiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, and Bryan\\nKian Hsiang Low.\\nUse your INSTINCT: instruction optimization for llms using neural bandits coupled with\\ntransformers. In Forty-first International Conference on Machine Learning, 2024b.\\nYi-Cheng Lin, Kang-Chieh Chen, Zhe-Yan Li, Tzu-Heng Wu, Tzu-Hsuan Wu, Kuan-Yu Chen, Hung-yi Lee, and\\nYun-Nung Chen. Creativity in llm-based multi-agent systems: A survey. arXiv preprint arXiv:2505.21116, 2025.\\nBang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song,\\nKunlun Zhu, et al. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary,\\ncollaborative, and safe systems. arXiv preprint arXiv:2504.01990, 2025a.\\nBo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min\\nLin, et al. SPIRAL: Self-play on zero-sum games incentivizes reasoning via multi-agent multi-turn reinforcement\\nlearning. arXiv preprint arXiv:2506.24119, 2025b.\\nChris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou.\\nSkywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024a.\\nChris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang,\\nJiacheng Xu, et al. Skywork-reward-v2: Scaling preference data curation via human-ai synergy. arXiv preprint\\narXiv:2507.01352, 2025c.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in\\nthe middle: How language models use long contexts. Transactions of the Association for Computational Linguistics,\\n12:157â€“173, 2024b.\\nSiwei Liu, Jinyuan Fang, Han Zhou, Yingxu Wang, and Zaiqiao Meng. SEW: Self-evolving agentic workflows for\\nautomated code generation. arXiv preprint arXiv:2505.18646, 2025d.\\nWei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, and Junxian He. Diving into self-evolving training for\\nmultimodal reasoning. In Forty-second International Conference on Machine Learning, 2025e.\\nWei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, and Junxian He. Learn\\nto reason efficiently with adaptive length-based reward shaping. arXiv preprint arXiv:2505.15612, 2025f.\\nWeiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu,\\nYuanqing Yu, Zezhong Wang, Yuxian Wang, Wu Ning, Yutai Hou, Bin Wang, Chuhan Wu, Xinzhi Wang, Yong\\nLiu, Yasheng Wang, Duyu Tang, Dandan Tu, Lifeng Shang, Xin Jiang, Ruiming Tang, Defu Lian, Qun Liu, and\\n42\\nEnhong Chen. ToolACE: Winning the points of LLM function calling. In The Thirteenth International Conference\\non Learning Representations, 2025g.\\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan\\nYang, et al. AgentBench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023a.\\nYanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yuwei Zhang, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin,\\nand Tianyu Du. Tool-Planner: Task planning with clusters across multiple tools. In The Thirteenth International\\nConference on Learning Representations, 2025h.\\nYixin Liu, Guibin Zhang, Kun Wang, Shiyuan Li, and Shirui Pan. Graph-augmented large language model agents:\\nCurrent progress and future prospects. arXiv preprint arXiv:2507.21407, 2025i.\\nZijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic LLM-Agent network: An LLM-agent collaboration\\nframework with agent team optimization. arXiv preprint arXiv:2310.02170, 2023b.\\nGoogle LLC and A2A Project Contributors. Agent2Agent (A2A) Protocol. https://github.com/a2aproject/A2A.\\nApache License 2.0, accessed 2025-07-31.\\nLin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, and Wei Li.\\nSeeing, listening,\\nremembering, and reasoning: A multimodal agent with long-term memory. arXiv preprint arXiv:2508.09736, 2025.\\nManikanta Loya, Divya Sinha, and Richard Futrell. Exploring the sensitivity of llmsâ€™ decision-making capabilities:\\nInsights from prompt variations and hyperparameters. In Findings of the Association for Computational Linguistics:\\nEMNLP, pages 3711â€“3716, 2023.\\nChris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI scientist: Towards fully\\nautomated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024a.\\nJunru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu. MemoChat: Tuning\\nLLMs to use memos for consistent long-range open-domain conversation. arXiv preprint arXiv:2308.08239, 2023.\\nSiyuan Lu, Jiaqi Shao, Bing Luo, and Tao Lin. MorphAgent: Empowering agents through self-evolving profiles and\\ndecentralized collaboration. arXiv preprint arXiv:2410.15048, 2024b.\\nYao Lu, Jiayi Wang, Raphael Tang, Sebastian Riedel, and Pontus Stenetorp. Strings from the library of babel: Random\\nsampling as a strong baseline for prompt optimisation. In Proceedings of the 2024 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies, pages 2221â€“2231, 2024c.\\nJunyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao,\\nQingqing Long, et al. Large language model agent: A survey on methodology, applications and challenges. arXiv\\npreprint arXiv:2503.21460, 2025a.\\nYichen Luo, Yebo Feng, Jiahua Xu, Paolo Tasca, and Yang Liu. LLM-powered multi-agent system for automated crypto\\nportfolio management. arXiv preprint arXiv:2501.00826, 2025b.\\nAndres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. Augmenting\\nlarge language models with chemistry tools. Nature Machine Intelligence, 6(5):525â€“535, 2024.\\nXiaowen Ma, Chenyang Lin, Yao Zhang, Volker Tresp, and Yunpu Ma. Agentic neural networks: Self-evolving multi-agent\\nsystems via textual backpropagation. arXiv preprint arXiv:2506.09046, 2025.\\nYubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao, and Aixin\\nSun. SciAgent: Tool-augmented language models for scientific reasoning. In Proceedings of the 2024 Conference on\\nEmpirical Methods in Natural Language Processing 2024, pages 15701â€“15736, 2024.\\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\\nShrimai Prabhumoye, Yiming Yang, et al. Self-Refine: Iterative refinement with self-feedback. Advances in Neural\\nInformation Processing Systems, pages 46534â€“46594, 2023.\\nNour Makke and Sanjay Chawla. Interpretable scientific discovery with symbolic regression: a review. Artificial\\nIntelligence Review, 57(1):2, 2024.\\nRaphael Mannadiar and Hans Vangheluwe. Debugging in domain-specific modelling. In International Conference on\\nSoftware Language Engineering, pages 276â€“285. Springer, 2010.\\nSamuele Marro and Agora Protocol Contributors. Agora Protocol (AGORA). https://agoraprotocol.org/. MIT License,\\naccessed 2025-07-31.\\n43\\nAndrew D McNaughton, Gautham Krishna Sankar Ramalaxmi, Agustin Kruel, Carter R Knutson, Rohith A Varikoti,\\nand Neeraj Kumar. Cactus: Chemistry agent connecting tool usage to science. ACS omega, 9(46):46563â€“46573, 2024.\\nGrÃ©goire Mialon, ClÃ©mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: a benchmark for\\ngeneral ai assistants. In The Twelfth International Conference on Learning Representations, 2023.\\nYingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng,\\nHuatong Song, et al. Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems.\\narXiv preprint arXiv:2412.09413, 2024.\\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast model editing at scale.\\nIn The Tenth International Conference on Learning Representations, 2022.\\nAli Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich SchÃ¼tze. RET-LLM: Towards a general read-write memory\\nfor large language models. arXiv preprint arXiv:2305.14322, 2023.\\nSumeet Ramesh Motwani, Chandler Smith, Rocktim Jyoti Das, Rafael Rafailov, Ivan Laptev, Philip HS Torr, Fabio\\nPizzati, Ronald Clark, and Christian Schroeder de Witt. MALT: Improving reasoning with multi-agent llm training.\\narXiv preprint arXiv:2412.01928, 2024.\\nJaap MJ Murre and Joeri Dros. Replication and analysis of ebbinghausâ€™ forgetting curve. PloS one, 10(7):e0120644,\\n2015.\\nMagnus MÃ¼ller and Gregor Å½uniÄ.\\nBrowser use: Enable AI to control your browser, 2024.\\nhttps://github.com/\\nbrowser-use/browser-use.\\nDeepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja,\\nDespoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, et al. MLGym: A new framework and benchmark for\\nadvancing ai research agents. arXiv preprint arXiv:2502.14499, 2025.\\nAnsong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. LEVER:\\nLearning to verify language-to-code generation with execution. In International Conference on Machine Learning,\\npages 26106â€“26128. PMLR, 2023.\\nAnsong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin. NExT:\\nTeaching large language models to reason about code execution. In Forty-first International Conference on Machine\\nLearning, 2024.\\nBoye Niu, Yiliao Song, Kai Lian, Yifan Shen, Yu Yao, Kun Zhang, and Tongliang Liu. Flow: Modularized agentic\\nworkflow automation. In The Thirteenth International Conference on Learning Representations, 2025.\\nAlexander Novikov, NgÃ¢n VËœu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey\\nShirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. AlphaEvolve: A coding agent for\\nscientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025.\\nKrista Opsahl-Ong, Michael J. Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, and Omar Khattab.\\nOptimizing instructions and demonstrations for multi-stage language model programs. In Proceedings of the 2024\\nConference on Empirical Methods in Natural Language Processing, pages 9340â€“9366, 2024.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.\\nAdvances in neural information processing systems, 35:27730â€“27744, 2022.\\nCharles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E Gonzalez. MemGPT: Towards\\nllms as operating systems. arXiv preprint arXiv:2310.08560, 2023.\\nAlexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Hanlin Zhang, Scott\\nEmmons, and Dan Hendrycks. Do the rewards justify the means? measuring trade-offs between rewards and ethical\\nbehavior in the machiavelli benchmark. In International conference on machine learning, pages 26837â€“26867. PMLR,\\n2023.\\nMelissa Z Pan, Mert Cemri, Lakshya A Agrawal, Shuyi Yang, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya\\nParameswaran, Kannan Ramchandran, Dan Klein, Joseph E. Gonzalez, Matei Zaharia, and Ion Stoica. Why do\\nMulti-Agent systems fail? In ICLR 2025 Workshop on Building Trust in Language Models and Applications, 2025a.\\nRui Pan, Shuo Xing, Shizhe Diao, Wenhe Sun, Xiang Liu, Kashun Shum, Jipeng Zhang, Renjie Pi, and Tong Zhang.\\nPlum: Prompt learning using metaheuristics. In Findings of the Association for Computational Linguistics, pages\\n2177â€“2197, 2024a.\\n44\\nRuwei Pan, Hongyu Zhang, and Chao Liu. CodeCoR: An llm-based self-reflective multi-agent framework for code\\ngeneration. arXiv preprint arXiv:2501.07811, 2025b.\\nYichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou,\\nTongshuang Wu, et al. WebCanvas: Benchmarking web agents in online environments. arXiv preprint arXiv:2406.12373,\\n2024b.\\nChanwoo Park, Seungju Han, Xingzhi Guo, Asuman E. Ozdaglar, Kaiqing Zhang, and Joo-Kyung Kim. MAPoRL:\\nMulti-agent post-co-training for collaborative large language models with reinforcement learning. In Proceedings of the\\n63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 30215â€“30248,\\n2025.\\nJoon Sung Park, Joseph Oâ€™Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein.\\nGenerative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on\\nuser interface software and technology, pages 1â€“22, 2023.\\nJunyeong Park, Junmo Cho, and Sungjin Ahn. MrSteve: Instruction-following agents in minecraft with what-where-when\\nmemory. arXiv preprint arXiv:2411.06736, 2024.\\nShishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with\\nmassive APIs. Advances in Neural Information Processing Systems, 37:126544â€“126565, 2024.\\nAnthropic PBC and Model Context Protocol Contributors.\\nModel Context Protocol (MCP).\\nhttps://\\nmodelcontextprotocol.io/overview. MIT License, accessed 2025-07-31.\\nJonas Pfeiffer, Aishwarya Kamath, Andreas RÃ¼cklÃ©, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non-\\ndestructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of\\nthe Association for Computational Linguistics, pages 487â€“503, 2021.\\nAkshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen,\\nThai Hoang, Juan Carlos Niebles, et al. APIGen-MT: Agentic pipeline for multi-turn data generation via simulated\\nagent-human interplay. arXiv preprint arXiv:2504.03601, 2025.\\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. GRIPS: Gradient-free, edit-based instruction search for\\nprompting large language models. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th\\nConference of the European Chapter of the Association for Computational Linguistics, pages 3827â€“3846, 2023.\\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization\\nwith \"gradient descent\" and beam search. In Proceedings of the 2023 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 7957â€“7968, 2023.\\nYingming Pu, Tao Lin, and Hongyu Chen. PiFlow: Principle-aware scientific discovery with multi-agent collaboration.\\narXiv preprint arXiv:2505.15047, 2025.\\nPranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent\\nQ: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024.\\nMeghana Puvvadi, Sai Kumar Arava, Adarsh Santoria, Sesha Sai Prasanna Chennupati, and Harsha Vardhan Puvvadi.\\nCoding agents: A comprehensive survey of automated bug fixing systems and benchmarks. In 2025 IEEE 14th\\nInternational Conference on Communication Systems and Network Technologies (CSNT), pages 680â€“686. IEEE, 2025.\\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su,\\nXin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. ChatDev: Communicative agents for software\\ndevelopment. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), pages 15174â€“15186, 2024.\\nCheng Qian, Chi Han, Yi R Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. CREATOR: Tool creation for disentangling\\nabstract and concrete reasoning of large language models. In 2023 Findings of the Association for Computational\\nLinguistics: EMNLP 2023, pages 6922â€“6939. Association for Computational Linguistics (ACL), 2023.\\nCheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-TÃ¼r, Gokhan Tur, and Heng Ji.\\nToolRL: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025a.\\nYiyue Qian, Shinan Zhang, Yun Zhou, Haibo Ding, Diego Socolinsky, and Yi Zhang. Enhancing LLM-as-a-Judge via\\nmulti-agent collaboration. amazon.science, 2025b.\\n45\\nShuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang,\\nand Huajun Chen. Agent planning with world knowledge model. In Advances in Neural Information Processing\\nSystems, 2024.\\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian,\\nSihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong\\nSun. ToolLLM: Facilitating large language models to master 16000+ real-world apis. In The Twelfth International\\nConference on Learning Representations, 2024.\\nJiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren,\\nXun Jiang, et al. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal\\nself-evolution. arXiv preprint arXiv:2505.20286, 2025.\\nChangle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. From\\nexploration to mastery: Enabling LLMs to master tools via self-driven interactions. In The Thirteenth International\\nConference on Learning Representations, 2025.\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct\\npreference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural\\nInformation Processing Systems, 2023.\\nAsif Rahman, Veljko Cvetkovic, Kathleen Reece, Aidan Walters, Yasir Hassan, Aneesh Tummeti, Bryan Torres, Denise\\nCooney, Margaret Ellis, and Dimitrios S Nikolopoulos. MARCO: Multi-agent code optimization with real-time\\nknowledge integration for high-performance computing. arXiv preprint arXiv:2505.03906, 2025.\\nZeeshan Rasheed, Malik Abdul Sami, Kai-Kristian Kemell, Muhammad Waseem, Mika Saari, Kari SystÃ¤, and Pekka\\nAbrahamsson. CodePori: Large-scale system for autonomous software development using multi-agent technology.\\narXiv preprint arXiv:2402.01411, 2024.\\nChristopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li,\\nWilliam Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. AndroidWorld: A dynamic benchmarking environment for\\nautonomous agents. arXiv preprint arXiv:2405.14573, 2024.\\nShaina Raza, Ranjan Sapkota, Manoj Karkee, and Christos Emmanouilidis. TRiSM for agentic ai: A review of trust,\\nrisk, and security management in llm-based agentic multi-agent systems. arXiv preprint arXiv:2506.04133, 2025.\\nAymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik KaunismÃ¤ki. â€œsmolagentsâ€: a\\nsmol library to build great agentic systems. https://github.com/huggingface/smolagents, 2025.\\nKai Ruan, Xuan Wang, Jixiang Hong, Peng Wang, Yang Liu, and Hao Sun. Liveideabench: Evaluating llmsâ€™ divergent\\nthinking for scientific idea generation with minimal context. arXiv preprint arXiv:2412.17596, 2024.\\nKai Ruan, Mowen Huang, Ji-Rong Wen, and Hao Sun. Benchmarking LLMsâ€™ swarm intelligence. arXiv preprint\\narXiv:2505.04364, 2025.\\nAli Safaya and Deniz Yuret. Neurocache: Efficient vector retrieval for long-range language modeling. In Proceedings\\nof the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies (Volume 1: Long Papers), pages 870â€“883, 2024.\\nSaket Sarin, Sunil K Singh, Sudhakar Kumar, Shivam Goyal, Brij Bhooshan Gupta, Wadee Alhalabi, and Varsha Arya.\\nUnleashing the power of multi-agent reinforcement learning for algorithmic trading in the digital financial frontier and\\nenterprise information systems. Computers, Materials & Continua, 80(2), 2024.\\nAnjana Sarkar and Soumyendu Sarkar. Survey of LLM agent communication with mcp: A software design pattern\\ncentric review. arXiv preprint arXiv:2506.05364, 2025.\\nTimo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola\\nCancedda, and Thomas Scialom. ToolFormer: Language models can teach themselves to use tools. Advances in\\nNeural Information Processing Systems, 36:68539â€“68551, 2023.\\nSamuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor. AgentClinic: a multimodal\\nagent benchmark to evaluate ai in simulated clinical environments. arXiv preprint arXiv:2405.07960, 2024.\\nLennart Schneider, Martin Wistuba, Aaron Klein, Jacek Golebiowski, Giovanni Zappella, and Felice Antonio Merra.\\nHyperband-based bayesian optimization for black-box prompt selection. In Forty-second International Conference on\\nMachine Learning, 2025.\\n46\\nAmrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan\\nBerant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for LLM reasoning. In The\\nThirteenth International Conference on Learning Representations, 2025.\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li,\\nYang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint\\narXiv:2402.03300, 2024.\\nChengshuai Shi, Kun Yang, Jing Yang, and Cong Shen. Best arm identification for prompt learning under a limited\\nbudget. arXiv preprint arXiv:2402.09723, 2024a.\\nJuanming Shi, Qinglang Guo, Yong Liao, and Shenglin Liang. Legalgpt: Legal chain of thought for the legal large\\nlanguage model multi-agent framework. In International Conference on Intelligent Computing, pages 25â€“37. Springer,\\n2024b.\\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents\\nwith verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:8634â€“8652, 2023.\\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre CÃ´tÃ©, Yonatan Bisk, Adam Trischler, and Matthew J. Hausknecht.\\nAlfworld: Aligning text and embodied environments for interactive learning. In 9th International Conference on\\nLearning Representations, 2021.\\nArnav Singhvi, Manish Shetty, Shangyin Tan, Christopher Potts, Koushik Sen, Matei Zaharia, and Omar Khattab. Dspy\\nassertions: Computational constraints for self-refining language model pipelines. arXiv preprint arXiv:2312.13382,\\n2023.\\nLinxin Song, Jiale Liu, Jieyu Zhang, Shaokun Zhang, Ao Luo, Shijian Wang, Qingyun Wu, and Chi Wang. Adaptive\\nin-conversation team building for language model agents. arXiv preprint arXiv:2405.19425, 2024.\\nDilara Soylu, Christopher Potts, and Omar Khattab. Fine-Tuning and Prompt Optimization: Two great steps that\\nwork better together. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,\\npages 10696â€“10710, 2024.\\nOlly Styles, Sam Miller, Patricio Cerda-Mardini, Tanaya Guha, Victor Sanchez, and Bertie Vidgen. Workbench: a\\nbenchmark dataset for agents in a realistic workplace setting. arXiv preprint arXiv:2405.00823, 2024.\\nJinwei Su, Yinghui Xia, Ronghua Shi, Jianhui Wang, Jianuo Huang, Yijin Wang, TIANYU SHI, Yang Jingsong, and\\nLewei He. DebFlow: Automating agent creation via agent debate. In ICML 2025 Workshop on Collaborative and\\nFederated Agentic Workflows, 2025.\\nVighnesh Subramaniam, Yilun Du, Joshua B Tenenbaum, Antonio Torralba, Shuang Li, and Igor Mordatch. Multiagent\\nfinetuning: Self improvement with diverse reasoning chains. arXiv preprint arXiv:2501.05707, 2025.\\nEmilio Sulis, Stefano Mariani, and Sara Montagna. A survey on agents applications in healthcare: Opportunities,\\nchallenges and trends. Computer Methods and Programs in Biomedicine, 236:107525, 2023.\\nHao Sun, Alihan HÃ¼yÃ¼k, and Mihaela van der Schaar. Query-Dependent prompt evaluation and optimization with\\noffline inverse RL. In The Twelfth International Conference on Learning Representations, 2024a.\\nJingyun Sun, Chengxiao Dai, Zhongze Luo, Yangbo Chang, and Yang Li. LawLuo: A Chinese law firm co-run by LLM\\nagents. arXiv preprint arXiv:2407.16252, 2024b.\\nZeyi Sun, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Tong Wu, Dahua Lin, and Jiaqi Wang. SEAgent:\\nSelf-evolving computer use agent with autonomous learning from experience. arXiv preprint arXiv:2508.04700, 2025.\\nYashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of intelligent llm agents.\\narXiv preprint arXiv:2306.03314, 2023.\\nXiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu, Zhuosheng\\nZhang, Yilun Zhao, et al. ChemAgent: Self-updating memories in large language models improves chemical reasoning.\\nIn The Thirteenth International Conference on Learning Representations, 2025a.\\nXiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang\\nWu, He Zhu, et al. Agent kb: Leveraging cross-domain experience for agentic problem solving. arXiv preprint\\narXiv:2507.06229, 2025b.\\nXinyu Tang, Xiaolei Wang, Wayne Xin Zhao, Siyuan Lu, Yaliang Li, and Ji-Rong Wen. Unleashing the potential of large\\nlanguage models as prompt optimizers: Analogical analysis with gradient-based model optimizers. In Proceedings of\\nthe AAAI Conference on Artificial Intelligence, pages 25264â€“25272, 2025c.\\n47\\nXunzhu Tang, Kisub Kim, Yewei Song, Cedric Lothritz, Bei Li, Saad Ezzini, Haoye Tian, Jacques Klein, and TegawendÃ© F\\nBissyandÃ©. CodeAgent: Autonomous communicative agents for code review. arXiv preprint arXiv:2402.02172, 2024.\\nYong-En Tian, Yu-Chien Tang, Kuang-Da Wang, An-Zi Yen, and Wen-Chih Peng. Template-based financial report\\ngeneration in agentic and decomposed information retrieval. In Proceedings of the 48th International ACM SIGIR\\nConference on Research and Development in Information Retrieval, pages 2706â€“2710. ACM, 2025.\\nYuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. DART-math: Difficulty-aware rejection tuning\\nfor mathematical problem-solving. In The Thirty-eighth Annual Conference on Neural Information Processing Systems,\\n2024.\\nKhanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry Oâ€™Sullivan, and Hoang D Nguyen.\\nMulti-Agent collaboration mechanisms: A survey of LLMs. arXiv preprint arXiv:2501.06322, 2025.\\nHarsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish\\nSabharwal, and Niranjan Balasubramanian. Appworld: A controllable world of apps and people for benchmarking\\ninteractive coding agents. arXiv preprint arXiv:2407.18901, 2024.\\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. Language models donâ€™t always say what they\\nthink: Unfaithful explanations in chain-of-thought prompting. In Thirty-seventh Conference on Neural Information\\nProcessing Systems, 2023.\\nTycho FA van der Ouderaa, Markus Nagel, Mart Van Baalen, Yuki M Asano, and Tijmen Blankevoort. The LLM\\nsurgeon. arXiv preprint arXiv:2312.17244, 2023.\\nPat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie\\nXu, Naomi White, and Patrick Lewis. Replacing judges with juries: Evaluating llm generations with a panel of diverse\\nmodels. arXiv preprint arXiv:2404.18796, 2024.\\nXingchen Wan, Han Zhou, Ruoxi Sun, and Sercan Ã–. Arik. From few to many: Self-improving many-shot reasoners\\nthrough iterative optimization and generation. In The Thirteenth International Conference on Learning Representations,\\n2025.\\nBoshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, and Yu Su. LLMs in the imaginarium: Tool learning\\nthrough simulated trial and error. In Proceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 10583â€“10604, 2024a.\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.\\nVoyager: An open-ended embodied agent with large language models. Transactions on Machine Learning Research,\\n2023a.\\nJize Wang, Ma Zerun, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le. GTA: a benchmark for general\\ntool agents. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks\\nTrack, 2024b.\\nJunlin Wang, Jue WANG, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents enhances large language\\nmodel capabilities. In The Thirteenth International Conference on Learning Representations, 2025a.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen,\\nYankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):\\n186345, 2024c.\\nNingning Wang, Xavier Hu, Pai Liu, He Zhu, Yue Hou, Heyuan Huang, Shengyu Zhang, Jian Yang, Jiaheng Liu,\\nGe Zhang, Changwang Zhang, Jun Wang, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Efficient agents: Building\\neffective agents while reducing cost, 2025b.\\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd:\\nVerify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 9426â€“9439, 2024d.\\nQian Wang, Tianyu Wang, Zhenheng Tang, Qinbin Li, Nuo Chen, Jingsheng Liang, and Bingsheng He. All it takes is\\none prompt: An autonomous LLM-MA system. In ICLR 2025 Workshop on Foundation Models in the Wild, 2025c.\\nQingyue Wang, Yanhe Fu, Yanan Cao, Shuai Wang, Zhiliang Tian, and Liang Ding. Recursively summarizing enables\\nlong-term dialogue memory in large language models. Neurocomputing, 639:130193, 2025d.\\nRenxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, and Haonan Li. ToolGen: Unified tool retrieval and\\ncalling via generation. In The Thirteenth International Conference on Learning Representations, 2025e.\\n48\\nShang Wang, Tianqing Zhu, Dayong Ye, and Wanlei Zhou. When machine unlearning meets retrieval-augmented\\ngeneration (RAG): Keep secret or forget knowledge? arXiv preprint arXiv:2410.15267, 2024e.\\nShilong Wang, Guibin Zhang, Miao Yu, Guancheng Wan, Fanci Meng, Chongye Guo, Kun Wang, and Yang Wang.\\nG-Safeguard: A topology-guided security lens and treatment on llm-based multi-agent systems. In Proceedings of the\\n63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7261â€“7276,\\n2025f.\\nSiyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang Ren. Symbolic working memory enhances language models for\\ncomplex rule application. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,\\npages 17583â€“17604, 2024f.\\nWenyi Wang, Hisham A Alyahya, Dylan R Ashley, Oleg Serikov, Dmitrii Khizbullin, Francesco Faccio, and JÃ¼rgen\\nSchmidhuber. How to correctly do semantic backpropagation on language-based agentic systems. arXiv preprint\\narXiv:2412.03624, 2024g.\\nXingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions\\nelicit better llm agents. In Forty-first International Conference on Machine Learning, 2024h.\\nXingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li,\\nJaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff,\\nYizhe Zhang, Binyuan Hui, Junyang Lin, and et al. OpenHands: An open platform for AI software developers as\\ngeneralist agents. In The Thirteenth International Conference on Learning Representations, 2025g.\\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, and Zhiting\\nHu. PromptAgent: Strategic planning with language models enables expert-level prompt optimization. In The Twelfth\\nInternational Conference on Learning Representations, 2024i.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and\\nDenny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International\\nConference on Learning Representations, 2023b.\\nYingxu Wang, Shiqi Fan, Mengzhu Wang, and Siwei Liu. Dynamically adaptive reasoning via LLM-guided mcts for\\nefficient and context-aware KGQA. arXiv preprint arXiv:2508.00719, 2025h.\\nYingxu Wang, Siwei Liu, Jinyuan Fang, and Zaiqiao Meng. EvoAgentX: An automated framework for evolving agentic\\nworkflows. arXiv preprint arXiv:2507.03616, 2025i.\\nYinjie Wang, Ling Yang, Guohao Li, Mengdi Wang, and Bryon Aragam. ScoreFlow: Mastering llm agent workflows via\\nscore-based preference optimization. arXiv preprint arXiv:2502.04306, 2025j.\\nYiying Wang, Xiaojing Li, Binzhu Wang, Yueyang Zhou, Yingru Lin, Han Ji, Hong Chen, Jinshi Zhang, Fei Yu, Zewei\\nZhao, et al. PEER: Expertizing domain-specific tasks with a multi-agent framework and tuning methods. arXiv\\npreprint arXiv:2407.06985, 2024j.\\nYu Wang and Xi Chen. MIRIX: Multi-agent memory system for llm-based agents. arXiv preprint arXiv:2507.07957,\\n2025.\\nZhiruo Wang, Graham Neubig, and Daniel Fried.\\nTroVE: Inducing verifiable and efficient toolboxes for solving\\nprogrammatic tasks. In Forty-first International Conference on Machine Learning, 2024k.\\nZihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat\\nNguyen, Licheng Liu, et al. RAGEN: Understanding self-evolution in llm agents via multi-turn reinforcement learning.\\narXiv preprint arXiv:2504.20073, 2025k.\\nZiyue Wang, Junde Wu, Chang Han Low, and Yueming Jin. MedAgent-Pro: Towards multi-modal evidence-based\\nmedical diagnosis via reasoning agentic workflow. arXiv preprint arXiv:2503.18968, 2025l.\\nZora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig.\\nAgent workflow memory.\\nIn Forty-second\\nInternational Conference on Machine Learning, 2024l.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny\\nZhou. Chain-of-Thought prompting elicits reasoning in large language models. In Advances in Neural Information\\nProcessing Systems, 2022.\\nJason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard\\nPassos, William Fedus, and Amelia Glaese. BrowseComp: A simple yet challenging benchmark for browsing agents.\\narXiv preprint arXiv:2504.12516, 2025a.\\n49\\nYangbo Wei, Zhen Huang, Huang Li, Wei W Xing, Ting-Jung Lin, and Lei He. Vflow: Discovering optimal agentic\\nworkflows for verilog generation. arXiv preprint arXiv:2504.03723, 2025b.\\nBin Wu, Edgar Meij, and Emine Yilmaz. A joint optimization framework for enhancing efficiency of tool utilization in\\nLLM agents. In Findings of the Association for Computational Linguistics, ACL, pages 22361â€“22373, 2025a.\\nJialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou,\\nPengjun Xie, and Fei Huang. WebWalker: Benchmarking llms in web traversal. In Proceedings of the 63rd Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10290â€“10305, 2025b.\\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang,\\nJiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversations. In First Conference on\\nLanguage Modeling, 2024a.\\nShirley Wu, Parth Sarthi, Shiyu Zhao, Aaron Lee, Herumb Shandilya, Adrian Mladenic Grobelnik, Nurendra Choudhary,\\nEddie Huang, Karthik Subbian, Linjun Zhang, et al. Optimas: Optimizing compound ai systems with globally aligned\\nlocal rewards. arXiv preprint arXiv:2507.03041, 2025c.\\nYurong Wu, Yan Gao, Bin Zhu, Zineng Zhou, Xiaodi Sun, Sheng Yang, Jian-Guang Lou, Zhiming Ding, and Linjun Yang.\\nStraGo: Harnessing strategic guidance for prompt optimization. In Findings of the Association for Computational\\nLinguistics: EMNLP, pages 10043â€“10061, 2024b.\\nZhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin AkyÃ¼rek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and\\nYoon Kim. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual\\ntasks. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies (Volume 1: Long Papers), pages 1819â€“1862, 2024c.\\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu\\nZhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou,\\nXiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing\\nHuang, Qi Zhang, and Tao Gui. The rise and potential of large language model based agents: a survey. Science China\\nInformation Sciences, 68(2), 2025.\\nJinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng, Jinhao Tu, Xinbing Liang, Sirui Hong, Chenglin Wu, and Yuyu\\nLuo. Self-supervised prompt optimization. arXiv preprint arXiv:2502.06855, 2025.\\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun Cheng,\\nDongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer\\nenvironments. Advances in Neural Information Processing Systems, 37:52040â€“52094, 2024.\\nHuajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan\\nLiang. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. arXiv preprint\\narXiv:2405.14333, 2024.\\nHuajian Xin, Z.Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu,\\nQiushi Du, Wenjun Gao, Haowei Zhang, Qihao Zhu, Dejian Yang, Zhibin Gou, Z.F. Wu, Fuli Luo, and Chong Ruan.\\nDeepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. In\\nThe Thirteenth International Conference on Learning Representations, 2025.\\nFrank Xing. Designing heterogeneous LLM agents for financial sentiment analysis. ACM Transactions on Management\\nInformation Systems, 16(1):1â€“24, 2025.\\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. GPS: genetic prompt search\\nfor efficient few-shot learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\\nProcessing, pages 8162â€“8171, 2022.\\nQiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation capability\\nof open-source large language models. arXiv preprint arXiv:2305.16504, 2023.\\nTianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie, Yongchao Chen, Shilong\\nLiu, Bochen Qian, et al. Crab: Cross-environment agent benchmark for multimodal language model agents. arXiv\\npreprint arXiv:2407.01511, 2024a.\\nTianwen Xu and Fengkui Ju. Multi-agent logic for reasoning about duties and powers in private law. In Proceedings of\\nthe Nineteenth International Conference on Artificial Intelligence and Law, pages 361â€“370, 2023.\\nWeijia Xu, Andrzej Banburski, and Nebojsa Jojic. Reprompting: Automated chain-of-thought prompt inference through\\ngibbs sampling. In Forty-first International Conference on Machine Learning, 2024b.\\n50\\nWujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-MEM: Agentic memory for llm\\nagents. arXiv preprint arXiv:2502.12110, 2025.\\nSikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Hinrich SchÃ¼tze, Volker\\nTresp, and Yunpu Ma. Memory-R1: Enhancing large language model agents to manage and utilize memories via\\nreinforcement learning. arXiv preprint arXiv:2508.19828, 2025.\\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang,\\nChenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a.\\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language\\nmodels as optimizers. In The Twelfth International Conference on Learning Representations, 2024a.\\nHongyang Yang, Boyu Zhang, Neng Wang, Cheng Guo, Xiaoli Zhang, Likun Lin, Junlin Wang, Tianyu Zhou, Mao\\nGuan, Runjia Zhang, et al. FinRobot: an open-source ai agent platform for financial applications using large language\\nmodels. arXiv preprint arXiv:2405.14767, 2024b.\\nLing Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E. Gonzalez, and Bin CUI. Buffer\\nof thoughts: Thought-augmented reasoning with large language models. In The Thirty-eighth Annual Conference on\\nNeural Information Processing Systems, 2024c.\\nRui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. GPT4Tools: Teaching large language\\nmodel to use tools via self instruction. In Advances in Neural Information Processing Systems, 2023.\\nWeiqing Yang, Hanbin Wang, Zhenghao Liu, Xinze Li, Yukun Yan, Shuo Wang, Yu Gu, Minghe Yu, Zhiyuan Liu, and\\nGe Yu. Enhancing the code debugging ability of llms via communicative agent based data refinement. CoRR, 2024d.\\nYingxuan Yang, Huacan Chai, Shuai Shao, Yuanyi Song, Siyuan Qi, Renting Rui, and Weinan Zhang. AgentNet:\\nDecentralized evolutionary coordination for llm-based multi-agent systems. arXiv preprint arXiv:2504.00587, 2025b.\\nYingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin,\\nGaowei Chang, et al. A survey of AI agent protocols. arXiv preprint arXiv:2504.16736, 2025c.\\nYingxuan Yang, Qiuying Peng, Jun Wang, Ying Wen, and Weinan Zhang. Unlocking the potential of decentralized llm-\\nbased MAS: privacy preservation and monetization in collective intelligence. In Proceedings of the 24th International\\nConference on Autonomous Agents and Multiagent Systems, pages 2896â€“2900, 2025d.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts:\\nDeliberate problem solving with large language models. Advances in neural information processing systems, 36:\\n11809â€“11822, 2023a.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. ReAct: Synergizing\\nreasoning and acting in language models. In The Eleventh International Conference on Learning Representations,\\n2023b.\\nShunyu Yao, Noah Shinn, Pedram Razavi, and Karthik R Narasimhan. Ï„-bench: A benchmark for Tool-Agent-User\\ninteraction in real-world domains. In The Thirteenth International Conference on Learning Representations, 2025.\\nWeiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh R. N., Zeyuan Chen,\\nJianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. Retroformer:\\nRetrospective large language agents with policy gradient optimization. In The Twelfth International Conference on\\nLearning Representations, 2024.\\nQinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. Prompt engineering a prompt engineer. arXiv\\npreprint arXiv:2311.05661, 2023.\\nRui Ye, Shuo Tang, Rui Ge, Yaxin Du, Zhenfei Yin, Siheng Chen, and Jing Shao. MAS-GPT: Training LLMs to build\\nLLM-based multi-agent systems. arXiv preprint arXiv:2503.03686, 2025.\\nAsaf Yehudai, Lilach Eden, Alan Li, Guy Uziel, Yilun Zhao, Roy Bar-Haim, Arman Cohan, and Michal Shmueli-Scheuer.\\nSurvey on evaluation of LLM-based agents. arXiv preprint arXiv:2503.16416, 2025.\\nFan Yin, Zifeng Wang, I Hsu, Jun Yan, Ke Jiang, Yanfei Chen, Jindong Gu, Long T Le, Kai-Wei Chang, Chen-\\nYu Lee, et al. Magnet: Multi-turn tool-use data synthesis and distillation via graph translation. arXiv preprint\\narXiv:2503.07826, 2025.\\nShuo Yin, Weihao You, Zhilong Ji, Guoqiang Zhong, and Jinfeng Bai. MuMath-Code: Combining tool-use large language\\nmodels with multi-perspective data augmentation for mathematical reasoning. In Proceedings of the 2024 Conference\\non Empirical Methods in Natural Language Processing, pages 4770â€“4785, 2024.\\n51\\nZhangyue Yin,\\nQiushi Sun,\\nCheng Chang,\\nQipeng Guo,\\nJunqi Dai,\\nXuanjing Huang,\\nand Xipeng Qiu.\\nExchange-of-Thought: Enhancing large language model capabilities through cross-model communication. In Proceed-\\nings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15135â€“15153, 2023.\\nJunwei Yu, Yepeng Ding, and Hiroyuki Sato. DynTaskMAS: A dynamic task graph-driven framework for asynchronous\\nand parallel LLM-based multi-agent systems. arXiv preprint arXiv:2503.07675, 2025.\\nMiao Yu, Shilong Wang, Guibin Zhang, Junyuan Mao, Chenlong Yin, Qijiong Liu, Qingsong Wen, Kun Wang, and Yang\\nWang. NetSafe: Exploring the topological safety of multi-agent networks. arXiv preprint arXiv:2410.15686, 2024a.\\nYangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yuechen Jiang, Yupeng Cao, Zhi Chen, Jordan Suchow, Zhenyu\\nCui, Rong Liu, et al. Fincon: A synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced\\nfinancial decision making. Advances in Neural Information Processing Systems, 37:137010â€“137045, 2024b.\\nLifan Yuan, Yangyi Chen, Xingyao Wang, Yi Fung, Hao Peng, and Heng Ji. CRAFT: Customizing LLMs by creating\\nand retrieving from specialized toolsets. In The Twelfth International Conference on Learning Representations, 2024a.\\nSiyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, and Deqing Yang. EvoAgent: Towards automatic\\nmulti-agent generation via evolutionary algorithms. In Proceedings of the 2025 Conference of the Nations of the\\nAmericas Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 6192â€“6217,\\n2025a.\\nSiyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Kan Ren, Dongsheng Li, and Deqing Yang. EASYTOOL:\\nenhancing llm-based agents with concise tool instruction. In Proceedings of the 2025 Conference of the Nations of the\\nAmericas Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 951â€“972,\\n2025b.\\nTongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li,\\nZhuosheng Zhang, et al. R-judge: Benchmarking safety risk awareness for llm agents. arXiv preprint arXiv:2401.10019,\\n2024b.\\nWeikang Yuan, Junjie Cao, Zhuoren Jiang, Yangyang Kang, Jun Lin, Kaisong Song, Pengwei Yan, Changlong Sun,\\nXiaozhong Liu, et al. Can large language models grasp legal theories? enhance legal reasoning with insights from\\nmulti-agent collaboration. arXiv preprint arXiv:2410.02507, 2024c.\\nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.\\nSelf-rewarding language models. In Forty-first International Conference on Machine Learning, 2024d.\\nMert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad:\\nAutomatic â€œdifferentiationâ€ via text. arXiv preprint arXiv:2406.07496, 2024.\\nMert YÃ¼ksekgÃ¶nÃ¼l, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou.\\nOptimizing generative AI by backpropagating language model feedback. Nature, 639(8055):609â€“616, 2025.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping reasoning with reasoning. In Advances\\nin Neural Information Processing Systems, volume 35, pages 15476â€“15488, 2022.\\nRuihong Zeng, Jinyuan Fang, Siwei Liu, and Zaiqiao Meng. On the structural memory of llm agents. arXiv preprint\\narXiv:2412.15266, 2024a.\\nRuihong Zeng, Jinyuan Fang, Siwei Liu, and Zaiqiao Meng. On the structural memory of llm agents. arXiv preprint\\narXiv:2412.15266, 2024b.\\nAlexander Zhang, Marcus Dong, Jiaheng Liu, Wei Zhang, Yejie Wang, Jian Yang, Ge Zhang, Tianyu Liu, Zhongyuan\\nPeng, Yingshui Tan, et al. CodeCriticBench: A holistic code critique benchmark for large language models. arXiv\\npreprint arXiv:2502.16614, 2025a.\\nChi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. AppAgent:\\nMultimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing\\nSystems, pages 70:1â€“70:20. ACM, 2025b.\\nDan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, and\\nYisong Yue. DataSciBench: An llm agent benchmark for data science. arXiv preprint arXiv:2502.13897, 2025c.\\nEnhao Zhang, Erkang Zhu, Gagan Bansal, Adam Fourney, Hussein Mozannar, and Jack Gerrits. Optimizing sequential\\nmulti-step tasks with parallel llm agents. arXiv preprint arXiv:2507.08944, 2025d.\\n52\\nGuibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, and\\nDawei Cheng. G-designer: Architecting multi-agent communication topologies via graph neural networks. arXiv\\npreprint arXiv:2410.11782, 2024a.\\nGuibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. G-Memory: Tracing hierarchical\\nmemory for multi-agent systems. arXiv preprint arXiv:2506.07398, 2025e.\\nGuibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, LEI BAI, and Xiang Wang. Multi-agent architecture search via\\nagentic supernet. In Forty-second International Conference on Machine Learning, 2025f.\\nGuibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, and\\nTianlong Chen. Cut the crap: An economical communication pipeline for llm-based multi-agent systems. In The\\nThirteenth International Conference on Learning Representations, 2025g.\\nHangfan Zhang, Zhiyao Cui, Xinrun Wang, Qiaosheng Zhang, Zhen Wang, Dinghao Wu, and Shuyue Hu. If multi-agent\\ndebate is the answer, what is the question. arXiv preprint arXiv:2502.08788, 2025h.\\nJenny Zhang, Shengran Hu, Cong Lu, Robert Lange, and Jeff Clune. Darwin godel machine: Open-ended evolution of\\nself-improving agents. arXiv preprint arXiv:2505.22954, 2025i.\\nJiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xiong-Hui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui\\nHong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, and Chenglin Wu. AFlow: Automating agentic workflow\\ngeneration. In The Thirteenth International Conference on Learning Representations, 2025j.\\nJun Zhang, Yuwei Yan, Junbo Yan, Zhiheng Zheng, Jinghua Piao, Depeng Jin, and Yong Li. A parallelized framework\\nfor simulating large-scale LLM agents with realistic environments and interactions. In Georg Rehm and Yunyao\\nLi, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 6:\\nIndustry Track), pages 1339â€“1349, Vienna, Austria, July 2025k. Association for Computational Linguistics. ISBN\\n979-8-89176-288-6. doi: 10.18653/v1/2025.acl-industry.94.\\nKechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. Self-Edit: Fault-aware code editor for code generation. arXiv preprint\\narXiv:2305.04087, 2023a.\\nPeiyan Zhang, Haibo Jin, Leyang Hu, Xinnuo Li, Liying Kang, Man Luo, Yangqiu Song, and Haohan Wang. Revolve:\\nOptimizing AI systems by tracking response evolution in textual optimization. In Forty-second International Conference\\non Machine Learning, 2025l.\\nShaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. Offline training\\nof language model agents with functions as learnable weights. In Forty-first International Conference on Machine\\nLearning, 2024b.\\nShaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and\\nGuilin Liu.\\nNemotron-research-tool-n1: Tool-using language models with reinforced reasoning.\\narXiv preprint\\narXiv:2505.00024, 2025m.\\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E. Gonzalez. TEMPERA: test-time prompt\\nediting via reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023b.\\nWentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorchestra: A hierarchical\\nmulti-agent framework for general-purpose task solving. arXiv preprint arXiv:2506.12508, 2025n.\\nYusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Arik. Chain of agents: Large language\\nmodels collaborating on long-context tasks. Advances in Neural Information Processing Systems, 37:132208â€“132237,\\n2024c.\\nZeyu Zhang, Quanyu Dai, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A\\nsurvey on the memory mechanism of large language model based agents. ACM Transactions on Information Systems,\\n2024d.\\nAndrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. ExpeL: LLM agents are\\nexperiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 19632â€“19642, 2024.\\nAndrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng,\\nand Gao Huang. Absolute Zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335,\\n2025a.\\n53\\nRuochen Zhao, Wenxuan Zhang, Yew Ken Chia, Weiwen Xu, Deli Zhao, and Lidong Bing. Auto-Arena: Automating\\nLLM evaluations with agent peer battles and committee discussions. In Proceedings of the 63rd Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 4440â€“4463, 2025b.\\nWanjia Zhao, Mert Yuksekgonul, Shirley Wu, and James Zou. SiriuS: Self-improving multi-agent systems via bootstrapped\\nreasoning. arXiv preprint arXiv:2502.04780, 2025c.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\\nZhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023.\\nChengqi Zheng, Jianda Chen, Yueming Lyu, Wen Zheng Terence Ng, Haopeng Zhang, Yew-Soon Ong, Ivor Tsang, and\\nHaiyan Yin. Mermaidflow: Redefining agentic workflow generation via safety-constrained evolutionary programming.\\narXiv preprint arXiv:2505.22967, 2025.\\nLongtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory\\nfor computer control. In The Twelfth International Conference on Learning Representations, 2023a.\\nSipeng Zheng, Jiazheng Liu, Yicheng Feng, and Zongqing Lu. Steve-Eye: Equipping llm-based embodied agents with\\nvisual perception in open worlds. In The Twelfth International Conference on Learning Representations, 2024.\\nZhiling Zheng, Oufan Zhang, Ha L Nguyen, Nakul Rampal, Ali H Alawadhi, Zichao Rong, Teresa Head-Gordon, Christian\\nBorgs, Jennifer T Chayes, and Omar M Yaghi. Chatgpt research group for optimizing the crystallinity of mofs and\\ncofs. ACS Central Science, 9(11):2161â€“2170, 2023b.\\nWanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models\\nwith long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 19724â€“19731, 2024.\\nHan Zhou, Xingchen Wan, Ivan Vulic, and Anna Korhonen. Survival of the most influential prompts: Efficient black-box\\nprompt search via clustering and pruning. In Findings of the Association for Computational Linguistics: EMNLP,\\npages 13064â€“13077, 2023a.\\nHan Zhou, Xingchen Wan, Yinhong Liu, Nigel Collier, Ivan VuliÄ‡, and Anna Korhonen. Fairer preferences elicit improved\\nhuman-aligned large language model judgments. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors,\\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1241â€“1252, Miami,\\nFlorida, USA, November 2024a. Association for Computational Linguistics.\\nHan Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine A Heller, and Subhrajit Roy. Batch\\ncalibration: Rethinking calibration for in-context learning and prompt engineering. In The Twelfth International\\nConference on Learning Representations, 2024b.\\nHan Zhou, Xingchen Wan, Ruoxi Sun, Hamid Palangi, Shariq Iqbal, Ivan VuliÄ‡, Anna Korhonen, and Sercan Ã– ArÄ±k.\\nMulti-Agent design: Optimizing agents with better prompts and topologies. arXiv preprint arXiv:2502.02533, 2025a.\\nHuichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao,\\nLinyi Yang, et al. Memento: Fine-tuning llm agents without fine-tuning llms. arXiv preprint arXiv:2508.16153, 2025b.\\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan\\nBisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint\\narXiv:2307.13854, 2023b.\\nWangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua\\nXu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532, 2024c.\\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large lan-\\nguage models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations,\\n2023c.\\nZijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low,\\nand Paul Pu Liang. MEM1: Learning to synergize memory and reasoning for efficient long-horizon agents. arXiv\\npreprint arXiv:2506.15841, 2025c.\\nKunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian,\\nXiangru Tang, Heng Ji, et al. MultiAgentBench: Evaluating the collaboration and competition of llm agents. arXiv\\npreprint arXiv:2503.01935, 2025.\\nTinghui Zhu, Kai Zhang, Jian Xie, and Yu Su. Deductive beam search: Decoding deducible rationale for chain-of-thought\\nreasoning. In First Conference on Language Modeling, 2024.\\n54\\nXinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang.\\nSolving math word problems via cooperative reasoning induced language models. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational\\nLinguistics (ACL), 2023.\\nYangyang Zhuang, Wenjia Jiang, Jiayu Zhang, Ze Yang, Joey Tianyi Zhou, and Chi Zhang. Learning to be a doctor:\\nSearching for effective medical agent architectures. arXiv preprint arXiv:2504.11301, 2025.\\nYuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. ToolQA: A dataset for LLM question answering\\nwith external tools. In Advances in Neural Information Processing Systems, 2023.\\nYuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A Rossi, Somdeb Sarkhel, and Chao Zhang.\\nToolchain*: Efficient action space navigation in large language models with a* search. In The Twelfth International\\nConference on Learning Representations, 2024.\\nMingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and JÃ¼rgen Schmidhuber. GPTSwarm:\\nLanguage agents as optimizable graphs. In Forty-first International Conference on Machine Learning, 2024a.\\nMingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie\\nChang, Raghuraman Krishnamoorthi, Yuandong Tian, et al. Agent-as-a-judge: Evaluate agents with agents. arXiv\\npreprint arXiv:2410.10934, 2024b.\\nHuhai Zou, Rongzhen Li, Tianhao Sun, Fei Wang, Tao Li, and Kai Liu. Cooperative scheduling and hierarchical memory\\nmodel for multi-agent systems. In 2024 IEEE International Symposium on Product Compliance Engineering - Asia\\n(ISPCE-ASIA), pages 1â€“6, 2024. doi: 10.1109/ISPCE-ASIA64773.2024.10756271.\\nKaiwen Zuo, Yirui Jiang, Fan Mo, and Pietro Lio. KG4Diagnosis: A hierarchical multi-agent LLM framework with\\nknowledge graph enhancement for medical diagnosis. In AAAI Bridge Program on AI for Medicine and Healthcare,\\npages 195â€“204. PMLR, 2025.\\n55\\n\\n\\nA SURVEY OF MULTI-AGENT DEEP REINFORCEMENT\\nLEARNING WITH COMMUNICATION\\nChangxi Zhu\\nDepartment of Information and Computing Sciences\\nUtrecht University\\nUtrecht\\nc.zhu@uu.nl\\nMehdi Dastani\\nDepartment of Information and Computing Sciences\\nUtrecht University\\nUtrecht\\nm.m.dastani@uu.nl\\nShihan Wang\\nDepartment of Information and Computing Sciences\\nUtrecht University\\nUtrecht\\ns.wang2@uu.nl\\nABSTRACT\\nCommunication is an effective mechanism for coordinating the behaviors of multiple agents, broad-\\nening their views of the environment, and to support their collaborations. In the field of multi-agent\\ndeep reinforcement learning (MADRL), agents can improve the overall learning performance and\\nachieve their objectives by communication. Agents can communicate various types of messages,\\neither to all agents or to specific agent groups, or conditioned on specific constraints. With the grow-\\ning body of research work in MADRL with communication (Comm-MADRL), there is a lack of a\\nsystematic and structural approach to distinguish and classify existing Comm-MADRL approaches.\\nIn this paper, we survey recent works in the Comm-MADRL field and consider various aspects of\\ncommunication that can play a role in designing and developing multi-agent reinforcement learn-\\ning systems. With these aspects in mind, we propose 9 dimensions along which Comm-MADRL\\napproaches can be analyzed, developed, and compared. By projecting existing works into the multi-\\ndimensional space, we discover interesting trends. We also propose some novel directions for de-\\nsigning future Comm-MADRL systems through exploring possible combinations of the dimensions.\\nKeywords Multi-Agent Reinforcement Learning Â· Deep Reinforcement Learning Â· Communication Â· Survey\\n1\\nIntroduction\\nMany real-world scenarios, such as autonomous driving [1], sensor networks [2], robotics [3] and game-playing [4, 5],\\ncan be modeled as multi-agent systems. Such multi-agent systems can be designed and developed using multi-agent\\nreinforcement learning (MARL) techniques to learn the behavior of individual agents, which can be cooperative,\\ncompetitive, or a mixture of them. As agents are often distributed in the environment where they only have access to\\ntheir local observations rather than the complete state of the environment, partial observability becomes an essential\\nassumption in MARL [6, 7, 8]. Moreover, MARL suffers from the non-stationary issue [9], since each agent faces a\\ndynamic environment that can be influenced by the changing and adapting policies of other agents. Communication\\nhas been viewed as a vital means to tackle the problems of partial observability and non-stationary in MARL. Agents\\ncan communicate individual information, e.g., observations, intentions, experiences, or derived features, to have a\\nbroader view of the environment, which in turn allows them to make well-informed decisions [9, 10].\\nDue to the recent success of deep learning [11] and its application to reinforcement learning [12], multi-agent deep\\nreinforcement learning (MADRL) has witnessed great achievements in recent years, where agents can process high-\\ndimensional data and have generalization ability in large state and action spaces [7, 8]. We notice that a large number\\narXiv:2203.08975v2  [cs.MA]  18 Oct 2024\\nof research works focus on learning tasks with communication, which aim at learning to solve domain-specific tasks,\\nsuch as navigation, traffic, and video games, by communicating and sharing information. To the best of our knowledge,\\nthere is a lack of survey literature that can cover recent works on learning tasks with communication in multi-agent\\ndeep reinforcement learning (Comm-MADRL). Early surveys consider the role of communication in MARL but as-\\nsume it to be predefined rather than a subject of learning [13, 14, 15]. Most Comm-MADRL surveys cover only a\\nsmall number of research works without proposing a fine-grained classification system to compare and analyze them.1\\nIn cooperative scenarios, Hernandez-Leal et al. [16] use learning communication to denote the area of learning com-\\nmunication protocols to promote the cooperation of agents.2 The only survey that we found classifying some early\\nworks in Comm-MADRL is from Gronauer and Diepold [17], which is based on distinguishing whether messages are\\nreceived by all agents, a set of agents, or a network of agents. However, other aspects of Comm-MADRL, such as the\\ntype of messages and training paradigms, which are essential for communication and can help characterize existing\\ncommunication protocols, are ignored. As a result, the reviewed papers in recent surveys regarding learning tasks\\nwith communication are rather limited and the proposed categorizations are too narrow to distinguish existing works\\nin Comm-MADRL. On the other hand, there is a closely related research area, emergent language/communication,\\nwhich also considers learning communication through various reinforcement learning techniques [18]. Different from\\nComm-MADRL, the primary goal of emergent language studies is to learn a symbolic language.3 However, a subset\\nof emergent language research works pursues an additional goal to leverage learnable symbolic language to enhance\\ntask-level performance. Notably, these research works have not been encompassed within existing Comm-MADRL\\nsurveys but included in our survey, referred to learning tasks with emergent language. In summary, our survey overlaps\\nin scope with surveys of emergent language (i.e., in learning tasks with emergent language), but our survey focuses\\non different primary goals (i.e., achieving domain-specific tasks rather than learning a symbolic language). We further\\nclarify the differences between learning tasks with communication and emergent language in Section 2.2.\\nIn our survey paper, we review the Comm-MADRL literature by focusing on how communication can be utilized\\nto improve the performance of multi-agent deep reinforcement learning techniques. Specifically, we focus on learn-\\nable communication protocols, which are aligned with recent works that emphasize the development of dynamic and\\nadaptive communication, including learning when, how, and what to communicate with deep reinforcement learning\\ntechniques. Through a comprehensive review of recent Comm-MADRL literature, we propose a systematic and struc-\\ntured classification methodology designed to differentiate and categorize various Comm-MADRL approaches. Such\\na methodology will also provide guidance for the design and advancement of new Comm-MADRL systems. Suppose\\nwe plan to develop a Comm-MADRL system for a domain task at hand. Starting with the questions of when, how,\\nand what to communicate, the system can be characterized from various aspects. Agents need to learn when to com-\\nmunicate, with whom to communicate, what information to convey, how to integrate received information, and, lastly,\\nwhat learning objectives can be achieved through communication. We propose 9 dimensions that correspond to unique\\naspects of Comm-MADRL systems: Controlled Goals, Communication Constraints, Communicatee Type, Commu-\\nnication Policy, Communicated Messages, Message Combination, Inner Integration, Learning Methods, and Training\\nSchemes. These dimensions, which form the skeleton of a Comm-MADRL system, can be used to analyze and gain\\ninsights into designed Comm-MADRL approaches thoroughly. By mapping recent Comm-MADRL approaches into\\nthis multi-dimensional structure, we not only provide insight into the current state of the art in this field but also\\ndetermine some important directions for designing future Comm-MADRL systems.\\nThe remaining sections of this paper are organized as follows. In Section 2 the preliminaries of multi-agent RL are\\ndiscussed, together with existing extensions regarding communication and a detailed comparison of recent surveys. In\\nSection 3, we present our proposed dimensions, explaining how we group the recent works in the categories of each\\ndimension. In Section 4, we discuss the trends that we found in the literature, and, driven by the proposed dimensions,\\nwe propose possible research directions in this research area. We finalize the paper with some conclusions in Section\\n5.\\n2\\nBackground\\nIn this section, we first provide the necessary background on multi-agent reinforcement learning. Then, we show how\\nmulti-agent reinforcement learning can be extended to consider communication between agents. Finally, we present\\n1We provide a detailed comparison of recent surveys on MADRL which involves communication in Section 2.3.\\n2In our survey, we extend the concept of learning communication to general multi-agent tasks and use the term learning tasks\\nwith communication to emphasize that the primary goal of recent research, which is centered on solving specific domain tasks\\nthrough the use of communication.\\n3In the literature, emergent language and emergent communication are used interchangeably. In our survey, we use emergent\\nlanguage for referring to both terms.\\n2\\nand compare recent surveys involving communication, from which we can directly see our motivations to fill the gaps\\namong existing surveys.\\n2.1\\nMulti-agent Reinforcement Learning\\nReal-world applications often contain more than one agent that operate in the environment. Agents are generally\\nassumed to be autonomous and required to learn their strategies for achieving their goals. A multi-agent environment\\ncan be formalized in several ways [19], depending on whether the environment is fully observable, how agentsâ€™ goals\\nare correlated, etc. Among them, the Partially Observable Stochastic Game (POSG) [20, 21] is one of the most flexible\\nformalizations. A POSG is defined by a tuple\\n\\nI, S, Ï0, {Ai} , P, {Oi} , O, {Ri}\\n\\x0b\\n, where I is a (finite) set of agents\\nindexed as {1, ..., n}, S is a set of environment states, Ï0 is the initial state distribution over state space S, Ai is a set of\\nactions available to agent i, and Oi is a set of observations of agent i. We denote a joint action space as A = Ã—iâˆˆIAi\\nand a joint observation space of agents as O = Ã—iâˆˆIOi. Therefore, P : S Ã— A â†’âˆ†(S) denotes the transition\\nprobability from a state s âˆˆS to a new state sâ€² âˆˆS given agentsâ€™ joint action âƒ—a = âŸ¨a1, ..., anâŸ©, where âƒ—a âˆˆA. With the\\nenvironment transitioning to the new state sâ€², the probability of observing a joint observation âƒ—o = âŸ¨o1, ..., onâŸ©(where\\nâƒ—o âˆˆO) given the joint action âƒ—a is determined according to the observation probability function O : S Ã— A â†’âˆ†(O).\\nEach agent then receives an immediate reward according to their own reward functions Ri : S Ã— A Ã— S â†’R.\\nSimilar to the joint action and observation, we could denote âƒ—r = âŸ¨r1, ..., rnâŸ©as a joint reward. If agentsâ€™ reward\\nfunctions happen to be the same, i.e., they have identical goals, then r1 = r2 = ... = rn holds for every time step.\\nIn this setting, the POSG is reduced to a Dec-POMDP [19]. If at every time step the state is uniquely determined\\nfrom the current set of observations of agents, i.e., s â‰¡âƒ—o, the Dec-POMDP is reduced to a Dec-MDP. If each agent\\nknows what the true environment state is, the Dec-MDP is reduced to a Multi-agent MDP. If there is only one single\\nagent in the set of agents, i.e., I = {1}, then the Multi-agent MDP is reduced to an MDP and the Dec-POMDP\\nis reduced to a POMDP. Due to the partial observability, MARL methods often use the observation-action history\\nÏ„i,t = {oi,0, ai,0, oi,1, ..., oi,t} up to time step t for each agent to approximate the environment state. Note that time\\nstep t is often omitted for the sake of simplification.\\nIn the multi-agent reinforcement learning setting, agents can learn their policies in either a decentralized or a cen-\\ntralized fashion. In decentralized learning (e.g., decentralized Q-learning [22, 23]), an n-agent MARL problem is\\ndecomposed into n decentralized single-agent problems where each agent learns its own policy by considering all\\nother agents as a part of the environment [24, 25]. In such a decentralized setting, the learned policy of each agent\\nis conditioned on its local observation and history. A major problem with decentralized learning is the so-called\\nnon-stationarity of the environment, i.e., the fact that each agent learns in an environment where other agents are\\nsimultaneously exploring and learning. Centralized learning enables the training of either a single joint policy for all\\nagents or a centralized value function to facilitate the learning of n decentralized policies. While centralized (joint)\\nlearning removes or mitigates issues of partial observability and non-stationarity, it faces the challenge of joint action\\n(and observation) spaces that expand exponentially with the number of agents and their actions. For a deeper dive into\\nvarious training schemes used in MARL, we recommend the comprehensive survey by [17], which offers valuable\\ninsights into the training and execution of policies. Based on whether policies are derived from value functions or\\ndirectly learned, multi-agent reinforcement learning methods can be categorized into value-based and policy-based\\nmethods. Both methods have been largely utilized in Comm-MADRL.\\nValue-based\\nValue-based methods in the multi-agent case borrow considerable ideas from the single-agent case. As\\none of the most popular value-based algorithms, the decentralized Q-learning learns a local Q-function for each agent.\\nIn the cooperative setting where agents share a common reward, the update rule for agent i is as follows:\\nQi(s, ai) â†Qi(s, ai) + Î±(r + Î³ max\\naâ€²\\ni\\nQi(sâ€², aâ€²\\ni)\\n|\\n{z\\n}\\nnew estimate\\nâˆ’Qi(s, ai)\\n|\\n{z\\n}\\ncurrent estimate\\n)\\n(1)\\nwhere r is the shared reward, and aâ€²\\ni is the action with the highest Q-value in the next state sâ€². In partially observable\\nenvironments, the environment state is not fully observable and is usually replaced by the individual observation\\nor history of each agent. The Q-values for each state-action pair are incrementally updated according to the TD\\nerror. This error, i.e., r + Î³ maxaâ€²\\ni Qi(sâ€², aâ€²\\ni) âˆ’Qi(s, ai), represents the difference between a new estimate (i.e.,\\nr + Î³ maxaâ€²\\ni Qi(sâ€², aâ€²\\ni)) and the current estimate (i.e., Qi(s, ai)) based on the Bellman equation [26]. As the state\\nand action space could be too large to be encountered frequently for accurate estimation, function approximation\\nmethods, like deep neural networks, have become popular for endowing value or policy models with generalization\\nabilities across both discrete and continuous states and actions [12]. For example, the Deep Q-network (DQN) [12]\\nminimizes the difference between the new estimate calculated from sampled rewards and the current estimate of a\\nparameterized Q-function. In DQN-based methods, the Q-function in Equation 1 is notated as Qi(s, ai; Î¸i), which\\n3\\ndepends on learnable parameters Î¸i. On the other hand, centralized learning in value-based methods learns a joint Q-\\nfunction Q(s,âƒ—a; Î¸) with parameters Î¸. However, this approach can be challenging to scale with an increasing number\\nof agents. Value decomposition methods [27, 28, 29, 30] are popular MARL methods that decompose a joint Q-\\nfunction to enable efficient training. These methods are also widely employed in research works in Comm-MADRL\\n[31, 32, 33]. In partially observable environments, linear value decomposition methods decompose history-based joint\\nQ-functions as follows:\\nQjoint(âƒ—Ï„,âƒ—a) =\\nn\\nX\\ni\\nwiQi(Ï„i, ai)\\n(2)\\nwhere the joint Q-function is based on the joint history of all agents and is decomposed into local Q-functions based\\non individual histories. The weight wi can either be a fixed value [27, 29] or a learnable parameter subject to certain\\nconstraints [30]. Advantage functions can also replace the Q-function in the above equation to reduce variance [34].\\nPolicy-based\\nPolicy-based methods directly search over the policy space instead of obtaining the policy through\\nvalue functions implicitly. The policy gradient theorem [26] provides an analytical expression of the gradients for a\\nstochastic policy with learnable parameters in single-agent cases. In the multi-agent case with centralized learning,\\nthe policy gradient theorem is expressed as follows:\\nâˆ‡Î¸J(Î¸) = Eâƒ—aâˆ¼Ï€(Â·|s),sâˆ¼ÏÏ€[âˆ‡Î¸ log Ï€(âƒ—a | s; Î¸)QÏ€(s,âƒ—a)]\\n(3)\\nwhere J(Î¸) represents the learning objective, and Ï€(âƒ—a | s; Î¸) denotes a stochastic policy parameterized by Î¸ (abbre-\\nviated as Ï€). Additionally, ÏÏ€ signifies the state distribution under the policy Ï€, and âˆ‡Î¸J(Î¸) represents the expected\\ngradient with respect to all possible actions and states. Due to the computational intractability of the expected gradient,\\nstochastic gradient ascent can be applied to update the parameters Î¸ at every learning step l as follows:\\nÎ¸l+1 = Î¸l + Î± \\\\\\nâˆ‡Î¸J(Î¸)\\nwhere Î± is the learning rate, and \\\\\\nâˆ‡Î¸J(Î¸) is an estimate of the expected gradient based on sampled actions and states.\\nMoreover, the Q-function in Equation 3 can be replaced by average returns over episodes to form REINFORCE\\nalgorithms [26], or by an estimated value function to form actor-critic algorithms [35, 36]. In actor-critic methods, the\\npolicy and value function are termed the actor and the critic, respectively. The critic will, therefore, guide the learning\\nof the actor.\\nActor-critic methods have undergone various adaptations for multi-agent environments [7, 8, 37, 38]. A typical ex-\\ntension is the multi-agent deep deterministic policy gradient (MADDPG) [7]. In MADDPG, the critic is a centralized\\nQ-function designed to capture global information and coordinate learning signals. Meanwhile, the actors are local\\npolicies, ensuring decentralized execution. MADDPG assumes deterministic actors with continuous actions, allowing\\nfor the backpropagation of gradients from the value function to the policies. The gradient of each parameterized actor\\nÂµÎ¸i(ai | oi) with learnable parameters Î¸i, abbreviated as Âµi, is defined as follows:\\nâˆ‡Î¸iJ (Î¸i) = Eâƒ—o,âƒ—aâˆ¼D\\n\\x02\\nâˆ‡Î¸iÂµi (ai | oi) âˆ‡aiQÂµ\\ni (âƒ—o, a1, . . . , aN) |ai=Âµi(oi)\\n\\x03\\nwhere D is the experience buffer that contains joint observation-action tuples âŸ¨âƒ—o,âƒ—a,âƒ—r, âƒ—oâ€²âŸ©. Each agentâ€™s Q-function,\\ndenoted as QÂµ\\ni (âƒ—o, a1, . . . , aN), takes joint observations and actions as inputs, while decentralized actors use local\\nobservations as inputs. Contrary to Equation 3, gradients with respect to the current action of agent i (specifically,\\nÂµi(oi)) are utilized to guide the update of the policy parameter Î¸i. Both MADDPG and its single-agent counterpart,\\nDDPG, have seen widespread application in Comm-MADRL [39, 40, 41, 42, 43].\\n2.2\\nExtensions with Communication\\nIn the MADRL literature where communication is used, we notice two closely related research areas, which we will re-\\nfer to with the terms emergent language and learning tasks with communication. The emergent language research area\\n[18, 44, 45, 46, 47] aims at learning a language grounded on symbols in communities of interacting/communicating\\nagents. This line of research tries to understand the evolution of the language in agents equipped with neural networks.\\nOn the other hand, learning tasks with communication [16, 48, 49, 50] focuses primarily on solving multi-agent rein-\\nforcement learning tasks with the aid of communication. Communication is often regarded as information exchange\\nrather than learning a (human-like) language. Despite the distinction, when using MADRL techniques on specific\\ndomain tasks, languages might emerge, which can potentially enhance the learning systemâ€™s explainability in accom-\\nplishing those tasks. We illustrate the research areas, emergent language and learning tasks with communication,\\nalong with their intersection learning tasks with emergent language in Figure 1. Notably, our survey focuses on learn-\\ning tasks with communication in multi-agent deep reinforcement learning, including the intersection with emergent\\n4\\nMulti-agent Reinforcement Learning Tasks\\nLearning Tasks with\\nCommunication\\nEmergent\\nLanguage\\nLearning Tasks with\\nEmergent Language\\nFigure 1: An illustration depicting the scope of this survey. The focus of our survey is represented by the blue part.\\nlanguage.4 Within this focus, multiple agents often operate in partially observable environments and learn to share\\ninformation encoded through neural networks. Furthermore, communication protocols, determining when and with\\nwhom to communicate, often leverage deep learning models to find the optimal choices that minimize communica-\\ntion overhead and yield more targeted communication. A multitude of works have been proposed to handle these\\nsubproblems inherent in Comm-MADRL. Most research works model only one or a few aspects of Comm-MADRL\\nwhile selecting a default approach for other aspects. Given that the common goal of Comm-MADRL approaches is to\\ndesign an effective and efficient communication protocol to improve agentsâ€™ learning performance in the environment,\\nthe proposed Comm-MADRL approaches inevitably share similarities to some extent. Consequently, establishing\\na classification system for Comm-MADRL becomes crucial. Such a system would aid in categorizing critical ele-\\nments like contributions, targeted problems, and learning objectives, from which we can compare and analyse existing\\nComm-MADRL approaches.\\nIn the emergent language literature, numerous works employ various forms of the Lewis game, often referred to as ref-\\nerential games and operate under a cheap-talk setting [51], as highlighted in several surveys [10, 18].5 In these games,\\na goal, often represented as a target location, an image, or a semantic concept, is given to a sender agent but remains\\nunrevealed from a receiver agent. The receiver agent must then either identify the correct goal based on the senderâ€™s\\nsignaling [52, 53, 54, 47, 55, 56, 57, 58] or accomplish its single-agent task using the received signals (messages)\\n[59, 60]. Research works in learning tasks with emergent language are grounded in a multi-agent environment where\\nthe joint actions of both sender and receiver agents impact environment transitions. Consequently, the learning tasks\\nwith emergent language literature considers multi-agent domain tasks [61, 62, 63, 64, 65], building on foundational\\nconcepts from MARL such as Dec-POMDPs or POSGs.\\nWe further distinguish explicit versus non-explicit communication [19] in the literature of MADRL with communica-\\ntion. Explicit communication refers to communication through a set of messages separate from domain-level actions.\\nHere, agentsâ€™ action policies are influenced by both their observations and the messages they receive. Such messages,\\ncrucial for supporting agentsâ€™ decision-making, are essential in both the training and execution phases. MADRL\\nframeworks without explicit communication can still allow for communication through domain-level actions, such as\\nthe act of influencing the observations of one agent through the actions of another. Furthermore, without explicit com-\\nmunication, agents can transmit gradient signals, which facilitate centralized training (and decentralized execution) but\\nare not utilized during execution phases. Specifically, in our survey, we focus on explicit and learnable communication.\\nDec-POMDPs and POSGs are often extended to accommodate explicit communication. The communication can\\nbe integrated into the action set, adding a collection of communication acts alongside domain-level actions. Al-\\nternatively, a Dec-POMDP or a POSG can be extended to explicitly include a set of messages [19].\\nFor in-\\nstance, the POSG can be expanded with a (shared) message space M, resulting in a POSG-Comm, defined as\\n\\nI, S, Ï0, {Ai} , P, {Oi} , O, {Ri} , M\\n\\x0b\\n, where all components remain unchanged except for the added message\\nspace M. A Dec-POMDP-Comm can be defined as similar to the POSG-Comm with shared rewards. In both POSG-\\nComm and Dec-POMDP-Comm, action policies take into account both environmental observations and inter-agent\\nmessages. Research works in Comm-MADRL that expand upon a POSG or a Dec-POMDP can be seen in references\\nsuch as [61, 63, 66, 65, 67].\\n4Throughout the remainder of our survey, Comm-MADRL will be used to specifically refer to the areas of our focus.\\n5In the emergent language research area, research works that do not adopt the cheap-talk setting but communicate through\\nobservable (domain-level) actions, are not included in our survey. Our survey focuses on explicit message transfer between agents.\\n5\\n2.3\\nCommunication in Recent Surveys\\nCommunication has attracted much attention in the field of multi-agent reinforcement learning (MARL). Previous\\nsurveys mentioning communication in MARL primarily focus on providing an overview of MARLâ€™s development.\\nThese surveys view communication as a subfield in MARL, and no extensive and substantial progress is reported. In\\nan early survey, Stone and Veloso [13] classify MARL based on whether agents communicate and whether agents\\nare homogeneous or not.6 They view learnable communication as a future research opportunity. Busoniu et al. [15]\\nconsider communication as a means to negotiate action choices and select equilibrium in the research direction of\\nexplicit coordination, without further classifying communication. With the advancement of deep learning, MARL\\nhas gradually incorporated deep neural networks such that recent developments are dominated by multi-agent deep\\nreinforcement learning (MADRL). In the MADRL context, Hernandez-Leal et al. [16], Nguyen et al. [68], and Pa-\\npoudakis et al. [9] briefly review early Comm-MADRL methods, which have now become baselines in many recent\\nworks. Specifically, Hernandez-Leal et al. [16] use learning communication to denote a new branch in MADRL.\\nPapoudakis et al. [9] consider communication as an approach to handle the non-stationary problem in MADRL, as\\nagents can exchange information to stabilize their training. Compared to the aforementioned surveys, OroojlooyJadid\\nand Hajinezhad [37] provide a more detailed review of Comm-MADRL, covering a significant number of existing\\nworks. They view communication as a way to solve cooperative MADRL problems but did not propose a catego-\\nrization model for Comm-MADRL. Zhang et al. [69] and Yang et al. [21] review communication from a theoretical\\nperspective. Their primary focus is on communication within networked multi-agent systems. In these systems, agents\\nshare information through a time-varying network, aiming to reach consensus on learned value functions or policies.\\nDespite this, no further classification of communication is made.\\nTwo more recent surveys in MADRL, proposed by Gronauer and Diepold [17] and Wong et al. [70], focus on classi-\\nfying existing works on communication. Gronauer and Diepold classify early research works in Comm-MADRL into\\nBroadcasting, Targeted, and Networked communication, based on whether messages are received from all agents, a\\nsubset of agents, or a network of agents. Wong et al., similar to the survey of Papoudakis et al. [9], view commu-\\nnication as a method to address the issues of non-stationarity and partial observability. In the survey of Wong et al.,\\nresearch works on communication are categorized into three groups from a high-level perspective: communication as\\nthe primary learning goal, communication as an instrument to learn a specific task, and peer-to-peer teaching. How-\\never, they do not delve into how agents utilize communication to enhance learning. These surveys focus on limited\\naspects of communication, making their categorizations too narrow to distinguish recent works effectively, given the\\nfact that many existing works share similar assumptions and conditions. To the best of our knowledge, only one survey\\n[71] exclusively focuses on communication issues in MADRL. They review algorithms for communication and coop-\\neration, including efforts to interpret languages developed through communication. Despite this, their survey mainly\\ncovers early models without proposing a categorization framework.\\nThe literature has investigated communication from other perspectives. Shoham and Leyton-Brown [72] investigate\\ncommunication from a game-theoretic perspective. They introduce several theories of communication in multi-agent\\nsystems, with the particular concern that agents can be self-motivated to convey information, driven by underlying\\nincentives (e.g., the knowledge of game structure), or communicate in a pragmatic way analogous to human com-\\nmunication. Deep neural networks and deep reinforcement learning techniques have greatly widened the scope of\\nlanguage development in multi-agent systems. Lazaridou and Baroni [18] provide an extensive survey focused on\\nemergent language, aiming to establish effective human-machine communication. As highlighted in section 2.2, the\\nprimary goal of emergent language research is to learn a human-like language from scratch. The goal of our survey\\nis, however, to classify the literature on learning tasks with communication that aims at exploiting communication to\\naccomplish multi-agent tasks.\\nIn summary, existing surveys in Comm-MADRL lack coverage of the latest developments. These surveys also do\\nnot elaborate on the fact that communication itself is a combinatorial problem. Importantly, communication models\\nengage with MADRL algorithms across various processes, including learning and decision-making. To effectively\\ndistinguish between existing Comm-MADRL approaches, it is crucial to analyze and classify them from a wider range\\nof perspectives. In the following section, we delve into the field of Comm-MADRL through multiple dimensions, each\\nlinked to a unique research question pertinent to system design. These dimensions allow us to provide a fine-grained\\nclassification, highlighting the differences between Comm-MADRL approaches even within similar domains.\\n6Homogeneous agents have the same internal structure including goals, domain knowledge, and possible actions.\\n6\\nTable 1: Proposed dimensions and associated research questions.\\nKey Components\\nTarget Questions\\nDimensions\\nIndex\\nProblem Settings\\nWhat kind of behaviors are desired to emerge\\nwith communication?\\nControlled Goals\\n1âƒ\\nHow to fulfill realistic requirements?\\nCommunication\\nCon-\\nstraints\\n2âƒ\\nWhich type of agents to communicate with?\\nCommunicatee Type\\n3âƒ\\nCommunication\\nProcesses\\nWhen and how to build communication links\\namong agents?\\nCommunication Policy\\n4âƒ\\nWhich piece of information to share?\\nCommunicated\\nMes-\\nsages\\n5âƒ\\nHow to combine received messages?\\nMessage Combination\\n6âƒ\\nHow to integrate combined messages into learn-\\ning models?\\nInner Integration\\n7âƒ\\nTraining Processes\\nHow to train and improve communication?\\nLearning Methods\\n8âƒ\\nHow to utilize collected experience from agents?\\nTraining Schemes\\n9âƒ\\n3\\nLearning Tasks with Communication in MADRL\\nIn our survey, we consider explicit communication where action policies of agents are conditioned on communica-\\ntion that is learnable and dynamic, rather than static and predefined. Therefore, both the content of the messages\\nand the chances of communication occurrences are subject to learning. As agents engage in multi-agent tasks, they\\nlearn domain-specific action policies and their communication protocols concurrently. As a result, learning tasks with\\ncommunication becomes a joint learning challenge, where agents employ reinforcement learning to maximize envi-\\nronmental rewards and simultaneously utilize various machine learning techniques to develop efficient and effective\\ncommunication protocols.\\nLearning tasks with communication in multi-agent deep reinforcement learning (Comm-MADRL) is a significant\\nresearch problem, particularly as communication can lead to higher rewards. Numerous studies have emerged, devel-\\noping effective and efficient Comm-MADRL systems, often sharing similarities. Our review begins with the seminal\\nworks such as DIAL[73], RIAL[73], and CommNet[48], and then expands to include the most relevant research works\\npresented at major AI conferences and journals like AAMAS, AAAI, NeurIPS, and ICML, totaling 41 models in\\nComm-MADRL. To better distinguish among these models, we propose classifying them based on several dimensions\\nin Comm-MADRL system design. These dimensions aim to comprehensively cover the current literature, allowing\\nus to project the research works into a space where their similarities and differences become clear. We start by focus-\\ning on three key components of Comm-MADRL systems: problem settings, communication processes, and training\\nprocesses. Problem settings encompass both communication-specific settings (e.g., communication constraints) and\\nnon-communication-specified settings (e.g., reward structures). Communication processes include common communi-\\ncation procedures, such as deciding whether to communicate and what messages to communicate. Training processes\\ncover the learning of both agents and communication within MADRL. Based on the three key components, we iden-\\ntify and summarize 9 research questions that commonly arise in Comm-MADRL system design, corresponding to 9\\ndimensions as detailed in Table 1. These research questions and dimensions are designed to capture various aspects\\nof Comm-MADRL, covering the learning objectives of agents and communication, the processes by which messages\\nare generated, transmitted, integrated, and learned within the MADRL framework. We outline a systematic procedure\\nfor providing a guideline to effectively navigate through these dimensions when developing Comm-MADRL systems.\\nThe procedure allows us to organize the dimensions, demonstrate their relevance in system design, and guide the\\ncreation of customized Comm-MADRL systems in a step-by-step manner.\\nAs outlined in Procedure 1, N reinforcement learning agents employ communication throughout their learning and\\ndecision-making. Initially, the learning objective for the N agents is set, defining rewards that induce cooperative,\\ncompetitive, or mixed behaviors, as captured by dimension 1. We then consider potential communication-specified\\nsettings like limited resources, addressing the need for realistic scenarios as described in dimension 2. Dimension 3\\nidentifies potential communicatees, determining the agents for messages to be received, which varies across domains.\\nAt each time step, agents decide when and with whom to communicate, as highlighted in dimension 4. The patterns of\\ncommunication occurrences are structured like a graph, where links, either undirected or directed, aid information ex-\\nchange. Subsequently, messages that encapsulate agentsâ€™ understanding of the environment are generated and shared,\\nrelating to dimension 5. Given that agents often receive multiple messages, they must decide on how to combine these\\nmessages effectively. This process, crucial for integrating messages into their policies or value functions, is captured\\n7\\nin dimensions 6 and 7. In cases of Comm-MADRL studies focusing on emergent language (i.e., learning tasks with\\nemergent language), where messages are modeled as communicative acts emitted alongside domain-level actions, a\\nspecific rearrangement of the procedure is required. Here, messages are not observed by other agents until the next\\ntime step. Therefore, the processes outlined in dimensions 6 and 7 (lines 8 and 9) are moved to the front of those in\\ndimension 4 (line 6). This rearrangement allows agents to combine and integrate messages from the previous time\\nstep before initiating new communication. As a result, agents make decisions and perform actions in the environment\\nbased not only on their environmental observations but also on information obtained from other agents (lines 10 and\\n11). During the training phase, experiences from both environmental interactions and inter-agent communication are\\nutilized to train how agents will behave and communicate, i.e., agentsâ€™ policies, value functions, and communication\\nprocesses, as characterized in dimensions 8 and 9 (line 14).\\nIn the following sections, we make an extensive survey on Comm-MADRL based on each dimension and classify the\\nliterature when we focus on a specific dimension. We finally provide a comprehensive table to frame recent works\\nwith the aid of the 9 dimensions.\\nProcedure 1 A guideline of Comm-MADRL systems\\nRequire: N reinforcement learning agents\\n1: Set goals for reinforcement learning agents\\nâ–·Dimension 1âƒ\\n2: Set possible communication constraints\\nâ–·Dimension 2âƒ\\n3: Set the type of communicatees\\nâ–·Dimension 3âƒ\\n4: for episode = 1, 2, ... do\\n5:\\nfor every environment step do\\n6:\\nDecide with whom and whether to communicate\\nâ–·Dimension 4âƒ\\n7:\\nDecide which piece of information to share\\nâ–·Dimension 5âƒ\\n8:\\nCombine received information shared from others\\nâ–·Dimension 6âƒ\\n9:\\nIntegrate messages into agentsâ€™ internal models\\nâ–·Dimension 7âƒ\\n10:\\nSelect actions based on communication\\n11:\\nPerform in the environment (and store experiences)\\n12:\\nend for\\n13:\\nif training is enablled then\\n14:\\nUpdate agentsâ€™ policies, value function, and communication processes\\nâ–·Dimensions 8âƒ& 9âƒ\\n15:\\nend if\\n16: end for\\nProcedure 1: A guideline of Comm-MADRL systems. The guideline positions dimensions where communication\\ninfluences interaction with the environment and training phases.\\n3.1\\nControlled Goal\\nWith a given reward configuration, reinforcement learning agents are guided to achieve their designated goals and\\ninterests. As agents communicate in order to obtain higher rewards, the goal of communication and the goal of\\nachieving domain-specific tasks are inherently aligned. The emergent behaviors of agents can be summarized into\\nthree types: cooperative, competitive, and mixed [96, 23], each corresponding to different reward configurations\\nand goals. Notably, some Comm-MADRL methods have been tested in more than one benchmark environment to\\nshow their flexibility and scalability, where the reward configurations may vary [49, 81, 89, 77, 92]. Furthermore, a\\nmulti-agent environment may consist of both fixed opponents and teammates, which typically do not participate in\\ncommunication. Therefore, we exclude fixed agents when identifying reward configurations. Consequently, we focus\\non (learnable) agents involved in communication and classify their behaviors that are desired to emerge, aligning them\\nwith associated reward configurations (summarized in Table 2).\\nCooperative\\nIn cooperative scenarios, agents have the incentive to communicate to achieve better team performance.\\nCooperative settings can be characterized by either a global reward that all agents share or a sum of local rewards that\\ncould be different among agents. Communication is usually used to promote cooperation as a team. Thus, in the\\nliterature, a team of agents can receive a global reward [73, 48, 61, 88, 39, 89, 40, 41, 43, 74, 31, 75, 66, 76, 63,\\n77, 78, 79, 90, 32, 91, 92, 93, 80, 64, 33, 94], which does not account for the contribution of each agent. The\\nagents can also receive local rewards, with designs to make the reward depend on teammatesâ€™ collective performance\\n[50, 88, 81, 49, 42, 82, 83, 87, 91], to penalize collisions [39, 81, 82, 83, 90, 86, 92, 93], or to share the reward with\\nother agents for encouraging mutual cooperation [84, 85, 65].\\nThere are a variety of cooperative environments where communication has shown performance improvements, from\\nsmall-scale games to complex video games. In early works, Foerster et al. [73] developed two simple games, named\\n8\\nTable 2: The category of controlled goals.\\nTypes\\nConfigurations\\nMethods\\nCooperative\\nGlobal Rewards\\nDIAL [73]; RIAL [73]; CommNet [48]; GCL [61]; MAGNet-\\nSA-GS-MG [40]; MADDPG-M [41]; SchedNet [43]; Agent-\\nEntity Graph [74]; VBC [31]; NDQ [75]; IMAC [66]; Gated-\\nACML [76]; Bias [63]; LSC [77]; Diff Discrete[78]; I2C\\n[79]; TMC [32]; GAXNet [80]; DCSS [64]; MAIC [33];\\nLocal Rewards\\nBiCNet [50]; DGN [81]; IC3Net [49]; MD-MADDPG [42];\\nDCC-MD [82]; GA-Comm [83]; NeurComm [84]; IP [85];\\nETCNet [86]; Variable-length Coding [87]; AE-Comm [65];\\nGlobal or Local Rewards\\nMS-MARL-GCM [88]; ATOC [39]; TarMAC [89]; IS [90];\\nHAMMER [91]; MAGIC [92]; FlowComm [93]; FCMNet\\n[94];\\nCompetitive\\nConflict Rewards\\nIC3Net [49]; R-MACRL [95];\\nMixed\\nSelf-interested Rewards\\nIC [62]; DGN [81]; TarMAC [89]; IC3Net [49]; NDQ [75];\\nLSC [77]; MAGIC [92];\\nSwitch Riddle and MNIST Games, for their proposed models, DIAL and RIAL. Sukhbaatar et al. [48] used Traffic\\nJunction for evaluating CommNet, which has become a popular testbed in recent works [88, 89, 49, 83, 79, 90, 92].\\nAmong them, MAGIC [92] achieved higher performance on Traffic Junction with local rewards compared to two\\nearly works, CommNet [48], IC3Net [49], and one recent work, GA-Comm [83]. StarCraft [97, 98, 99] is another\\nbenchmark environment in cooperative MARL with relatively flexible settings. BiCNet [50] and MS-MARL-GCM\\n[88] are evaluated on an early version of StarCraft [97]. Then, a new version of StarCraft, SMAC, has become\\npopular in recent works [31, 75, 66, 32, 33, 94]. By controlling a team of agents, the cooperative goal in SMAC is\\nto defeat enemies on easy, hard, and super hard maps. FCMNet [94] and MAIC are two recent works that surpass\\nmultiple communication methods and value decomposition methods (e.g., QMIX) on different maps. Google research\\nfootball [100] is an even more challenging game with a physics-based 3D soccer simulator. Only MAGIC has reported\\nperformance on this platform with communication, and more investigations on this environment are needed. Compared\\nto the above approaches in Comm-MADRL, ATOC [39] has been examined using a significantly larger number of\\nlearning agents in the predator-prey domain. Predator-prey is a grid world game with a long history in MARL. It\\nhas been developed with several versions [101, 102, 7], while still viewed as a standard test environment due to its\\nflexibility and customizability. ATOC reports performance on this platform with continuous state and action spaces. In\\nthe subfield learning tasks with emergent language, cooperative scenarios are popularly used. They are mostly based\\non grid world or particle environments and have explicit role assignments, e.g., senders and receivers [61, 63, 65, 64].\\nCompetitive\\nIn case agents need to compete with each other to occupy limited resources, they are assigned com-\\npetitive learning objectives. In some competitive games, such as zero-sum games, one player wins and the others lose\\nand therefore rational agents do not have the incentive to communicate. Nevertheless, in other competitive scenarios\\nwhere agents compete for long-term goals, communication can allow for low-level cooperation among agents before\\nthe (long-term) goals are achieved. Based on our observations, only one work, IC3Net [49], tests competitive set-\\ntings and enables agents to compete for rewards.7 IC3Net shows that competitive agents communicate only when it\\nis profitable, e.g., before catching prey in the predator-prey domain. R-MACRL [95] considers communication from\\nmalicious agents to improve the worst-case performance. In R-MACRL, the whole environment is cooperative while\\nagents learn to defend against malicious messages. Although the environment is cooperative, we classify this work\\nunder the competitive category as the learning goal between malicious agents and other agents is competitive.\\nMixed\\nFor a MAS where we care about self-interest agents, individual rewards can be designed and distributed to\\neach agent [81, 89, 49, 75, 77, 92, 94]. Therefore, cooperative and competitive behaviors coexist during learning,\\nwhich may show more complex communication patterns. Specifically, DGN [81] considers a game where each agent\\ngets positive rewards by eating food but gets higher rewards by attacking other agents. However, being attacked will\\nget a high punishment. With communication, agents can learn to share resources collaboratively rather than attacking\\neach other. IC3Net [49], TarMAC [89] and MAGIC [92] are evaluated on a mixed version of Predator-prey, and agents\\nlearn to communicate only when necessary. NDQ [75] is examined in an independent search scenario, where two\\n7IC3Net has been tested in several settings, including cooperative, competitive, and mixed scenarios, with different reward\\nconfigurations.\\n9\\nTable 3: The category of communication constraints.\\nTypes\\nSubtypes\\nMethods\\nUnconstrained Communi-\\ncation\\nCommNet [48]; BiCNet [50]; MS-MARL-GCM [88]; ATOC\\n[39]; DGN [81]; TarMAC [89]; MAGNet-SA-GS-MG [40];\\nMADDPG-M [41]; IC3Net [49]; MD-MADDPG [42]; DCC-MD\\n[82]; Agent-Entity Graph [74]; GA-Comm [83]; LSC [77]; Neur-\\nComm [84]; IP [85]; I2C [79]; IS [90]; HAMMER [91]; MAGIC\\n[92]; FlowComm [93]; GAXNet [80]; FCMNet [94];\\nConstrained Communica-\\ntion\\nLimited Band-\\nwidth\\nRIAL [73]; DIAL [73]; GCL [61]; IC [62]; SchedNet [43]; VBC\\n[31]; NDQ [75]; IMAC [66]; Gated-ACML [76]; Bias [63]; ETC-\\nNet [86]; Variable-length Coding [87]; TMC [32]; AE-Comm\\n[65]; MAIC [33];\\nCorrupted\\nMessages\\nDIAL [73]; Diff Discrete[78]; DCSS [64]; R-MACRL [95];\\nagents are rewarded according to their own goals, and shows that agents learn to not communicate in independent\\nscenarios. IC [62] considers a scenario in which sender and receiver agents have different abilities to complete the\\ngoal. The sender agents have more vision but cannot clean obstacles, while receiver agents have limited vision but are\\nable to clear obstacles. With communication, agents show collaborative behaviors to get higher rewards.\\n3.2\\nCommunication Constraints\\nPractical concerns such as communication cost and noisy environment impair Comm-MADRL systems from embrac-\\ning realistic applications more than simulations. This dimension, Communication Constraints, determines which type\\nof communication concerns are handled in a Comm-MADRL system. We categorize recent works on this dimension\\ninto the following categories (summarized in Table 3).\\nUnconstrained Communication\\nIn this category, communication processes, including communication channels,\\nthe content and transmission of messages, and the decisions of whether to communicate or not, are not explicitly\\nrestricted. In principle, agents can communicate as much as information they can without any decision to disallow\\ncommunication in order to prevent communication overhead [48, 50, 88, 81, 89, 40, 42, 90, 91, 80, 94]. Specifically,\\nseveral works consider blocking communication through predefined or learnable decisions of whether to communicate\\nor not, while aiming to differentiate useful communicated information [39, 41, 49, 82, 74, 83, 77, 84, 85, 79, 92, 93].\\nWe also put those works under this category as they do not explicitly assume that communication is limited by cost.\\nConstrained Communication\\nIn this category, communication processes are explicitly constrained by cost or noise.\\nThus, agents need to utilize communication resources efficiently to promote learning. We further identify two practical\\nconcerns that have been considered in the literature.\\nâ€¢ Limited Bandwidth. In this category, communication bandwidth is limited by channel capacity. Thus, communi-\\ncation needs to be used more efficiently, both in the number of times that agents can communicate and the size of\\ncommunicated information. Early works focus on transmitting succinct messages to avoid communication overhead.\\nRIAL and DIAL [73] are proposed to communicate very little information (i.e., a binary value or a real number)\\nat every time step to reduce the bandwidth needed. MD-MADDPG [42] considers a fixed-size memory, which\\nis shared by all agents. Agents communicate through the shared memory instead of ad hoc channels. VBC [31]\\nand TMC [32] reduce communication costs by using predefined thresholds to filter unnecessary communication,\\nand both show lower communication overhead. NDQ [75] cuts 80% of messages by ordering the distributions of\\nmessages according to their means and drops accordingly to prevent meaningless messages. MAIC [33] also cuts\\nmessages by examining several message pruning rates. In MAIC, messages are encoded to consider their respective\\nimportance. Sent messages are ordered and then pruned with a given pruning rate. IMAC [66] explicitly models\\nbandwidth limitation as a constraint to optimization. An upper bound of the mutual information between messages\\nand observations is derived according to bandwidth constraint, which turns out to minimize the entropy of messages.\\nThen agents learn not only to maximize cumulative rewards but also to generate low-entropy messages. The number\\nof agents to communicate can also be restricted to reduce the total amount of communication. SchedNet [43] con-\\nsiders a scenario of a shared channel together with limited bandwidth. Only a subset of agents are chosen to convey\\ntheir messages according to their importance. Gated-ACML [76] learns a probabilistic gate unit to block messages\\ntransmitting between each agent and a centralized message coordinator, with the extra cost of learning optimal gates.\\nInspired by Gated-ACML and IMAC, ETCNet [86] puts constraints on the behaviors of deciding whether to send\\n10\\nmessages or not. A penalty term is added to the environment rewards, and an additional reinforcement learning\\nalgorithm is used to optimize the sending behaviors. Variable-length Coding [87] also utilizes a penalty term while\\nencouraging short messages. When learning tasks with emergent language, symbolic languages are acquired for\\ncommunication through a limited number of tokens. Therefore, we classify those works under limited bandwidth\\n[61, 62, 63, 65].\\nâ€¢ Corrupted Messages. In this category, messages transmitted among agents can be corrupted due to environmental\\nnoise or malicious intentions. DIAL [73] shows that during training, adding Gaussian noise to the communication\\nchannel can push the distribution of messages into two modes to convey different types of information. Diff Discrete\\n[78] considers how to backpropagate gradients through a discrete communication channel (between 2 agents) with\\nunknown noise. An encoder/channel/decoder system is modeled, where the encoder is used to discretize a real-\\nvalued signal into a discrete message to pass through the discrete communication channel, and the decoder is used\\nto compute an approximation of the original signal. Later they show that the encoder/channel/decoder system is\\nequivalent to an analog communication channel with additive noise. With the additional assumption that training\\nis centralized, the gradient of the receiver with respect to real-value messages from the sender can be computed\\nto allow backpropagation. DCSS [64] also considers a noisy setting. They prove that representing messages as\\none-hot vectors may not be optimal when the environment becomes noisy. Inspired by word embedding in the\\nNLP field, they propose to generate a semantic representation of discrete tokens that are communicated among\\nagents. The results show that such representation is robust in noisy environments and benefits human understanding\\nof communication. Different from noisy environments, R-MACRL [95] assumes that an agent holds a malicious\\nmessaging policy, producing adversarial messages that can mislead other agentsâ€™ action selections. Therefore, other\\nagents need to prevent being exploited by learning a defense policy in order to filter the messages.\\n3.3\\nCommunicatee Type\\nCommunicatee Type determines which type of agents are assumed to receive messages in a Comm-MADRL system.\\nWe found that in the literature, communicatee type can be classified into the following categories based on whether\\nagents in the environment communicate with each other directly or not.\\nAgents in the MAS\\nIn this category, the set of communicatees consists of agents in the environment, and they di-\\nrectly communicate with each other. Nevertheless, due to partial observability, agents may not be able to communicate\\nwith every agent in the MAS, and thus we further distinguish the types of communicatees as follows:\\nâ€¢ Nearby Agents. In many Comm-MADRL systems, communication is only allowed between neighbors. Nearby\\nagents can be defined as observable agents [80], agents within a certain distance [81, 74, 77] or neighboring agents\\non a graph [84]. GAXNet [80] labels observable agents and enables communication between them. DGN [81] limits\\ncommunication within 3 closest neighbors while using a distance metric to find them. Agent-Entity Graph [74] also\\nuses distance to measure whether agents are nearby or not. As long as two agents are close to each other, they will\\nbe allowed to communicate. LSC [77] enables agents within a cluster radius to decide whether to become a leader\\nagent. Then all non-leader agents in the same cluster will only communicate with the leader agent. NeurComm [84]\\nand IP [85] preset a graph structure among agents built upon networked multi-agent systems. In both NeurComm\\nand IP, communicatees are restricted to neighbors on the graph. MAGNet-SA-GS-MG [40] uses a pre-trained graph\\nto limit communication and restricts communication on neighboring agents. Neighboring agents can also emerge\\nduring learning instead of being predetermined, as proposed in GA-Comm [83], MAGIC [92] and FlowComm [93],\\nwhich explicitly learn a graph structure among agents. Specifically, in GA-Comm [83] and MAGIC [92], a central\\nunit (e.g., GNN) learns a graph inside and coordinates messages based on the (complete) graph simultaneously. In\\nthis case, agents do not communicate with each other directly; instead, they communicate through a virtual agent\\nwho does not affect the environment. Therefore, we categorize these two works into the proxy category.\\nâ€¢ Other (Learning) Agents. If nearby agents are not identified, the set of communicatees typically consists of other\\n(learning) agents. Specifically, IC3Net [49] enables communication between learning agents and their opponents.\\nExperiments indicate that these opponents eventually learn to not communicate to avoid being exploited. Some\\nworks assume explicit role assignments, i.e., senders and receivers. The role of the receiver can be taken by a\\ndisjoint set of agents separate from the senders [62, 63, 64] or by all other agents in the environment [61, 65]. In\\nboth cases, agents communicate with each other directly.\\nProxy\\nA proxy is a virtual agent that plays an essential role (e.g., as a medium) in facilitating communication but\\ndoes not directly affect the environment. Using a proxy as the communicatee means that agents will not directly\\ncommunicate with each other, instead viewing the proxy as a medium, coordinating and transforming messages for\\nspecific purposes. MS-MARL-GCM [88] utilizes a master agent that collects local observations and hidden states\\nfrom agents in the environment and sends a common message back to each of them. Similarly, HAMMER [91]\\nemploys a central proxy that gathers local observations from agents and sends a private message to each agent.\\n11\\nTable 4: The category of communicatee type.\\nTypes\\nSubtypes\\nMethods\\nAgents in the MAS\\nNearby Agents\\nDGN [81]; MAGNet-SA-GS-MG [40]; Agent-Entity Graph [74];\\nLSC [77]; NeurComm [84]; IP [85]; FlowComm [93]; GAXNet\\n[80];\\nOther Agents\\nDIAL [73]; RIAL [73]; CommNet [48]; GCL [61]; BiCNet [50];\\nIC [62]; TarMAC [89]; MADDPG-M [41]; IC3Net [49]; Sched-\\nNet [43]; DCC-MD [82]; VBC [31]; NDQ [75]; Bias [63]; Diff\\nDiscrete[78]; I2C [79]; IS [90]; ETCNet [86]; Variable-length\\nCoding [87]; TMC [32]; AE-Comm [65]; DCSS [64]; R-MACRL\\n[95]; MAIC [33]; FCMNet [94];\\nProxy\\nMS-MARL-GCM [88]; ATOC [39]; MD-MADDPG [42]; IMAC\\n[66]; GA-Comm [83]; Gated-ACML [76]; HAMMER [91];\\nMAGIC [92];\\nAgent 1\\nAgent 3\\nProxy\\nAgent 4\\nobservable field\\nobservable field\\nAgent 2\\nAgent 5\\nFigure 2: Three communicatee types in the same system.\\nMD-MADDPG [42] maintains a shared memory among agents, learning to selectively store and retrieve local\\nobservations from the memory. IMAC [66] defines a scheduler that aggregates encoded information from all agents\\nand sends individual messages to each agent. These works primarily focus on how to encode messages through\\nthe proxy without determining whether to send or receive messages. By contrast, ATOC [39], Gated-ACML [103],\\nGA-Comm [83] and MAGIC [92] are all designed for agents to decide whether to communicate with a message\\ncoordinator. In ATOC and Gated-ACML, each agentâ€™s decisions are made locally based on individual observations,\\nwith messages aggregated from nearby agents and from the entire MAS, respectively. Both GA-Comm and MAGIC\\ndevelop a global communication graph, coupled with a graph neural network (GNN) to aggregate messages by\\nweights and send new messages back to each agent, informing action selection in the environment.\\nTable 4 summarizes recent works on communication types in MAS. To illustrate these categories, we present an\\nexample of different communication methods used in a Comm-MADRL system in Figure 2. The system consists of\\nfive agents and one proxy. Agent 3 is the nearby agent of Agent 1, while Agent 4 is the nearby agent of Agent 2. Agent\\n5 is out of the view range of Agents 1 and 2. If communication is limited to nearby agents, Agent 1 will communicate\\nonly with Agent 3, and Agent 2 will communicate only with Agent 4. However, if communication involves a proxy,\\nall agents can send their messages to the proxy and receive coordinated messages.\\n3.4\\nCommunication Policy\\nCommunication Policy determines when and with which agents (i.e., communicatees) to communicate in order to\\nenable message transmission. A Communication Policy defines a set of communication actions, which can be modeled\\nin different ways. For example, a communication action can be represented as a vector of binary values, where each\\nvalue indicates whether communication with one of the other agents is allowed at a certain time step. These actions\\nform communication links between pairs of agents, which can be represented as a communication graph among agents.\\nIn the literature, communication policies can be either predefined or learned, allowing communication with all other\\nagents or only a subset of agents. Furthermore, communication policies can be centralized, controlling communication\\namong all agents, or decentralized, enabling individual agents to control whether to communicate. Therefore, we first\\ncategorize the literature based on whether communication policies are predefined or learned. We find that in predefined\\ncommunication policies, the literature often uses either full communication among agents, where the communication\\n12\\nTable 5: The category of communication policy\\nTypes\\nSubtypes\\nMethods\\nPredefined\\nFull Communication\\nDIAL [73]; RIAL [73]; CommNet [48]; GCL [61]; BiCNet\\n[50]; MS-MARL-GCM [88]; TarMAC [89]; MD-MADDPG\\n[42]; DCC-MD [82]; IMAC [66]; Diff Discrete[78]; IS [90];\\nVariable-length Coding [87]; HAMMER [91]; AE-Comm [65];\\nR-MACRL [95]; FCMNet [94];\\nPartial Structure\\nIC [62]; DGN [81]; MAGNet-SA-GS-MG [40]; Agent-Entity\\nGraph [74]; VBC [31]; NDQ [75]; Bias [63]; NeurComm [84];\\nIP [85]; TMC [32]; GAXNet [80]; DCSS [64]; MAIC [33];\\nLearnable\\nIndividual Control\\nATOC [39]; MADDPG-M [41]; IC3Net [49]; Gated-ACML\\n[76]; LSC [77]; I2C [79]; ETCNet [86];\\nGlobal Control\\nSchedNet [43]; GA-Comm [83]; MAGIC [92]; FlowComm\\n[93];\\n...\\nA1\\n...\\nA2\\nAn\\nP\\nP\\nA1\\nA2\\nAn\\n...\\nA1\\n...\\nA2\\nAn\\nP\\nP\\nA1\\nA2\\nAn\\n...\\nA1\\n...\\nA2\\nAn\\nP\\nP\\nA1\\nA2\\nAn\\nComm\\nPolicy 1\\nComm\\nPolicy 2\\nComm\\nPolicy n\\nComm\\nPolicy\\n...\\nA1\\n...\\nA2\\nAn\\nP\\nP\\nA1\\nA2\\nAn\\nFigure 3: Four types of communication policy with agents (shown as A) in the environment and a possible proxy\\n(shown as P).\\ngraph becomes complete, or a partial graph structure to incorporate constraints on communication policies. On the\\nother hand, in learnable communication policies, we identify two distinct categories: individual control and global\\ncontrol. In individual control, communication policies are learned by each agent independently, whereas in global\\ncontrol, these policies are learned and implemented centrally, applying to all agents in Comm-MADRL systems. As\\na result, we have identified four subcategories within the dimension of communication policy: Full Communication,\\n(Predefined) Partial Structure, Individual Control, and Global Control. These categorizations are summarized in Table\\n5.\\nWe present examples of how agents form communication links in the four categories of communication policy, as\\nillustrated in Figure 3. Both Full Communication and Partial Structure rely on a predefined communication policy to\\ndetermine communication actions. In contrast, Individual Control and Global Control involve the learning of a local\\ncommunication policy and a global communication policy, respectively, to establish communication links between\\nagents or a potential proxy. If a proxy is involved, it coordinates messages from agents choosing to communicate\\nthrough this proxy. The categories and their associated research works are introduced as follows:\\nFull Communication\\nIn this category, every pair of agents is connected so that messages are transmitted in a broad-\\ncast manner. Full communication can be regarded as a fully connected graph, often used in early works on Comm-\\nMADRL. DIAL [73], RIAL [73], CommNet [48], and BiCNet [50] learn a communication protocol which connect\\nall agents together. Inspired by BiCNet, FCMNet [94] uses multiple RNNs to link all agents together with different\\nsequences, allowing agents to benefit from communication flow from various directions. In contrast, Diff Discrete\\n[78] and Variable-length Coding [87] focus on two-agent cases but do not learn to block messages from each other.\\nTarMAC [89] and IS [90] learn meaningful messages while using a broadcast way to share messages, thus still adher-\\ning to full communication. DCC-MD [82] and R-MACRL [95] introduce a strategy to drop out received messages\\nwithout specifying whether to send messages. Specifically, DCC-MD drops out messages with a fixed probability to\\nreduce input dimensions, and R-MACRL learns to drop out adversary messages through a defense policy. In Comm-\\nMADRL methods like IMAC [66], MS-MARL-GCM [88] and HAMMER [91], a central proxy that receives local\\nobservations or encoded messages is always connected with agents in the MAS. In addition, GCL [61] and AE-Comm\\n13\\n[65] learn a language grounded on discrete tokens among agents, where all agents have the capability to send and\\nreceive messages.\\n(Predefined) Partial Structure\\nIn this category, the communication between agents is captured by a predetermined\\npartial graph to reduce overall communication. Then, each agent communicates with a limited number of agents within\\nthe MAS, rather than with every agent. NeurComm [84] and IP [85] operate in a networked multi-agent environment,\\nrandomly generating a communication network while maintaining a fixed average number of connections per agent\\nduring the learning process. DGN [81], MAGNet-SA-GS-MG [40], and GAXNet [80] restrict communication to a\\ncertain proximity of agents. The Agent-Entity Graph [74] employs a pre-trained graph to capture agent relationships.\\nComm-MADRL approaches like VBC [31], NDQ [75], TMC [32], and MAIC [33] utilize handcrafted thresholds or\\npruning rates to limit communication opportunities. In IC [62], Bias [63], and DCSS [64], disjoint sets of agents are\\ndesignated as either senders or receivers, facilitating unidirectional communication from senders to receivers only.\\nIndividual Control\\nIn this category, each agent actively and individually determines whether to communicate with\\nother agents, implicitly forming a graph structure. A common method employed in Comm-MADRL studies within\\nthis category is a learnable gate mechanism, which aids agents in making the decision to communicate. For instance,\\nIC3Net [49] and ATOC [39] use a gate mechanism that enables agents to decide whether to broadcast their messages,\\nin a deterministic and probabilistic manner, respectively. ETCNet [86] also implements a gate unit but limits the\\noverall probability of message-sending behaviors. If a proxy, such as a message coordinator, is present, Gated-ACML\\n[76] introduces a learning mechanism for each agent to decide whether to communicate with the proxy, as opposed\\nto direct communication with other agents. Diverging from the gate function approach, I2C [79] allows each agent\\nto unilaterally decide on communication with other agents, based on evaluating the impact of those agents on its\\nown policy. LSC [77] allows each group of agents, defined by a specific radius, to compare their weights in order\\nto elect a leader. This system then facilitates communication from each group to their respective leaders and from\\nleader to leader. Notably, the leader agent in this model is not considered a proxy, as it still directly interacts with the\\nenvironment.\\nGlobal Control\\nIn this category, a globally shared communication policy is learned, providing more complete con-\\ntrol over the communication links between agents. SchedNet [43] employs a global scheduler that limits the number\\nof agents allowed to broadcast their messages, thereby reducing overall communication. FlowComm [93] learns a di-\\nrected graph among agents, enabling unilateral or bilateral communication between them. Similarly, GA-Comm [83]\\nand MAGIC [92] develop an undirected and a directed graph for communication, respectively. These Comm-MADRL\\nsystems incorporate an additional message coordinator to coordinate and transform messages sent by the agents.\\n3.5\\nCommunicated Messages\\nAfter establishing communication links among agents through a communication policy, agents should determine which\\nspecific information to communicate. This information can derive from historical experiences, intended actions, or\\nfuture plans, enriching the messages with valuable insights. Consequently, the communicated information can expand\\nthe agentsâ€™ understanding of the environment and enhance the coordination of their behaviors. In the dimension of\\ncommunicated messages, an important consideration is whether the communication includes future information, such\\nas intentions and plans. This kind of information, being inherently private, often requires an (estimated) model of the\\nenvironment to effectively simulate and generate conjectured intentions and plans. Accordingly, we categorize recent\\nstudies in this dimension into two categories, as summarized in Table 6.\\nExisting Knowledge\\nIn this category, agents share their knowledge of the environment (e.g., past observations),\\nprevious movements, or policies to assist other agents in selecting actions. As historical information accumulates,\\nagents use a low-dimensional encoding of their knowledge as messages to reduce communication overhead. Notably,\\nthe RNN family (e.g., LSTM and GRU) is commonly used as an encoding function, capable of selectively retaining and\\nforgetting historical observations [48, 50, 88, 89, 49, 42, 83, 79, 92, 93, 33, 94], action-observation histories [73, 50],\\nor action-observation-message histories [62, 63]. When a proxy is present, messages are generated and transformed\\nfrom agents to the proxy, and then from the proxy to agents. Thus, local observations can either be encoded [42,\\n66, 83, 76, 92] or directly sent [88, 91] to the proxy. The proxy, after gathering these local (encoded) observations,\\ncan generate a unified message for all agents [88], or individualized messages for each agent [42, 66, 83, 76, 91, 92].\\nBoth methods provide a message containing global information, relieving agents from the task of combining multiple\\nreceived messages. In Comm-MADRL systems without a proxy, messages are sent directly to each agent. Specifically,\\nin MADDPG-M [41], agents communicate local observations without an encoding of them. On the other hand,\\nDIAL and RIAL [73] encode past observations, actions, and current observations as messages. BiCNet [50] encodes\\nboth local observations of each agent and a global view of the environment. Other research works employ various\\nmethods such as simple feed-forward networks [43, 78, 86, 87], MLP [40, 31, 32, 95], autoencoders [82], CNNs [81],\\nRNNs [48, 89, 49, 79, 93, 33, 94], or GNNs [74, 77] to encode local observations as messages. Furthermore, agents\\ncan communicate more specific information, such as in GAXNet [80], where agents coordinate their local attention\\n14\\nTable 6: The category of communicated messages.\\nTypes\\nMethods\\nExisting Knowledge\\nDIAL [73]; RIAL [73]; CommNet [48]; GCL [61]; BiCNet [50]; MS-MARL-\\nGCM [88]; IC [62]; DGN [81]; TarMAC [89]; MAGNet-SA-GS-MG [40];\\nMADDPG-M [41]; IC3Net [49]; MD-MADDPG [42]; SchedNet [43]; DCC-MD\\n[82]; Agent-Entity Graph [74]; VBC [31]; NDQ [75]; IMAC [66]; GA-Comm\\n[83]; Gated-ACML [76]; Bias [63]; LSC [77]; Diff Discrete[78]; I2C [79]; ETC-\\nNet [86]; Variable-length Coding [87]; TMC [32]; HAMMER [91]; MAGIC [92];\\nFlowComm [93]; AE-Comm [65]; GAXNet [80]; DCSS [64]; R-MACRL [95];\\nMAIC [33]; FCMNet [94];\\nImagined Future Knowledge\\nATOC [39]; NeurComm [84]; IP [85]; IS [90];\\nTable 7: The category of message combination.\\nTypes\\nMethods\\nEqually Valued\\nDIAL [73]; RIAL [73]; CommNet [48]; GCL [61]; IC [62]; MADDPG-M [41];\\nIC3Net [49]; SchedNet [43]; VBC [31]; NDQ [75]; Bias [63]; Diff Discrete[78];\\nIS [90]; ETCNet [86]; Variable-length Coding [87]; FlowComm [93]; AE-Comm\\n[65]; DCSS [64];\\nUnequally Valued\\nBiCNet [50]; MS-MARL-GCM [88]; ATOC [39]; DGN [81]; TarMAC [89];\\nMAGNet-SA-GS-MG [40]; MD-MADDPG [42]; DCC-MD [82]; Agent-Entity\\nGraph [74]; IMAC [66]; GA-Comm [83]; Gated-ACML [76]; LSC [77]; Neur-\\nComm [84]; IP [85]; I2C [79]; TMC [32]; HAMMER [91]; MAGIC [92]; GAXNet\\n[80]; R-MACRL [95]; MAIC [33]; FCMNet [94];\\nweights, integrating hidden states from neighboring agents. Messages can also be modeled as random variables, as\\nseen in NDQ [75], where messages are drawn from a multivariate Gaussian distribution to maximize expressiveness\\nby maximizing mutual information between messages and receiversâ€™ action selection. In learning tasks with emergent\\nlanguage, agents often communicate goal-related information, such as the goalâ€™s location [61, 62, 63, 65, 64].\\nImagined Future Knowledge\\nIn this context, Imagined Future Knowledge refers to aspects such as intended actions\\n[39], policy fingerprints (i.e., action probabilities in a given state) [85, 84], or future plans [90]. Since intentions\\nare related to the current environment state, recent works often combine intended actions with local observations to\\nproduce more relevant messages. The concept of future plans extends this idea further by utilizing an approximated\\nmodel of the environment and the behavior models of other agents. This approach enables the generation of a sequence\\nof possible future observations and actions [90]. Such knowledge is shared among agents, allowing the receivers to\\nconsider the potential future outcomes of the sendersâ€™ actions.\\n3.6\\nMessage Combination\\nWhen agents receive more than one message, current works often aggregate all received messages to reduce the\\ninput for the action policy. Message Combination determines how to integrate multiple messages before they are\\nprocessed by an agentâ€™s internal model. If a proxy is involved, each agent receives already coordinated and combined\\nmessages from the proxy, eliminating the need for further message combination. If no proxy is presented, each agent\\nindependently determines how to combine multiple messages. Since communicated messages encode the sendersâ€™\\nunderstanding of the learning process or the environment, some messages can be more valuable than others. As shown\\nin Table 7, recent works in the dimension of message combination are categorized based on how agents prioritize\\nreceived messages.\\nEqually Valued\\nIn this category, messages received by agents are treated without preference, meaning they are\\nassigned equal weights or simply no weights at all. Without having preferences, agents can concatenate all messages,\\nensuring no loss of information, though it may significantly expand the input space for the action policy [73, 61, 62,\\n41, 43, 75, 78, 90, 86, 87]. Recent research involving concatenated messages typically represent the sent messages\\neither as single values [73, 62, 87, 86] or as short vectors [61, 41, 43, 75, 78, 90]. Alternatively, messages can be\\ncombined by averaging [48, 49, 31] or summing [93], under the assumption that messages from different agents\\nhave the same dimension. In some cases, particularly in two-agent scenarios, no explicit preferences are assigned to\\nmessages [63, 65, 64].\\n15\\nTable 8: The category of inner integration.\\nTypes\\nMethods\\nPolicy-level\\nCommNet [48]; GCL [61]; MS-MARL-GCM [88]; ATOC [39]; MAGNet-SA-\\nGS-MG [40]; IC3Net [49]; MD-MADDPG [42]; SchedNet [43]; IMAC [66]; GA-\\nComm [83]; Gated-ACML [76]; Diff Discrete[78]; IP [85]; I2C [79]; IS [90];\\nETCNet [86]; Variable-length Coding [87]; HAMMER [91]; FlowComm [93];\\nGAXNet [80]; R-MACRL [95];\\nValue-level\\nDIAL [73]; RIAL [73]; DGN [81]; DCC-MD [82]; VBC [31]; NDQ [75]; LSC\\n[77]; TMC [32]; MAIC [33];\\nPolicy- and Value-level\\nBiCNet [50]; IC [62]; TarMAC [89]; MADDPG-M [41]; Agent-Entity Graph [74];\\nBias [63]; NeurComm [84]; MAGIC [92]; AE-Comm [65]; DCSS [64]; FCMNet\\n[94];\\nUnequally Valued\\nIn this category, messages are assigned distinct preferences, which potentially impose differences\\non sender agents. DCC-MD [82] and TMC [32] use handcrafted rules to prune received messages. In DCC-MD, each\\nreceived message can be dropped out with a certain probability. TMC stores the received messages and checks whether\\nthey are expired or not within a preset time window. Only valid messages are integrated into an agentâ€™s model. Instead\\nof using fixed rules, R-MACRL [95] learns a gate unit to decide whether to use a received message. An attention\\nmechanism can also be learned to assign weights to received messages and then combine them, rather than filtering\\nmessages out, as seen in research works [89, 40, 74, 33]. Moreover, a neural network can aggregate received messages\\ninto a single message or a low-dimensional vector, which implicitly imposes preferences on messages during the\\nmapping. Feedforward neural networks [66, 76, 91], CNNs [81], LSTMs (or RNNs) [50, 88, 39, 42, 84, 79, 80, 94],\\nand GNNs [83, 77, 85, 92] have been used as aggregators. Among them, GNNs utilize a learned graph structure of\\nagents and assign different weights to neighboring agents.\\n3.7\\nInner Integration\\nInner Integration determines how to integrate (combined) messages into an agentâ€™s learning model, such as a policy\\nor a value function. In most existing literature, messages are viewed as additional observations. Agents take messages\\nas extra input to a policy function, a value function, or both. Thus, in the dimension of inner integration, we classify\\nrecent works into categories based on the learning model that is used to integrate messages. These categories are\\nsummarized in Table 8.\\nPolicy-level\\nBy exploiting information from other agents, each agent will no longer act independently. Policies can\\nbe learned through policy gradient methods like REINFORCE, as seen in studies [48, 61, 88, 49, 83], which collect\\nrewards during episodes and train the policy models at the end of episodes. Moreover, the Comm-MADRL approaches\\nthat utilize actor-critic methods [39, 40, 42, 43, 66, 76, 78, 85, 79, 90, 86, 87, 91, 93, 80, 95] assume that a critic model\\n(i.e., a Q-function) guides the learning of an actor model (i.e., a policy network).\\nValue-level\\nIn this category, a value function incorporates messages as input, and a policy is derived by selecting the\\naction with the highest Q-value. Most works in this category employ DQN-like methods to train their value functions\\n[73, 81, 82, 31, 75, 77, 32, 33]. Specifically, Comm-MADRL approaches like VBC [31], NDQ [75], TMC [32], and\\nMAIC [33] are based on value decomposition methods in cooperative scenarios (with global rewards). These methods\\ninvolve learning to decompose a joint Q-function.\\nPolicy- and Value-level\\nIntegrating messages using both a policy function and a value function typically relies\\non actor-critic methods. In Comm-MADRL approaches within this category, received messages can be treated as\\nextra inputs for both the actor and critic models [50, 74, 64]. Alternatively, messages can be combined with local\\nobservations to generate new internal states, which are then shared with both the actor and critic models [62, 89, 41,\\n63, 84, 92, 65, 94].\\n3.8\\nLearning Methods\\nLearning methods determine which type of machine learning techniques is used to learn a communication protocol.\\nThe learning of communication is at the center of modern Comm-MADRL and can benefit from the advancements\\nin the machine learning field. If proper assumptions about communication are made, such as being able to calculate\\nthe derivatives with respect to the message generator function and the communication policy, then the training of\\ncommunication can be integrated into the overall learning process of agents. This integration allows for the use of fully\\ndifferentiable methods for backpropagation. Other machine learning techniques, including reinforcement learning,\\n16\\nTable 9: The assumptions behind different learning methods.\\nTypes\\nAssumptions\\nFully differentiable\\nThe messages or the communication actions are generated by a differ-\\nentiable function and thus backpropagation is used everywhere.\\nSupervised learning\\nTrue labels (or the ground truth) are assumed to be given or defined to\\nguide the learning of communication policy or messages.\\nReinforcement learning\\nEnvironment rewards or self-defined rewards are used to update com-\\nmunication policy or messages incrementally.\\nRegularizers\\nRegularizations such as entropy inspired from information theory are\\nadded to agentsâ€™ optimization objectives to regularize the learning of\\ncommunication.\\nTable 10: The category of learning methods.\\nTypes\\nMethods\\nDifferentiable\\nGCL [61]; DIAL [73]; CommNet [48]; BiCNet [50]; MS-MARL-GCM [88]; DGN\\n[81]; TarMAC [89]; MAGNet-SA-GS-MG [40]; MD-MADDPG [42]; DCC-MD\\n[82]; Agent-Entity Graph [74]; VBC [31]; GA-Comm [83]; Diff Discrete[78];\\nNeurComm [84]; IP [85]; IS [90]; Variable-length Coding [87]; TMC [32];\\nMAGIC [92]; FlowComm [93]; GAXNet [80]; DCSS [64]; FCMNet [94];\\nSupervised\\nDCSS [64]; ATOC [39]; Gated-ACML [76]; I2C [79]; R-MACRL [95];\\nReinforced\\nGCL [61]; RIAL [73]; IC [62]; MADDPG-M [41]; IC3Net [49]; SchedNet [43];\\nLSC [77]; ETCNet [86]; HAMMER [91];\\nRegularized\\nNDQ [75]; IMAC [66]; Bias [63]; AE-Comm [65]; MAIC [33];\\nsupervised learning, and regularizations, can also be utilized to incorporate our requirements and available ground\\ntruth into the learning of communication, each carrying respective assumptions. The assumptions used in the literature\\nare summarized in Table 9. For instance, supervised methods require defining true labels for communication (e.g., the\\ncorrect information to share or the right agents to communicate with). In contrast, reinforced methods use rewards\\nas learning signals. Regularized methods, which use neither true labels nor rewards, employ an additional learning\\nobjective by using regularizers, such as minimizing the entropy of messages to reduce stochasticity. Therefore, we\\nclassify recent works based on how they differ in the learning of communication (summarized in Table 10).\\nDifferentiable\\nIn this category, communication is learned and improved by backpropagating gradients from agent to\\nagent. When the communication policy is predefined, such as in full communication [73, 48, 61, 50, 88, 89, 42, 82, 78,\\n90, 87, 94] or by communicating with a subset of agents [81, 40, 74, 31, 84, 85, 32, 80, 64], agents learn the content of\\nmessages through backpropagation. Several recent studies [61, 83, 92, 93, 64] address the issue of non-differentiable\\ncommunication actions by utilizing gradient estimators like Gumbel-softmax [104], which replaces non-differentiable\\nsamples with a differentiable approximation during training, albeit requiring additional parameter tuning. Specifically,\\nboth GCL [61] and DCSS [64] employ a differential message function in their approaches. Additionally, GCL inte-\\ngrates auxiliary rewards, and DCSS utilizes labeled messages for training communication policies. Thus, they are cat-\\negorized under the Differentiable category, each additionally aligning with the Reinforced and Supervised categories\\nrespectively. Freed et al. [78] propose an alternative method, Diff Discrete, to address the challenge of continuous\\nmessages versus discrete channels. This method models message transmitting as an encoder/channel/decoder system,\\nwhere the receiver decodes the messages and reconstructs the original signals. These reconstructed signals enable the\\ncalculation of derivatives with respect to the sender, allowing gradients to be sent back to the sender.\\nSupervised\\nIn this category, additional efforts need to be made to define the true label for when and what informa-\\ntion to communicate. ATOC [39] and Gated-ACML [76] use the difference in Q-values between actions chosen with\\nand without a message to define a label of communication actions. If the difference exceeds a threshold, the message is\\n17\\ndeemed valuable, indicating a high probability of sending it; otherwise, the probability is 0. This process sets up a clas-\\nsification task to decide whether to communicate. Similarly, I2C [79] trains a classifier to determine communication\\nbut relies on the causal effect between two agents, using a threshold to tag effective communication. R-MACRL [95]\\nlearns a classifier to identify malicious messages, using the status of a message (malicious or not) as a label. DCSS\\n[64] learns message content by using a small dataset that maps observations to desired communication symbols. In\\nDCSS, the gradient from the supervised loss is added to the policy loss, leading agents to use communication that\\naligns with the grounding data and enables high task performance.\\nReinforced\\nIn this category, reinforcement learning is utilized to train communication in addition to the learning\\nof action policies. RIAL [73] and HAMMER [91] focus on learning the content of messages through reinforcement\\nlearning, without addressing the decision of whether to communicate. In GCL [61], auxiliary rewards are used for\\npredicting goals and consolidating symbols, facilitating the development of a compositional language for communi-\\ncation. IC [62] employs the difference in outcomes from using and not using communication on action policies as\\nrewards. Maximizing the rewards can enhance the influence of communication on the receiversâ€™ action policies. Other\\nstudies [41, 49, 43, 77, 86] consider both the learning of communication content and the decision to communicate. No-\\ntably, MADDPG-M [41] suggests using intrinsic rewards to train the communication policy instead of relying solely\\non environmental rewards. ETCNet [86] shapes environmental rewards by introducing a penalty term to discourage\\nunnecessary communication.\\nRegularized\\nRegularized methods are used to reduce redundant information in communication [75, 66, 33]. NDQ\\n[75] calculates a lower bound of the mutual information between received messages and the receiversâ€™ action selection.\\nThis approach suggests that messages can be optimized to decrease the uncertainty in the action-value functions of the\\nreceivers. IMAC [66] establishes an upper bound on the mutual information between messages and the sendersâ€™ obser-\\nvations, and minimizing this upper bound helps agents send messages with lower uncertainty. MAIC [33] employs an\\nestimated model of teammates and aims to maximize the mutual information between teammatesâ€™ actions and hidden\\nvariables from this model. This model then guides the encoding of messages, resulting in tailored communications\\nfor different agents. Bias [63] focuses on the long-term impact of messages on agentsâ€™ decision-making to enhance\\nsignaling and listening effectiveness. AE-Comm [65] adopts an autoencoder to learn a low-dimensional encoding of\\nobservations.\\n3.9\\nTraining Schemes\\nThis dimension focuses on how to utilize the collected experiences (such as observations, actions, rewards, and mes-\\nsages) of agents to train their action policies and communication architectures in a Comm-MADRL system. Agents\\ncan train their models in a fully decentralized manner using only their local experience. Alternatively, when global\\ninformation is accessible, the experiences of all agents can be collected to centrally train a single (centralized) model\\nthat controls all agents. However, each approach has inherent challenges. Fully decentralized learning must cope with\\na non-stationary environment due to the changing and adapting behaviors of agents, while fully centralized learning\\nfaces the complexities of joint observation and policy spaces. As a balanced solution, Centralized Training and Decen-\\ntralized Execution (CTDE) [105, 73] has emerged as a popular training schemes in MADRL. CTDE approaches allow\\nagents to learn their local policies using guidance from central information. Therefore, in the dimension of training\\nschemes, we categorize recent works based on how agentsâ€™ experiences are collected and utilized, as detailed in Table\\n11.\\nCentralized Learning\\nAs shown in Figure 4a, experiences are gathered into a central unit, then learning to control\\nall agents. Based on our observations, recent works on Comm-MADRL usually do not assume a central controller.\\nFully Decentralized Learning\\nAs illustrated in Figure 4b, in fully decentralized learning, experiences are collected\\nindividually by each agent, and they undergo independent training processes. Recent works in this category often\\nemploy actor-critic based methods for each agent [40, 41, 82, 74, 84, 85, 95]. Specifically, decentralized learning has\\ngained much attention in learning tasks with emergent language, as it most closely resembles language learning in\\nnature [62, 63, 65].\\nCentralized Training and Decentralized Execution\\nIn CTDE approaches, the experiences of all agents are col-\\nlectively used for optimization. Gradients derived from the joint experiences of agents guide the learning of local\\npolicies. However, once training is complete, only the policies are needed and gradients can be discarded, facilitating\\ndecentralized execution. When agents are assumed to be homogeneous, meaning they have identical sensory inputs,\\nactuators, and model structures, they can share parameters. Parameters sharing reduces the overall number of parame-\\nters, potentially enhancing learning efficiency compared to training in separate processes. Despite sharing parameters,\\nagents can still exhibit distinct behaviors because they are likely to receive different observations at the same time step.\\nBased on these considerations, recent works in this field can be further divided into the following subcategories.\\n18\\nPolicy\\xa01\\nPolicy\\xa02\\nPolicy n\\n...\\n<a1, a2, ..., an>\\nUpdate with <r1, r2, ..., rn>\\n<o1, o2, ..., on>\\nEnvironment\\n(a) Fully Centralized Learning\\nEnvironment\\n...\\nPolicy n\\nUpdate with rn\\nan\\non\\nPolicy 1\\nUpdate with r1\\na1\\no1\\nPolicy 2\\nUpdate with r2\\na2\\no2\\n(b) Fully Decentralized Learning\\nEnvironment\\nPolicy\\xa01\\na1\\no1\\n...\\nCentral Unit\\nUpdate\\nGlobal Information\\xa0\\ne.g., rewards\\nPolicy 2\\na2\\no2\\nPolicy n\\nan\\non\\nUpdate\\nUpdate\\n(c) Individual Parameters\\nEnvironment\\nPolicy\\xa01\\na1\\no1\\n...\\nCentral Unit\\nUpdate\\nGlobal Information\\xa0\\ne.g., rewards\\nPolicy 2\\na2\\no2\\nPolicy n\\nan\\non\\n(d) Parameter Sharing\\nEnvironment\\nPolicy\\xa01\\na1\\no1\\nPolicy\\xa02\\n...\\na2\\no2\\nan\\non\\nPolicy n\\nCentral Unit 1\\nCentral Unit 2\\nCentral Unit n\\nUpdate\\nUpdate\\nUpdate\\nGlobal Information (e.g., rewards)\\n(e) Concurrent\\nFigure 4: Five types of Training Schemes.\\n19\\nTable 11: The category of training schemes.\\nTypes\\nSubtypes\\nMethods\\nFully\\nDecentralized\\nLearning\\nIC [62]; MAGNet-SA-GS-MG [40]; MADDPG-M [41]; DCC-\\nMD [82]; Agent-Entity Graph [74]; Bias [63]; NeurComm [84];\\nIP [85]; AE-Comm [65]; R-MACRL [95];\\nCentralized\\nTraining\\nand\\nDecentralized\\nExecution\\nIndividual\\nParame-\\nters\\nMS-MARL-GCM [88]; SchedNet [43]; IMAC [66]; Gated-\\nACML [76]; GAXNet [80]; DCSS [64];\\nParameter Sharing\\nDIAL [73]; RIAL [73]; CommNet [48]; GCL [61]; BiCNet\\n[50]; ATOC [39]; DGN [81]; TarMAC [89]; IC3Net [49]; VBC\\n[31]; NDQ [75]; GA-Comm [83]; LSC [77]; Diff Discrete[78];\\nI2C [79]; ETCNet [86]; Variable-length Coding [87]; TMC\\n[32]; HAMMER [91]; MAGIC [92]; FlowComm [93]; MAIC\\n[33]; FCMNet [94];\\nConcurrent\\nMD-MADDPG [42]; IS [90];\\nâ€¢ Independent Policies. In this category, each local policy is trained with its own set of learning parameters. A central\\nunit collects experiences from all agents to provide global information and guidance, such as gradients, as depicted\\nin Figure 4c. The training of the entire system can employ policy gradient algorithms (e.g., using REINFORCE)\\n[88], or actor-critic methods [43, 66, 76, 80, 64].\\nâ€¢ Parameter Sharing. In this category, all local policies (or local value functions) utilize a shared set of parameters,\\nas illustrated in Figure 4d. Commonly used algorithms in this scenario include DQN-like algorithms, actor-critic\\nmethods, and policy gradient algorithms with REINFORCE. When employing a DQN-like algorithm, a shared\\nlocal Q-function, which processes each agentâ€™s individual experience, is learned collectively across agents [73, 81,\\n77]. Additionally, DQN-based methods can be integrated with value decomposition models (e.g., QMIX [28])\\nin cooperative environments, which enable learning from factorized rewards (value functions) [31, 75, 32, 33].\\nIn the case of actor-critic methods, a shared actor (i.e., policy model) is trained using all individual experiences,\\nsupported by gradient guidance from a central critic [50, 39, 89, 78, 79, 86, 87, 91, 92, 93, 94]. Policy gradient with\\nREINFORCE can alternatively be used, requiring the collection of sampled rewards over episodes [48, 61, 49, 83].\\nâ€¢ Concurrent. In scenarios where storing all experiences in a central unit is not feasible, agents can alternatively\\ncreate backups of all experiences, with the assumption that they are able to observe other agentsâ€™ actions and obser-\\nvations. The concurrent approaches differ inherently from fully decentralized learning. In CTDE with concurrent\\napproaches, each agent maintains an individual set of policy parameters and receives the guidance from a local unit\\nthat collects global information (with additional assumptions on observability), as depicted in Figure 4e. Concurrent\\nCTDE often employs actor-critic methods, where each agent has its own central critic to guide its local actor (policy)\\n[42, 90].\\n3.10\\nPossible Relations of Dimensions\\nWe have introduced 9 dimensions for Comm-MADRL and identified a range of categories within each dimension. It\\nis crucial to consider the potential interdependencies among these dimensions. We realize that the dimensions do not\\ninherently depend on one another based on the criteria used for classifying the literature. However, specific implemen-\\ntations of Comm-MADRL systems may create dependencies between dimensions. For instance, limited bandwidth\\nconstraints (defined in the communication constraints dimension) can be realized by setting a limited number of times\\nto communicate, rendering the full communication category (within the communication policy dimension) infeasible.\\nThis scenario illustrates how the dimensions of communication constraints (Section 3.2) and communication policy\\n(Section 3.4) become interdependent due to specific implementations. Another example about communicated mes-\\nsages shows that the classification criteria we used do not depend on each other. During implementation, a proxy (in\\nthe communicatee type dimension) or corrupted message constraints (in the communication constraints dimension)\\nmay change the value of message content. However, we categorize communicated messages as Existing Knowledge\\nor Imagined Future Knowledge, based on whether future knowledge is simulated and utilized. This classification cri-\\nterion is not inherently linked to a specific type of communicatee or communication constraint. Thus, the dimensions\\nof communicatee type (Section 3.3) and communication constraints (Section 3.2) are independent from the viewpoint\\nof classification criteria. Consequently, the proposed categories and dimensions effectively encapsulate the literature\\nfrom their unique perspectives.\\n20\\nTable 12: The notations of all categories.\\nDimensions\\nNotations\\nControlled Goals (CG)\\nCoo: Cooperative; Com: Competitive; M: Mixed\\nCommunication Constraints (CC)\\nU: Unconstrained Communication;Lb: Limited Bandwidth; Cm: Cor-\\nrupted Messages;\\nCommunicatee Type (CT)\\nNa: Nearby Agents; A: Other (Learning) Agents; P: Proxy\\nCommunication Policy (CP)\\nFc: Full Communication; Ps: Predefined (Partial) Structure; Ic: Individ-\\nual Control; Gc: Global Control\\nCommunicated Messages (CM)\\nE: Existing Knowledge; I: Imagined Future Knowledge\\nMessage Combination (MC)\\nVe: Equally Valued; Vu: Unequally Valued\\nInner Integration (II)\\nPl: Policy-level; Vl: Value-level; PV: Policy-level & Value-level\\nLearning Methods (LM)\\nD: Differentiable; Sp: Supervised; Re: Reinforced; Rg: Regularized\\nTraining Schemes (TS)\\nCL:\\nCentralized Learning; DL:\\nDecentralized Learning; CT DEip:\\nCTDE with Individual (Policy) Parameters; CT DEps: CTDE with Pa-\\nrameter Sharing; CT DEc: Concurrent CTDE\\n4\\nFindings, Discussions, and Research Directions\\nIn this section, we discuss the trend of the current literature and provide our observations and findings based on the\\nproposed dimensions and categorizations. We also dive into the dimensions and suggest possible future research\\ndirections.\\n4.1\\nFindings and Discussions\\nTo provide a more comprehensive overview of the literature, we have utilized the proposed 9 dimensions to categorize\\nexisting works, thereby creating an extensive table. For ease of reference, we introduce notations for these dimensions\\nand their associated categories in Table 12. These notations are subsequently employed to categorize research works\\nin Table 13. In Table 13, research works are sorted based on their publication or archival dates (e.g., on arXiv). Our\\nproposed 9 dimensions offer different perspectives for analyzing and comparing recent works in the field of Comm-\\nMADRL. Through these dimensions and categories, we have observed several intriguing findings.\\nâ€¢ In the dimension of Controlled Goals, recent research has focused on various cooperative settings, together with\\na few mixed scenarios. Communication in non-cooperative multi-agent tasks, however, has not been extensively\\nexplored. In such (non-cooperative) environments, the goals of different agents may conflict. In the emergent lan-\\nguage literature, Noukhovitch et al. [47] have investigated how communication emerges between sender and receiver\\nagents when they exhibit different levels of competitiveness, ranging from full cooperation to full competition. The\\nresults reveal that both sender and receiver agents can obtain higher rewards through communication when the level\\nof competition is not high. However, their research primarily focuses on a simplified game without considering\\nstate transitions. The effectiveness of communication in MARL tasks with large state spaces, particularly in partial\\ncompetitive settings where agents can still gain mutual benefits through low-level cooperation, remains an area for\\nfurther exploration. Moreover, in non-cooperative settings, agents may be motivated to deceive or manipulate the\\ncommunication channel to mislead others. The notion of trust in multi-agent systems introduces the possibility of\\nestablishing a truthful communication protocol [106, 107]. Agents could assess the reliability of opponents and\\ndefend against malicious messages. Additionally, agents might evaluate interactions of opponents with other agents\\nto determine their reputations, which could influence the priorities of communicating with other agents.\\nâ€¢ In the dimension of Communication Constraints, many existing works do not account for communication con-\\nstraints, which may limit their applicability in realistic scenarios that have such limitations. For instance, transmit-\\nting messages in a large multi-agent system across long distances can result in delays, losses, or even be infeasible.\\nCommunication might be asynchronous, requiring several time steps for information exchange. These factors in-\\ntroduce new challenges to Comm-MADRL systems, such as validating previously sent messages and integrating\\nmessages from different time steps. Moreover, if communication resources are limited due to budget or capacity\\nconstraints, agents must decide how to allocate these resources effectively, especially when their goals vary. Con-\\nveying too much information might benefit others while sacrificing the agentâ€™s own learning opportunities. The\\nconcept of fairness, which has been extensively studied in multi-agent systems, focuses on developing fair solutions\\nfor resource allocation. The ideas of maximizing the utility of worse-off agents and decreasing the difference in\\n21\\nutilities between agents in the fairness study can be utilized to distribute communication resources equally. For\\ninstance, agents with lower utilities could be allotted more resources to facilitate their communication with others.\\nâ€¢ In the dimension of Communicatee Type, the concept of a proxy is utilized to facilitate message coordination. When\\nglobal observability is available, a proxy often considers all agents within the environment. This proxy can be\\nparticularly effective and targeted by utilizing the independence among agents, coordinating messages among only\\na subset of agents as necessary.\\nâ€¢ In the dimension of Communication Policy, current works often assume a binary communication action regarding\\nwhether or not to communicate with other agents (or a specific agent). However, communication actions can be\\nmore fine-grained and descriptive. For instance, agents might opt to send only a portion of their messages due\\nto uncertainty or lack of confidence. Additionally, a communication action could be defined more specifically,\\nsuch as communicate with others if the budget exceeds a predetermined threshold. Thus, a communication policy\\ncan encompass a variety of communication actions, tailored to align with human heuristics and specific system\\nrequirements.\\nâ€¢ In the dimension of Communicated Messages, various methods have been proposed to utilize the existing knowl-\\nedge of agents for message generation. Some existing works consider incorporating agentsâ€™ intentions or future\\nplans. However, intentions or future plans may lead to catastrophic errors due to insufficient understanding of the\\nunderlying (transition) dynamics. Model-based Reinforcement Learning (RL) could assist agents in making more\\naccurate predictions about future situations, thereby enabling the agents to communicate information with more\\ncertainty regarding upcoming changes. Additionally, current literature often assumes that messages are conveyed as\\nsingle values or vectors. In contrast, modern devices allow for more complex formats, such as graphs and logical\\nexpressions. These formats can convey a substantial amount of knowledge or facts concisely, facilitating fast coor-\\ndination. However, the challenge lies in effectively encoding and decoding complex information structures, which\\nrequires more sophisticated learning signals.\\nâ€¢ In the dimension of Message Combination, as messages often contain information related to each agentâ€™s individual\\nexperiences, goals, etc., many recent works consider the varying importance of these messages. These research\\nworks mostly rely on attention mechanisms to impose weights on received messages. Furthermore, agents can\\nincorporate their prior knowledge or preferences about other agentsâ€™ capabilities into these weights, enhancing the\\nrelevance and effectiveness of message combination.\\nâ€¢ In the dimension of Inner Integration, many recent works have focused on integrating messages into the policy\\nmodel. This trend is likely due to the growing interest in policy-based methods, particularly actor-critic algorithms,\\nwithin the field of MADRL, where significant advancements have been achieved. Given that neural networks typ-\\nically feature a hierarchical structure, there is potential for agents to effectively integrate messages into different\\nlayers. This approach would allow for considering varying levels of abstraction, potentially enhancing the decision-\\nmaking process.\\nâ€¢ In the dimension of Learning Methods, the learning process for communication typically requires instantaneous\\nfeedback from agents who receive and act upon messages. This feedback could be in the form of gradient informa-\\ntion or changes in the policies or rewards of the receiving agents. However, obtaining instantaneous feedback from\\nother agents might not always be feasible in real-time decision-making systems. Despite this challenge, agents can\\nstill observe changes in the environment and their rewards to self-evaluate the effectiveness of their communication.\\nThis self-evaluation process enables agents to update and learn their communication protocols over time.\\nâ€¢ In the dimension of Training Schemes, parameter sharing combined with centralized training and decentralized\\nexecution is widely adopted in Comm-MADRL to reduce the number of learning parameters. However, accessing\\nother agentsâ€™ memories and parameters might raise privacy concerns. On the other hand, fully decentralized learning\\npresents significant challenges and remains a key research area in MARL. In fully decentralized learning, agents\\nhave limited knowledge about the environment and must deal with non-stationarity, a problem that intensifies with\\nan increasing number of agents. Nonetheless, Comm-MADRL can benefit from advancements in MARL, potentially\\nleading to the development of novel training paradigms that better balance knowledge sharing, privacy, and learning\\nefficiency.\\nBased on the proposed dimensions, we have identified a range of findings and potential issues in the field of Comm-\\nMADRL. Among these issues, achieving fully decentralized learning and self-evaluated communication protocols\\nremains a significant challenge. This difficulty arises because each agent has access only to their own data collected\\nfrom the environment, adding complexity to message evaluation without the help of other agents. Decentralized ac-\\ntion policies and self-evaluated communication protocols, however, could be advantageous in areas like Electronic\\nCommerce [108], Networks [109], and Blockchain [110], where synchronizing knowledge and information among\\nusers or agents can be computationally demanding. Another open question involves how to effectively communicate\\nusing more complex message formats and implement efficient training methods, potentially leading to more sophisti-\\n22\\ncated communication architectures. Importantly, advancements in multi-agent systems and multi-agent reinforcement\\nlearning can significantly contribute to the progress of Comm-MADRL.\\nTable 13: An overview of recent works in Comm-MADRL. a+b denotes that the research work considers categories a\\nand b simultaneously in the environment. a/b denotes that the research work has been examined in multiple categories\\nbut in separate environments or settings.\\nMethods\\nCG\\nCC\\nCT\\nCP\\nCM\\nMC\\nII\\nLM\\nTS\\nDIAL [73]\\nCoo\\nLb+Cm\\nA\\nFc\\nE\\nVe\\nVl\\nD\\nCT DEps\\nRIAL [73]\\nCoo\\nLb\\nA\\nFc\\nE\\nVe\\nVl\\nRe\\nCT DEps\\nCommNet [48]\\nCoo\\nU\\nA\\nFc\\nE\\nVe\\nPl\\nD\\nCT DEps\\nGCL [61]\\nCoo\\nLb\\nA\\nFc\\nE\\nVe\\nPl\\nD+Re\\nCT DEps\\nBiCNet [50]\\nCoo\\nU\\nA\\nFc\\nE\\nVu\\nPV\\nD\\nCT DEps\\nMS-MARL-\\nGCM [88]\\nCoo\\nU\\nP\\nFc\\nE\\nVu\\nPl\\nD\\nCT DEip\\nATOC [39]\\nCoo\\nU\\nP\\nIc\\nI\\nVu\\nPl\\nSp\\nCT DEps\\nIC [62]\\nCoo\\nLb\\nA\\nFc\\nE\\nVe\\nPV\\nRe\\nDL\\nDGN [81]\\nCoo/M\\nU\\nNa\\nPs\\nE\\nVu\\nVl\\nD\\nCT DEps\\nTarMAC [89]\\nCoo/M\\nU\\nA\\nFc\\nE\\nVu\\nPV\\nD\\nCT DEps\\nMAGNet-SA-\\nGS-MG [40]\\nCoo\\nU\\nNa\\nPs\\nE\\nVu\\nPl\\nD\\nDL\\nMADDPG-M\\n[41]\\nCoo\\nU\\nA\\nIc\\nE\\nVe\\nPV\\nRe\\nDL\\nIC3Net [49]\\nCoo/Com/MU\\nA\\nIc\\nE\\nVe\\nPl\\nRe\\nCT DEps\\nMD-MADDPG\\n[42]\\nCoo\\nU\\nP\\nFc\\nE\\nVu\\nPl\\nD\\nCT DEc\\nSchedNet [43]\\nCoo\\nLb\\nA\\nGc\\nE\\nVe\\nPl\\nRe\\nCT DEip\\nDCC-MD [82]\\nCoo\\nU\\nA\\nFc\\nE\\nVu\\nVl\\nD\\nDL\\nAgent-Entity\\nGraph [74]\\nCoo\\nU\\nNa\\nPs\\nE\\nVu\\nPV\\nD\\nDL\\nVBC [31]\\nCoo\\nLb\\nA\\nPs\\nE\\nVe\\nVl\\nD\\nCT DEps\\nNDQ [75]\\nCoo/M\\nLb\\nA\\nPs\\nE\\nVe\\nVl\\nRg\\nCT DEps\\nIMAC [66]\\nCoo\\nLb\\nP\\nFc\\nE\\nVu\\nPl\\nRg\\nCT DEip\\nGA-Comm [83]\\nCoo\\nU\\nP\\nGc\\nE\\nVu\\nPl\\nD\\nCT DEps\\nGated-ACML\\n[76]\\nCoo\\nLb\\nP\\nIc\\nE\\nVu\\nPl\\nSp\\nCT DEip\\nBias [63]\\nCoo\\nLb\\nA\\nFc\\nE\\nVe\\nPV\\nRg\\nDL\\nLSC [77]\\nCoo/M\\nU\\nNa\\nIc\\nE\\nVu\\nVl\\nRe\\nCT DEps\\nDiff Discrete[78]\\nCoo\\nCm\\nA\\nFc\\nE\\nVe\\nPl\\nD\\nCT DEps\\nNeurComm [84]\\nCoo\\nU\\nNa\\nPs\\nI\\nVu\\nPV\\nD\\nDL\\nIP [85]\\nCoo\\nU\\nNa\\nPs\\nI\\nVu\\nPl\\nD\\nDL\\nI2C [79]\\nCoo\\nU\\nA\\nIc\\nE\\nVu\\nPl\\nSp\\nCT DEps\\nIS [90]\\nCoo\\nU\\nA\\nFc\\nI\\nVe\\nPl\\nD\\nCT DEc\\nETCNet [86]\\nCoo\\nLb\\nA\\nIc\\nE\\nVe\\nPl\\nRe\\nCT DEps\\nContinued on next page\\n23\\nVariable-length\\nCoding [87]\\nCoo\\nLb\\nA\\nFc\\nE\\nVe\\nPl\\nD\\nCT DEps\\nTMC [32]\\nCoo\\nLb\\nA\\nPs\\nE\\nVu\\nVl\\nD\\nCT DEps\\nHAMMER [91]\\nCoo\\nU\\nP\\nFc\\nE\\nVu\\nPl\\nRe\\nCT DEps\\nMAGIC [92]\\nCoo/M\\nU\\nP\\nGc\\nE\\nVu\\nPV\\nD\\nCT DEps\\nFlowComm [93]\\nCoo\\nU\\nNa\\nGc\\nE\\nVe\\nPl\\nD\\nCT DEps\\nAE-Comm [65]\\nCoo\\nLb\\nA\\nFc\\nE\\nVe\\nPV\\nRg\\nDL\\nGAXNet [80]\\nCoo\\nU\\nNa\\nPs\\nE\\nVu\\nPl\\nD\\nCT DEip\\nDCSS [64]\\nCoo\\nCm\\nP\\nFc\\nE\\nVe\\nPV\\nD+Sp\\nCT DEip\\nR-MACRL [95]\\nCom\\nCm\\nA\\nFc\\nE\\nVu\\nPl\\nSp\\nDL\\nMAIC [33]\\nCoo\\nLb\\nA\\nPs\\nE\\nVu\\nVl\\nRg\\nCT DEps\\nFCMNet [94]\\nCoo\\nU\\nA\\nFc\\nE\\nVu\\nPV\\nD\\nCT DEps\\nIn addition to these findings, the evaluation metrics used in Comm-MADRL research are of significant interest. It is\\nnoteworthy that existing works have been evaluated across various platforms and games, employing different metrics\\nto assess performance. Crucially, Comm-MADRL studies often use varying settings of experiments, such as the\\nnumber of agents or the use of parameter sharing. These settings can make it challenging to fairly compare the relative\\nstrengths and limitations of algorithms based on their performance outcomes [111]. We have identified four evaluation\\nmetrics commonly used in Comm-MADRL studies as follows:\\nâ€¢ Reward-based: This metric employs the converged return or average rewards per episode or time step to demon-\\nstrate the profit gained by agents.\\nâ€¢ Win or Fail Rate: This metric calculates the percentage that agents achieve their goal or fail the game during\\nlearning. It is often used in episodic tasks.\\nâ€¢ Steps Taken: This metric evaluates the number of time steps learned to reach the goal. It is often used in episodic\\ntasks and essential in scenarios where time efficiency is key.\\nâ€¢ Communication Efficiency: This metric evaluates how much communication resource has been used, such as the\\nfrequency of communication between agents.\\nâ€¢ Emergence Degree: Originating from the field of emergent language, this metric evaluates and detects the emer-\\ngence of language [112, 45]. It is often used in learning tasks with emergent language. Positive signaling and\\npositive listening are two common approaches. Positive signaling measures the correlation between a message and\\nthe senderâ€™s observation or intended action. Positive listening assesses the impact of an observed message on the\\nreceiverâ€™s beliefs or behavior.\\nWe have analyzed the number of times that the above performance metrics are used in existing Comm-MADRL\\nstudies, as illustrated in Figure 5. It is shown that the metric of communication efficiency has not been extensively\\nused in the literature, requiring further investigation into the use of communication resources in Comm-MADRL\\napproaches. The Emergence Degree metric, intended to measure whether a language is emergent, is primarily utilized\\nin emergent language studies. Nonetheless, this metric can also yield significant insights for other Comm-MADRL\\nsystems. By analyzing the correlation between communication and the observations and behaviors of both senders\\nand receivers, we could obtain a deeper understanding and explanation of communication for Comm-MADRL.\\nIn the next section, inspired by the proposed dimensions, we demonstrate the potential for discovering new ideas\\nthrough our survey. We identify several possible research directions that jointly explore multiple dimensions, aiming\\nto bridge the gaps in current works.\\n4.2\\nResearch Directions\\nComm-MADRL is a young but rapidly enlarging field. There are still lots of possibilities to develop new Comm-\\nMADRL systems. Our proposed dimensions encapsulate several aspects of Comm-MADRL, from which we can\\nidentify new research directions. Therefore, we showcase four research directions motivated by leveraging the possible\\ncombinations of dimensions and the extensions of corresponding categories. We also point out further challenges for\\nComm-MADRL.\\n24\\nReward-based\\nWin or Fail Rate\\nSteps Taken\\nCommunication Efficiency\\nEmergence Degree\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nCount\\nFigure 5: The Statistics of Evaluation Metrics in existing Comm-MADRL systems.\\n4.2.1\\nMultimodal Communication\\nA versatile robot can hear by sound sensors, read text or talk with human partners. Intelligent agents may be surrounded\\nby different data sources and act based on multimodal input. By jointly considering the dimensions of communicatee\\ntype and communicated messages, we can imagine a fertile scenario where communication is not limited to images\\nor handcrafted features but encompasses multimodal data, such as speech, videos, and text from humans or domestic\\nrobots, to prosper applications like smart home. To the best of our knowledge, existing works in Comm-MADRL do\\nnot consider communicating multimodal data or encoding them. Recent works often use encoded images as messages,\\nwhich only cover visually-based applications. Therefore, we believe exploring multimodal communication represents\\na promising research direction and introduces several challenges that need to be addressed. In multimodal communi-\\ncation, agents have to coordinate heterogeneous modalities and encode various types of information into messages. A\\npossible solution is to use separate channels to communicate specific modalities, while agents must decide on the right\\nchannel to communicate and merge data from different channels. A more efficient way is to learn a joint representation\\nof multimodal observations and communicate on one channel. Due to the progress of Multimodal machine learning\\n[113], we can bring ideas from this area to equip agents with the ability to create a single representation of multimodal\\ndata. Nevertheless, it is unclear how the solutions from Multimodal machine learning can be extended to multi-agent\\nreinforcement learning. Poklukar et al. [114] propose learning an aligned representation from multiple modalities,\\nalthough their tests are conducted in a single-agent reinforcement learning task. The multi-agent scenarios, however,\\nmay need to consider the individual abilities and preferences of different agents. For example, a voice-activated agent\\nmay favor voice data for interaction, while a monitoring agent may only access video data. Therefore, in multi-agent\\nsettings, agents need to align their individual preferences regarding multimodality when learning a joint representation\\nof the multimodal data. Another crucial technical issue is how to represent multimodal messages in low-dimensional\\nvectors without losing essential information from each modality, as Comm-MADRL systems often consider reducing\\ncommunication costs. Eventually, we expect the progress of multimodal communication will benefit human-agent\\ninteraction and diverse communicating agents.\\nThe emergence of new research works would introduce new categories under each dimension. For example, with\\ndevelopments in multimodal communication, we can extend the categories of communicated messages with speech,\\nimage, text, and video data. Nevertheless, our proposed dimensions can be adaptive and robust to cover new Comm-\\nMADRL research in this direction.\\n4.2.2\\nStructural Communication\\nThrough the Internet, electronic devices like routers can process and transmit information. On social media, chatbots\\ncan be community members [115, 116], to engage in conversations with users and share information/opinions. In those\\nlarge-scale multi-agent systems [117, 118], agents may belong to different groups, where their relationships can be\\ncomplicated. For example, local area networks create boundaries of communication and interaction between devices.\\nChatbots may not be able to reach some users because of limited permission or the lack of friendship relations. These\\nrestricted connectivities among agents require more efficient usage of communication structure. Therefore, we think\\nthat the research direction focusing on structural communication opens up possibilities for enabling communication\\namong a larger number of agents. In the current literature in Comm-MADRL, ATOC [39] and LSC [77] have inves-\\ntigated communication with multiple groups, where agents can only communicate with other agents who belong to\\nthe same group. In both approaches, different groups may share common member agents, i.e., bridge agents, which\\nare used to enable information to flow from group to group. However, communication through bridge agents is not\\n25\\ntargeted and each agent unconsciously shares their information with other groups. In terms of the dimension of con-\\ntrolled goals, agents may have individualized goals and require collaboration with a specific set of agents. Therefore,\\nan important future direction of structural communication is to send critical information and opinions to target agents.\\nFor example, agent 1 may observe the goal location of agent 2 while they belong to different groups. If agent 3 hap-\\npens to be a common friend of agent 1 and 2, agent 1 can actively send the goal information to agent 2 with the help of\\nagent 3. If communication is costly and information is private, agents need to make thoughtful decisions about which\\nbridge agents to be used to find the shortest and safe path to reach targeted agents. At the same time, bridge agents\\nneed to agree on the communication path to transmit information successfully. If a complex and hierarchical friend-\\nship network is identified, another important question is how to prioritize and schedule different communication paths\\nto make communication fluent. Regarding communicated messages, agents need to build a common protocol with\\ntargeted agents so that information can be encoded and decoded successfully. As a result, agents can more actively\\nutilize the communication structure among agents to achieve better collaboration and agreements.\\n4.2.3\\nRobust Centralized Unit\\nRobustness has been widely considered in the field of reinforcement learning [119, 120], where an agent needs to cope\\nwith disturbances in learning in order to achieve a robust policy that can generalize under changes in training/test data.\\nIn MARL, agentsâ€™ policies can be sensitive to environmental noise or malicious intentions of opponents, and thus\\nrobust policies are required [121, 122]. With communication, opponents may produce malicious messages, imply-\\ning adversary intentions. Preventing malicious messages is important in non-cooperative settings as adversary agents\\nmay manipulate communication to achieve their own goals at the expense of other agentsâ€™ benefits. Existing works on\\nComm-MADRL, such as R-MACRL [95], have investigated how to detect adversary information and reconstruct orig-\\ninal messages. However, as we discussed in the dimensions of communicatee type and training paradigm, proxy and\\ncritics are often centralized and gather information from all agents. Robustness becomes essential for these central-\\nized units as all agents involved in communication can be misled by polluted feedback, for example, incorrect gradient\\nsignals from critics or malicious messages from a proxy. Moreover, malicious messages can easily spread through the\\n(centralized) proxy. Therefore, we think building a robust centralized unit is a promising and underdeveloped direc-\\ntion for safe communication in MADRL, where proxies and critics need to avoid communication being exploited by\\nadversaries or affected by harmful environmental changes. By considering the dimension of communication policy,\\nsender agents can learn a versatile communication policy. For example, the communication policy can be defined to\\nselect different encoding protocols for different groups of agents, in case malicious agents may easily find a solution to\\ncheat on a specific encoding protocol. Besides, as malicious or noisy messages can be hidden in the centralized proxy,\\nit is important to figure out which messages are malicious and how to reconstruct the original messages. Nonetheless,\\ndeveloping robust centralized units is vital for reliable and protected Comm-MADRL systems.\\n4.2.4\\nLearning Tasks with Emergent Language\\nIn this survey, we have identified the intersection between learning tasks with communication and emergent language\\nin the field of MADRL, which we have called learning tasks with emergent language. We also observed that there\\nis only a limited number of research works concerning this sub-area learning tasks with emergent language, which\\nlearns a language while achieving a MADRL task. We believe this area can be further expanded and investigated, by\\nconsidering several dimensions proposed by our survey. First, the communicated messages, as we discussed earlier,\\ncan be encoded into more complex symbolic formats, such as graphs or logical expressions. Existing works in the\\nfield only learn how to communicate through atomic symbols or a combination [61, 63, 64]. However, it is important\\nto learn the relation between symbols. For example, symbol A is on the left of symbol B. Those messages can express\\nfacts about what agents know or conjecture. Therefore, receivers can quickly adapt their behaviors by successfully\\ndecoding the messages. The important question is how to learn both encoding and decoding with complex expressions\\nof messages, which can have a significant number of possibilities. The senders should also properly encapsulate their\\nknowledge and the receivers should reason on the messages correctly. In addition, how complex symbolic formats\\ncan emerge in non-cooperative settings is an interesting but unexplored research area. Whatâ€™s more, the combination\\nof complex messages will not be as easy as handling single values or vectors. Therefore, learning together with\\ncomplicated communication is still challenging.\\n4.2.5\\nFurther Challenges\\nIn the field of Comm-MADRL, there are further challenges. For instance, the design of neural network architectures\\nplays a critical role in performance and communication. A deeper neural network may be effective in some domains\\nwhile failing in other domains. For example, LSTM is effective in capturing history information while may require\\nmuch time to train the parameters [123, 124], which could greatly slow down the learning in tasks with high complex-\\nity. The choice of architectures and fine-tuning hyperparameters are significant problems of Comm-MADRL. With\\ncommunication, another crucial issue is the explainability of communicated messages. Emergent language has made\\na step towards human-like language. However, whether machines communicate in a human-like way and can learn a\\nhuman-interpretable language is still unclear. A great number of existing works regarding learning tasks with commu-\\n26\\nnication seek hidden, deep, and obscure codes for messages [48, 49, 79, 92], which still need to be further interpreted\\nand understood.\\n5\\nConclusions\\nOur survey proposes to classify the literature based on 9 dimensions. These dimensions constitute the basis of design-\\ning Comm-MADRL systems. We further categorize existing works under each dimension, where readers can easily\\ncompare research works from a unique perspective. Based on those dimensions, we also observe findings through\\nthe trend of the literature and identify new research directions by filling the gap among recent works. Our survey\\nconcludes that while the number of works in Comm-MADRL is notable and represents significant achievements,\\ncommunication can be more fruitful and versatile to incorporate non-cooperative settings, heterogeneous players, and\\nmany more agents. Agents can communicate information not only from raw image inputs or handcrafted features but\\nalso from diverse data sources such as voice and text. Furthermore, we can explore novel metrics to better understand\\nthe contribution of communication to the overall learning process. Ultimately, Comm-MADRL can benefit from the\\nMARL community and take advantage of good solutions from MARL.\\nReferences\\n[1] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for\\nautonomous driving. CoRR, abs/1610.03295, 2016.\\n[2] Meritxell Vinyals, Juan A. RodrÂ´Ä±guez-Aguilar, and JesÂ´us Cerquides.\\nA survey on sensor networks from a\\nmultiagent perspective. Comput. J., 54(3):455â€“470, 2011.\\n[3] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. Int. J. Robotics\\nRes., 32(11):1238â€“1274, 2013.\\n[4] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas\\nHubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, Laurent Sifre,\\nGeorge van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human\\nknowledge. Nature, 550(7676):354â€“359, 2017.\\n[5] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):885â€“890,\\n2019.\\n[6] Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs. Springer Briefs\\nin Intelligent Systems. 2016.\\n[7] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for\\nmixed cooperative-competitive environments. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\\nWallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information\\nProcessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9,\\n2017, Long Beach, CA, USA, pages 6379â€“6390, 2017.\\n[8] Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Coun-\\nterfactual multi-agent policy gradients. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings\\nof the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications\\nof Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelli-\\ngence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 2974â€“2982, 2018.\\n[9] Georgios Papoudakis, Filippos Christianos, Arrasy Rahman, and Stefano V. Albrecht.\\nDealing with non-\\nstationarity in multi-agent deep reinforcement learning. CoRR, abs/1906.04737, 2019.\\n[10] Mohamed Salah ZaÂ¨Ä±em and Etienne Bennequin. Learning to communicate in multi-agent reinforcement learning\\n: A review. CoRR, abs/1911.05438, 2019.\\n[11] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436â€“444, 2015.\\n[12] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex\\nGraves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir\\nSadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hass-\\nabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529â€“533, 2015.\\n[13] Peter Stone and Manuela M. Veloso. Multiagent systems: A survey from a machine learning perspective. Auton.\\nRobots, 8(3):345â€“383, 2000.\\n[14] Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the art. Auton. Agents Multi Agent\\nSyst., 11(3):387â€“434, 2005.\\n27\\n[15] Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement\\nlearning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156â€“\\n172, 2008.\\n[16] Pablo Hernandez-Leal, Bilal Kartal, and Matthew E. Taylor. A survey and critique of multiagent deep rein-\\nforcement learning. Auton. Agents Multi Agent Syst., 33(6):750â€“797, 2019.\\n[17] Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. Artificial Intelligence\\nReview, pages 1â€“49, 2021.\\n[18] Angeliki Lazaridou and Marco Baroni. Emergent multi-agent communication in the deep learning era. CoRR,\\nabs/2006.02419, 2020.\\n[19] Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs. Springer Briefs\\nin Intelligent Systems. 2016.\\n[20] Eric A. Hansen, Daniel S. Bernstein, and Shlomo Zilberstein. Dynamic programming for partially observable\\nstochastic games. In Deborah L. McGuinness and George Ferguson, editors, Proceedings of the Nineteenth\\nNational Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial\\nIntelligence, July 25-29, 2004, San Jose, California, USA, pages 709â€“715, 2004.\\n[21] Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical per-\\nspective. CoRR, abs/2011.00583, 2020.\\n[22] Ming Tan. Multi-agent reinforcement learning: Independent versus cooperative agents. In Paul E. Utgoff,\\neditor, Machine Learning, Proceedings of the Tenth International Conference, University of Massachusetts,\\nAmherst, MA, USA, June 27-29, 1993, pages 330â€“337, 1993.\\n[23] LaÂ¨etitia Matignon, Guillaume J. Laurent, and Nadine Le Fort-Piat.\\nIndependent reinforcement learners in\\ncooperative markov games: a survey regarding coordination problems. Knowledge Engineering Revie, 27(1):1â€“\\n31, 2012.\\n[24] Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems.\\nIn Jack Mostow and Chuck Rich, editors, Proceedings of the Fifteenth National Conference on Artificial Intel-\\nligence and Tenth Innovative Applications of Artificial Intelligence Conference, AAAI 98, IAAI 98, July 26-30,\\n1998, Madison, Wisconsin, USA, pages 746â€“752, 1998.\\n[25] Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul\\nVicente. Multiagent cooperation and competition with deep reinforcement learning. CoRR, abs/1511.08779,\\n2015.\\n[26] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, 2nd Edition. The MIT Press.\\n2018.\\n[27] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, VinÂ´Ä±cius Flores Zambaldi, Max\\nJaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel. Value-decomposition\\nnetworks for cooperative multi-agent learning based on team reward. In Elisabeth AndrÂ´e, Sven Koenig, Mehdi\\nDastani, and Gita Sukthankar, editors, Proceedings of the 17th International Conference on Autonomous Agents\\nand MultiAgent Systems, AAMAS 2018, Stockholm, Sweden, July 10-15, 2018, pages 2085â€“2087, 2018.\\n[28] Tabish Rashid, Mikayel Samvelyan, Christian SchrÂ¨oder de Witt, Gregory Farquhar, Jakob N. Foerster, and\\nShimon Whiteson. QMIX: monotonic value function factorisation for deep multi-agent reinforcement learning.\\nIn Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine\\nLearning, ICML 2018, StockholmsmÂ¨assan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of\\nMachine Learning Research, pages 4292â€“4301, 2018.\\n[29] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Hostallero, and Yung Yi. QTRAN: learning to factorize\\nwith transformation for cooperative multi-agent reinforcement learning. In Kamalika Chaudhuri and Ruslan\\nSalakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019,\\n9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages\\n5887â€“5896, 2019.\\n[30] Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. DOP: off-policy multi-agent\\ndecomposed policy gradients. In 9th International Conference on Learning Representations, ICLR 2021, Virtual\\nEvent, Austria, May 3-7, 2021, 2021.\\n[31] Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Efficient communication in multi-agent reinforcement learning via\\nvariance based control. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÂ´e-Buc,\\nEmily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32 (NeurIPS),\\npages 3230â€“3239, 2019.\\n28\\n[32] Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Succinct and robust multi-agent communication with temporal\\nmessage control. In Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-\\nTien Lin, editors, Advances in Neural Information Processing Systems 33 (NIPS), 2020.\\n[33] Lei Yuan, Jianhao Wang, Fuxiang Zhang, Chenghe Wang, Zongzhang Zhang, Yang Yu, and Chongjie Zhang.\\nMulti-agent incentive communication via decentralized teammate modeling. In Thirty-Sixth AAAI Conference\\non Artificial Intelligence (AAAI-22), 2022.\\n[34] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. QPLEX: duplex dueling multi-agent\\nq-learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021, 2021.\\n[35] Vijay R. Konda and John N. Tsitsiklis. Actor-critic algorithms. In Sara A. Solla, Todd K. Leen, and Klaus-\\nRobert MÂ¨uller, editors, Advances in Neural Information Processing Systems 12, NIPS Conference, pages 1008â€“\\n1014, 1999.\\n[36] John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-dimensional contin-\\nuous control using generalized advantage estimation. In Yoshua Bengio and Yann LeCun, editors, 4th Interna-\\ntional Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference\\nTrack Proceedings, 2016.\\n[37] Afshin Oroojlooyjadid and Davood Hajinezhad. A review of cooperative multi-agent deep reinforcement learn-\\ning. CoRR, abs/1908.03963, 2019.\\n[38] Georgios Papoudakis, Filippos Christianos, Lukas SchÂ¨afer, and Stefano V Albrecht. Comparative evaluation of\\ncooperative multi-agent deep reinforcement learning algorithms. arXiv: 2006.07869, 2020.\\n[39] Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation. In Advances\\nin Neural Information Processing Systems 31 (NIPS), pages 7265â€“7275, 2018.\\n[40] Aleksandra Malysheva, Tegg Tae Kyong Sung, Chae-Bong Sohn, Daniel Kudenko, and Aleksei Shpilman. Deep\\nmulti-agent reinforcement learning with relevance graphs. CoRR, abs/1811.12557, 2018.\\n[41] Ozsel Kilinc and Giovanni Montana. Multi-agent deep reinforcement learning with extremely noisy observa-\\ntions. CoRR, abs/1812.00922, 2018.\\n[42] Emanuele Pesce and Giovanni Montana. Improving coordination in small-scale multi-agent deep reinforcement\\nlearning through memory-driven communication. Machine Learning, 109(9-10):1727â€“1747, 2020.\\n[43] Daewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan Son, and Yung Yi.\\nLearning to schedule communication in multi-agent reinforcement learning. In 7th International Conference\\non Learning Representations (ICLR), 2019.\\n[44] Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z. Leibo, Karl Tuyls, and Stephen Clark. Emergent com-\\nmunication through negotiation. In 6th International Conference on Learning Representations, ICLR 2018,\\nVancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.\\n[45] Ryan Lowe, Jakob N. Foerster, Y-Lan Boureau, Joelle Pineau, and Yann N. Dauphin. On the pitfalls of measur-\\ning emergent communication. In Edith Elkind, Manuela Veloso, Noa Agmon, and Matthew E. Taylor, editors,\\nProceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS â€™19,\\nMontreal, QC, Canada, May 13-17, 2019, pages 693â€“701, 2019.\\n[46] Kalesha Bullard, Douwe Kiela, Joelle Pineau, and Jakob N. Foerster. Quasi-equivalence discovery for zero-shot\\nemergent communication. CoRR, abs/2103.08067, 2021.\\n[47] Michael Noukhovitch, Travis LaCroix, Angeliki Lazaridou, and Aaron C. Courville. Emergent communication\\nunder competition. In Frank Dignum, Alessio Lomuscio, Ulle Endriss, and Ann NowÂ´e, editors, AAMAS â€™21:\\n20th International Conference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom,\\nMay 3-7, 2021, pages 974â€“982, 2021.\\n[48] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with backpropaga-\\ntion. In Advances in Neural Information Processing Systems 29 (NIPS), pages 2244â€“2252, 2016.\\n[49] Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate at scale in multiagent\\ncooperative and competitive tasks. In 7th International Conference on Learning Representations, ICLR 2019,\\nNew Orleans, LA, USA, May 6-9, 2019, 2019.\\n[50] Peng Peng, Quan Yuan, Ying Wen, Yaodong Yang, Zhenkun Tang, Haitao Long, and Jun Wang. Multiagent\\nbidirectionally-coordinated nets for learning to play starcraft combat games. CoRR, abs/1703.10069, 2017.\\n[51] Joseph Farrell and Matthew Rabin. Cheap talk. Journal of Economic perspectives, 10(3):103â€“118, 1996.\\n29\\n[52] Hyowoon Seo, Jihong Park, Mehdi Bennis, and MÂ´erouane Debbah. Semantics-native communication with\\ncontextual reasoning. CoRR, abs/2108.05681, 2021.\\n[53] Tadahiro Taniguchi, Yuto Yoshida, Akira Taniguchi, and Yoshinobu Hagiwara.\\nEmergent communication\\nthrough metropolis-hastings naming game with deep generative models. CoRR, abs/2205.12392, 2022.\\n[54] Rahma Chaabouni, Florian Strub, Florent AltchÂ´e, Eugene Tarassov, Corentin Tallec, Elnaz Davoodi, Kory Wal-\\nlace Mathewson, Olivier Tieleman, Angeliki Lazaridou, and Bilal Piot. Emergent communication at scale. In\\nThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022,\\n2022.\\n[55] Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco Baroni. Compo-\\nsitionality and generalization in emergent languages. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.\\nTetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\\nACL 2020, Online, July 5-10, 2020, pages 4427â€“4442, 2020.\\n[56] Cinjon Resnick, Abhinav Gupta, Jakob N. Foerster, Andrew M. Dai, and Kyunghyun Cho. Capacity, bandwidth,\\nand compositionality in emergent language learning. In Amal El Fallah Seghrouchni, Gita Sukthankar, Bo An,\\nand Neil Yorke-Smith, editors, Proceedings of the 19th International Conference on Autonomous Agents and\\nMultiagent Systems, AAMAS â€™20, Auckland, New Zealand, May 9-13, 2020, pages 1125â€“1133, 2020.\\n[57] Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, and Marco Baroni. Anti-efficient encoding in\\nemergent communication. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÂ´e-Buc,\\nEmily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual\\nConference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,\\nBC, Canada, pages 6290â€“6300, 2019.\\n[58] Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to communicate with\\nsequences of symbols. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus,\\nS. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30:\\nAnnual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,\\nUSA, pages 2149â€“2159, 2017.\\n[59] Alexander Imani Cowen-Rivers and Jason Naradowsky. Emergent communication with world models. CoRR,\\nabs/2002.09604, 2020.\\n[60] Ivana Kajic, Eser AygÂ¨un, and Doina Precup. Learning to cooperate: Emergent communication in multi-agent\\nnavigation. In Stephanie Denison, Michael Mack, Yang Xu, and Blair C. Armstrong, editors, Proceedings of\\nthe 42th Annual Meeting of the Cognitive Science Society - Developing a Mind: Learning in Humans, Animals,\\nand Machines, CogSci 2020, virtual, July 29 - August 1, 2020, 2020.\\n[61] Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent populations.\\nIn Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on\\nArtificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the\\n8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana,\\nUSA, February 2-7, 2018, pages 1495â€“1502, 2018.\\n[62] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, CÂ¸ aglar GÂ¨ulcÂ¸ehre, Pedro A. Ortega, DJ Strouse, Joel Z.\\nLeibo, and Nando de Freitas.\\nSocial influence as intrinsic motivation for multi-agent deep reinforcement\\nlearning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International\\nConference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of\\nProceedings of Machine Learning Research, pages 3040â€“3049, 2019.\\n[63] Tom Eccles, Yoram Bachrach, Guy Lever, Angeliki Lazaridou, and Thore Graepel. Biases for emergent com-\\nmunication in multi-agent reinforcement learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,\\nFlorence dâ€™AlchÂ´e-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing\\nSystems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December\\n8-14, 2019, Vancouver, BC, Canada, pages 13111â€“13121, 2019.\\n[64] Mycal Tucker, Huao Li, Siddharth Agrawal, Dana Hughes, Katia P. Sycara, Michael Lewis, and Julie A. Shah.\\nEmergent discrete communication in semantic spaces. In Marcâ€™Aurelio Ranzato, Alina Beygelzimer, Yann N.\\nDauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing\\nSystems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December\\n6-14, 2021, virtual, pages 10574â€“10586, 2021.\\n[65] Toru Lin, Jacob Huh, Christopher Stauffer, Ser-Nam Lim, and Phillip Isola. Learning to ground multi-agent\\ncommunication with autoencoders. In Marcâ€™Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy\\nLiang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual\\n30\\nConference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual,\\npages 15230â€“15242, 2021.\\n[66] Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, and Zinovi Rabinovich. Learning efficient multi-agent\\ncommunication: An information bottleneck approach. In Proceedings of the 37th International Conference\\non Machine Learning (ICML), volume 119 of Proceedings of Machine Learning Research, pages 9908â€“9918,\\n2020.\\n[67] Wanqi Xue, Wei Qiu, Bo An, Zinovi Rabinovich, Svetlana Obraztsova, and Chai Kiat Yeo. Mis-spoke or\\nmis-lead: Achieving robustness in multi-agent communicative reinforcement learning. CoRR, abs/2108.03803,\\n2021.\\n[68] Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep reinforcement learning for multi-agent\\nsystems: A review of challenges, solutions and applications. CoRR, abs/1812.11794, 2018.\\n[69] Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Multi-agent reinforcement learning: A selective overview of\\ntheories and algorithms. CoRR, abs/1911.10635, 2019.\\n[70] Annie Wong, Thomas BÂ¨ack, Anna V. Kononova, and Aske Plaat. Multiagent deep reinforcement learning:\\nChallenges and directions towards human-like approaches. CoRR, abs/2106.15691, 2021.\\n[71] Mohamed Salah ZaÂ¨Ä±em and Etienne Bennequin. Learning to communicate in multi-agent reinforcement learning\\n: A review. CoRR, abs/1911.05438, 2019.\\n[72] Yoav Shoham and Kevin Leyton-Brown.\\nMultiagent Systems - Algorithmic, Game-Theoretic, and Logical\\nFoundations. Cambridge University Press. 2009.\\n[73] Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with\\ndeep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems 29 (NIPS),\\npages 2137â€“2145, 2016.\\n[74] Akshat Agarwal, Sumit Kumar, Katia P. Sycara, and Michael Lewis. Learning transferable cooperative be-\\nhavior in multi-agent teams. In Proceedings of the 19th International Conference on Autonomous Agents and\\nMultiagent Systems (AAMAS), pages 1741â€“1743, 2020.\\n[75] Tonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. Learning nearly decomposable value\\nfunctions via communication minimization. In 8th International Conference on Learning Representations,\\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\\n[76] Hangyu Mao, Zhengchao Zhang, Zhen Xiao, Zhibo Gong, and Yan Ni. Learning agent communication under\\nlimited bandwidth by message pruning. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, pages\\n5142â€“5149, 2020.\\n[77] Junjie Sheng, Xiangfeng Wang, Bo Jin, Junchi Yan, Wenhao Li, Tsung-Hui Chang, Jun Wang, and Hongyuan\\nZha. Learning structured communication for multi-agent reinforcement learning. CoRR, abs/2002.04235, 2020.\\n[78] Benjamin Freed, Guillaume Sartoretti, Jiaheng Hu, and Howie Choset. Communication learning via back-\\npropagation in discrete channels with unknown noise. In The Thirty-Fourth AAAI Conference on Artificial\\nIntelligence, pages 7160â€“7168, 2020.\\n[79] Ziluo Ding, Tiejun Huang, and Zongqing Lu. Learning individually inferred communication for multi-agent\\ncooperation. In Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien\\nLin, editors, Advances in Neural Information Processing Systems 33 (NeurIPS), 2020.\\n[80] Won Joon Yun, Byungju Lim, Soyi Jung, Young-Chai Ko, Jihong Park, Joongheon Kim, and Mehdi Bennis.\\nAttention-based reinforcement learning for real-time UAV semantic communication. CoRR, abs/2105.10716,\\n2021.\\n[81] Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement learning. In\\n8th International Conference on Learning Representations (ICLR), 2020.\\n[82] Woojun Kim, Myungsik Cho, and Youngchul Sung. Message-dropout: An efficient training method for multi-\\nagent deep reinforcement learning. In The Thirty-Third AAAI Conference on Artificial Intelligence, pages 6079â€“\\n6086, 2019.\\n[83] Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, and Yang Gao. Multi-agent game abstraction\\nvia graph attention neural network. In The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI),\\npages 7211â€“7218, 2020.\\n[84] Tianshu Chu, Sandeep Chinchali, and Sachin Katti. Multi-agent reinforcement learning for networked system\\ncontrol. In 8th International Conference on Learning Representations (ICLR), 2020.\\n31\\n[85] Chao Qu, Hui Li, Chang Liu, Junwu Xiong, James Zhang, Wei Chu, Yuan Qi, and Le Song. Intention propaga-\\ntion for multi-agent reinforcement learning. CoRR, abs/2004.08883, 2020.\\n[86] Guangzheng Hu, Yuanheng Zhu, Dongbin Zhao, Mengchen Zhao, and Jianye Hao. Event-triggered multi-agent\\nreinforcement learning with communication under limited-bandwidth constraint. CoRR, abs/2010.04978, 2020.\\n[87] Benjamin Freed, Rohan James, Guillaume Sartoretti, and Howie Choset. Sparse discrete communication learn-\\ning for multi-agent cooperation through backpropagation. In IEEE/RSJ International Conference on Intelligent\\nRobots and Systems (IROS), pages 7993â€“7998, 2020.\\n[88] Xiangyu Kong, Bo Xin, Fangchen Liu, and Yizhou Wang. Revisiting the master-slave architecture in multi-\\nagent deep reinforcement learning. CoRR, abs/1712.07305, 2017.\\n[89] Abhishek Das, ThÂ´eophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau.\\nTarmac: Targeted multi-agent communication. In Proceedings of the 36th International Conference on Machine\\nLearning (ICML), pages 1538â€“1546, 2019.\\n[90] Woojun Kim, Jongeui Park, and Youngchul Sung.\\nCommunication in multi-agent reinforcement learning:\\nIntention sharing. In 9th International Conference on Learning Representations (ICLR), 2021.\\n[91] Nikunj Gupta, G. Srinivasaraghavan, Swarup Kumar Mohalik, and Matthew E. Taylor. HAMMER: multi-level\\ncoordination of reinforcement learning agents via learned messaging. CoRR, abs/2102.00824, 2021.\\n[92] Yaru Niu, Rohan R. Paleja, and Matthew C. Gombolay. Multi-agent graph-attention communication and team-\\ning. In 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pages 964â€“\\n973, 2021.\\n[93] Yali Du, Bo Liu, Vincent Moens, Ziqi Liu, Zhicheng Ren, Jun Wang, Xu Chen, and Haifeng Zhang. Learning\\ncorrelated communication topology in multi-agent reinforcement learning. In 20th International Conference on\\nAutonomous Agents and Multiagent Systems (AAMAS), pages 456â€“464, 2021.\\n[94] Yutong Wang and Guillaume Sartoretti. Fcmnet: Full communication memory net for team-level cooperation\\nin multi-agent systems. CoRR, abs/2201.11994, 2022.\\n[95] Wanqi Xue, Wei Qiu, Bo An, Zinovi Rabinovich, Svetlana Obraztsova, and Chai Kiat Yeo. Mis-spoke or\\nmis-lead: Achieving robustness in multi-agent communicative reinforcement learning. CoRR, abs/2108.03803,\\n2021.\\n[96] Lucian Busoniu, Robert Babuska, and Bart De Schutter. Multi-agent reinforcement learning: A survey. In Ninth\\nInternational Conference on Control, Automation, Robotics and Vision (ICARCV), pages 1â€“6, 2006.\\n[97] Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala, TimothÂ´ee Lacroix, Zeming Lin, Florian\\nRichoux, and Nicolas Usunier. Torchcraft: a library for machine learning research on real-time strategy games.\\nCoRR, abs/1611.00625, 2016.\\n[98] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo,\\nAlireza Makhzani, Heinrich KÂ¨uttler, John P. Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig\\nPetersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy P. Lillicrap, Kevin Calderone,\\nPaul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft II:\\nA new challenge for reinforcement learning. CoRR, abs/1708.04782, 2017.\\n[99] Mikayel Samvelyan, Tabish Rashid, Christian SchrÂ¨oder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J.\\nRudner, Chia-Man Hung, Philip H. S. Torr, Jakob N. Foerster, and Shimon Whiteson. The starcraft multi-agent\\nchallenge. In Edith Elkind, Manuela Veloso, Noa Agmon, and Matthew E. Taylor, editors, Proceedings of the\\n18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS â€™19, Montreal, QC,\\nCanada, May 13-17, 2019, pages 2186â€“2188, 2019.\\n[100] Karol Kurach, Anton Raichuk, Piotr Stanczyk, Michal Zajac, Olivier Bachem, Lasse Espeholt, Carlos\\nRiquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly. Google research football:\\nA novel reinforcement learning environment. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,\\npages 4501â€“4510, 2020.\\n[101] LaÂ¨etitia Matignon, Guillaume J. Laurent, and Nadine Le Fort-Piat.\\nIndependent reinforcement learners in\\ncooperative markov games: a survey regarding coordination problems. Knowl. Eng. Rev., 27(1):1â€“31, 2012.\\n[102] Tim Brys, Ann NowÂ´e, Daniel Kudenko, and Matthew E. Taylor. Combining multiple correlated reward and\\nshaping signals by measuring confidence. In Carla E. Brodley and Peter Stone, editors, Proceedings of the\\nTwenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, QuÂ´ebec City, QuÂ´ebec, Canada,\\npages 1687â€“1693, 2014.\\n32\\n[103] Hangyu Mao, Zhengchao Zhang, Zhen Xiao, Zhibo Gong, and Yan Ni. Learning agent communication under\\nlimited bandwidth by message pruning. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI\\n2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth\\nAAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February\\n7-12, 2020, pages 5142â€“5149, 2020.\\n[104] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In 5th Interna-\\ntional Conference on Learning Representations (ICLR), 2017.\\n[105] Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized\\nplanning. Neurocomputing, 190:82â€“94, 2016.\\n[106] Jones Granatyr, Vanderson Botelho, Otto Robert Lessing, Edson EmÂ´Ä±lio Scalabrin, Jean-Paul A. Barth`es, and\\nFabrÂ´Ä±cio Enembreck. Trust and reputation models for multiagent systems. ACM Comput. Surv., 48(2):27:1â€“\\n27:42, 2015.\\n[107] Dogan Gunes, Taha. Strategic and Adaptive Behaviours in Trust Systems. PhD thesis, University of Southamp-\\nton, 2021.\\n[108] JÂ¨org P. MÂ¨uller and Klaus Fischer. Application impact of multi-agent systems and technologies: A survey. In\\nOnn Shehory and Arnon Sturm, editors, Agent-Oriented Software Engineering - Reflections on Architectures,\\nMethodologies, Languages, and Frameworks, pages 27â€“53. 2014.\\n[109] Manuel Herrera, Marco PÂ´erez-HernÂ´andez, Ajith Kumar Parlikad, and JoaquÂ´Ä±n Izquierdo. Multi-agent systems\\nand complex networks: Review and applications in systems engineering. Processes, 8(3), 2020.\\n[110] Davide Calvaresi, Alevtina Dubovitskaya, Jean-Paul Calbimonte, Kuldar Taveter, and Michael Schumacher.\\nMulti-agent systems and blockchain: Results from a systematic literature review. In Yves Demazeau, Bo An,\\nJavier Bajo, and Antonio FernÂ´andez-Caballero, editors, Advances in Practical Applications of Agents, Multi-\\nAgent Systems, and Complexity: The PAAMS Collection - 16th International Conference, PAAMS 2018, Toledo,\\nSpain, June 20-22, 2018, Proceedings, volume 10978 of Lecture Notes in Computer Science, pages 110â€“126,\\n2018.\\n[111] Georgios Papoudakis, Filippos Christianos, Lukas SchÂ¨afer, and Stefano V. Albrecht. Benchmarking multi-agent\\ndeep reinforcement learning algorithms in cooperative tasks. In Joaquin Vanschoren and Sai-Kit Yeung, editors,\\nProceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS\\nDatasets and Benchmarks 2021, December 2021, virtual, 2021.\\n[112] Ben Bogin, Mor Geva, and Jonathan Berant. Emergence of communication in an interactive world with consis-\\ntent speakers. CoRR, abs/1809.00549, 2018.\\n[113] Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and\\ntaxonomy. IEEE Trans. Pattern Anal. Mach. Intell., 41(2):423â€“443, 2019.\\n[114] Petra Poklukar, Miguel Vasco, Hang Yin, Francisco S. Melo, Ana Paiva, and Danica Kragic.\\nGeometric\\nmultimodal contrastive representation learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba\\nSzepesvÂ´ari, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022,\\n17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages\\n17782â€“17800, 2022.\\n[115] Joseph Seering, Michal Luria, Geoff Kaufman, and Jessica Hammer. Beyond dyadic interactions: Considering\\nchatbots as community members. In Stephen A. Brewster, Geraldine Fitzpatrick, Anna L. Cox, and Vassilis\\nKostakos, editors, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI\\n2019, Glasgow, Scotland, UK, May 04-09, 2019, page 450, 2019.\\n[116] Joseph Seering, Michal Luria, Connie Ye, Geoff Kaufman, and Jessica Hammer. It takes a village: Integrating\\nan adaptive chatbot into an online gaming community. In Regina Bernhaupt, Florian â€™Floydâ€™ Mueller, David\\nVerweij, Josh Andres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, Alix Goguey, Pernille BjÃ¸n,\\nShengdong Zhao, Briane Paul Samson, and Rafal Kocielnik, editors, CHI â€™20: CHI Conference on Human\\nFactors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020, pages 1â€“13, 2020.\\n[117] AsbjÃ¸rn FÃ¸lstad and Petter Bae BrandtzÃ¦g. Chatbots and the new world of HCI. Interactions, 24(4):38â€“42,\\n2017.\\n[118] Romit Roy Choudhury, Krishna Paul, and Somprakash Bandyopadhyay. Marp: a multi-agent routing protocol\\nfor mobile wireless ad hoc networks. Autonomous Agents and Multi-Agent Systems, 8(1):47â€“68, 2004.\\n[119] Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learn-\\ning. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine\\n33\\nLearning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learn-\\ning Research, pages 2817â€“2826, 2017.\\n[120] Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust deep re-\\ninforcement learning with adversarial attacks. In Elisabeth AndrÂ´e, Sven Koenig, Mehdi Dastani, and Gita\\nSukthankar, editors, Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent\\nSystems, AAMAS 2018, Stockholm, Sweden, July 10-15, 2018, pages 2040â€“2042, 2018.\\n[121] Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. Robust multi-agent reinforcement\\nlearning via minimax deep deterministic policy gradient. In The Thirty-Third AAAI Conference on Artificial\\nIntelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI\\n2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu,\\nHawaii, USA, January 27 - February 1, 2019, pages 4213â€“4220, 2019.\\n[122] Kaiqing Zhang, Tao Sun, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer Basar.\\nRobust multi-agent\\nreinforcement learning with model uncertainty. In Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell,\\nMaria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\\nvirtual, 2020.\\n[123] Sepp Hochreiter and JÂ¨urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735â€“1780, 1997.\\n[124] Jakob N. Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip H. S. Torr, Pushmeet Kohli,\\nand Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement learning. In Doina Pre-\\ncup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML\\n2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research,\\npages 1146â€“1155, 2017.\\n34\\n',\n",
       " 'final_answer': \"Transformer agents represent a significant advancement in artificial intelligence, particularly in the field of multi-agent deep reinforcement learning (MADRL). These systems leverage the transformer architecture, which excels in understanding and processing complex data, to create autonomous agents capable of engaging in complex tasks within dynamic environments.\\n\\n### What Are Transformer Agents?\\n\\nTransformer agents are essentially AI agents that utilize the transformer neural network architecture to process information. They have been widely adopted for their ability to handle long-range dependencies in data and their parallel processing capabilities. These agents are often large language models (LLMs) capable of generating text, reasoning, and adapting their responses based on interactions with their environments and other agents.\\n\\n**Key Characteristics of Transformer Agents:**\\n\\n1. **Self-Attention Mechanism:** Unlike previous architectures that relied heavily on recurrence, transformers utilize self-attention to weigh the importance of different words or data points in a sequence, allowing for more nuanced understanding and quicker learning.\\n\\n2. **Generative Capabilities:** Transformer agents can produce human-like text and engage in dialogue, making them suitable for roles ranging from customer service bots to collaborative assistants in various tasks.\\n\\n3. **Learnable Communication:** These agents are capable of learning from interactions with their environment and other agents, adapting their communication strategies over time to enhance efficiency and effectiveness.\\n\\n4. **Multi-Agent Coordination:** Transformer agents can communicate and coordinate with multiple counterparts, leading to improved performance in cooperative or competitive scenarios through learned communication protocols.\\n\\n### Recent Evolutions in Research\\n\\nResearch into transformer agents is rapidly evolving, with numerous advancements aimed at enhancing their capabilities and applicability. Here are some of the current trends and innovations:\\n\\n1. **Self-Evolving Agents:** Recent studies emphasize developing self-evolving agentsâ€”autonomous systems that adapt and optimize their internal processes based on continuous feedback from their environment. This includes adapting their communication strategies and workflow based on past interactions and observed outcomes.\\n\\n2. **Communication Efficiency:** The integration of communication in MADRL settings highlights the importance of efficient message sharing among agents. Researchers are exploring frameworks that determine when, how, and what to communicate, allowing agents to maintain useful interactions while minimizing overhead.\\n\\n3. **Robustness and Safety:** Ensuring that transformer agents can operate safely, especially in real-world applications, is a critical focus area. This involves creating robust systems capable of resisting adversarial attacks and maintaining functionality despite unpredictable interactions.\\n\\n4. **Memory and Tool Use:** Some transformer agents incorporate advanced memory architectures, enabling them to retain and access information over long periods, facilitating ongoing learning and adaptation. Additionally, they are being designed to effectively use external tools, greatly extending their range of actions in practical tasks.\\n\\n5. **Evaluation Frameworks:** As these agents become more complex, so too do the metrics for their evaluation. Current methodologies go beyond simple success rates to encompass reasoning quality, generalization abilities, and compliance with safety and alignment standards.\\n\\n6. **Domain-Specific Optimizations:** There is growing interest in tailoring transformer agents for specific domains, such as medicine or finance, where their architectures can be fine-tuned to meet the unique requirements of those fields.\\n\\n7. **Multi-Agent Systems:** New research highlights the development of systems where multiple transformer agents can collaborate, facilitating complex problem solving by distributing tasks among specialized agents, enhancing both efficiency and efficacy.\\n\\n### Conclusion\\n\\nTransformer agents are at the forefront of AI research, playing essential roles in various applications by leveraging advanced models to understand, learn, and interact in complex environments. Ongoing research is focused on refining these agents' capabilities, addressing challenges related to communication, safety, and domain specialization, thus paving the way for more robust, efficient, and trustworthy AI systems equipped for real-world applications.\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425788d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
