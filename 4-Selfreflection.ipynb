{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3679b0",
   "metadata": {},
   "source": [
    "### üß† What is Self-Reflection in RAG?\n",
    "Self-reflection = LLM evaluates its own output:\n",
    "‚ÄúIs this clear, complete, and accurate?‚Äù\n",
    "\n",
    "#### Self-Reflection in RAG using LangGraph, we‚Äôll design a workflow where the agent:\n",
    "\n",
    "1. Generates an initial answer using retrieved context\n",
    "2. Reflects on that answer with a dedicated self-critic LLM step\n",
    "3. If unsatisfied, it can revise the query, retrieve again, or regenerate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9f3ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_classic.schema import Document\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf072a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load llm models\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "llm=init_chat_model(\"openai:gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bc7a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = TextLoader(\"research_notes.txt\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2246d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2. State Definition\n",
    "# -------------------------\n",
    "class RAGReflectionState(BaseModel):\n",
    "    question: str\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = \"\"\n",
    "    reflection: str = \"\"\n",
    "    revised: bool = False\n",
    "    attempts: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a454cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3. Nodes\n",
    "# -------------------------\n",
    "\n",
    "# a. Retrieve\n",
    "def retrieve_docs(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    docs = retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"retrieved_docs\": docs})\n",
    "\n",
    "# b. Generate Answer\n",
    "def generate_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "Use the following context to answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{state.question}\n",
    "\"\"\"\n",
    "    answer = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update={\"answer\": answer, \"attempts\": state.attempts + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de27da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Self-Reflect\n",
    "def reflect_on_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Reflect on the following answer to see if it fully addresses the question. \n",
    "State YES if it is complete and correct, or NO with an explanation.\n",
    "\n",
    "Question: {state.question}\n",
    "\n",
    "Answer: {state.answer}\n",
    "\n",
    "Respond like:\n",
    "Reflection: YES or NO\n",
    "Explanation: ...\n",
    "\"\"\"\n",
    "    result = llm.invoke(prompt).content\n",
    "    is_ok = \"reflection: yes\" in result.lower()\n",
    "    return state.model_copy(update={\"reflection\": result, \"revised\": not is_ok})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a07cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Finalizer\n",
    "def finalize(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eafcf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4. LangGraph DAG\n",
    "# -------------------------\n",
    "builder = StateGraph(RAGReflectionState)\n",
    "\n",
    "builder.add_node(\"retriever\", retrieve_docs)\n",
    "builder.add_node(\"responder\", generate_answer)\n",
    "builder.add_node(\"reflector\", reflect_on_answer)\n",
    "builder.add_node(\"done\", finalize)\n",
    "\n",
    "builder.set_entry_point(\"retriever\")\n",
    "\n",
    "builder.add_edge(\"retriever\", \"responder\")\n",
    "builder.add_edge(\"responder\", \"reflector\")\n",
    "builder.add_conditional_edges(\n",
    "    \"reflector\",\n",
    "    lambda s: \"done\" if not s.revised or s.attempts >= 2 else \"retriever\"\n",
    ")\n",
    "\n",
    "builder.add_edge(\"done\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c3ed5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Final Answer:\n",
      " Based on the provided context, the transformer variants mentioned for production deployments include EfficientFormer and TinyBERT. EfficientFormer is deployed on a Raspberry Pi 4, and TinyBERT is used for classification tasks such as support ticket priority tagging.\n",
      "\n",
      "üîÅ Reflection Log:\n",
      " Reflection: NO\n",
      "\n",
      "Explanation: The answer identifies two transformer variants, EfficientFormer and TinyBERT, with specific deployment examples. However, it lacks a comprehensive overview of other widely used transformer variants in production deployments. Notably, it omits mentioning popular variants such as BERT, GPT, RoBERTa, DistilBERT, and others that are commonly utilized across various applications and industries. A complete answer would include a broader range of transformer variants to fully address the question.\n",
      "üîÑ Total Attempts: 2\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 5. Run the Agent\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What are the transformer variants in production deployments?\"\n",
    "    init_state = RAGReflectionState(question=user_query)\n",
    "    result = graph.invoke(init_state)\n",
    "\n",
    "    print(\"\\nüß† Final Answer:\\n\", result[\"answer\"])\n",
    "    print(\"\\nüîÅ Reflection Log:\\n\", result[\"reflection\"])\n",
    "    print(\"üîÑ Total Attempts:\", result[\"attempts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4c8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
